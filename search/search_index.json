{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FusionBench: A Comprehensive Benchmark/Toolkit of Deep Model Fusion","text":"<p>FusionBench: \u4e00\u4e2a\u5168\u9762\u7684\u6df1\u5ea6\u6a21\u578b\u878d\u5408\u7684\u57fa\u51c6/\u5de5\u5177\u5305</p> <p> </p> Breaking Changes in v0.2 (\u91cd\u5927\u53d8\u66f4) <p>Recent upgrade to v0.2.0 may cause some breaking changes. Make some documented instructions may be outdated. You can install a specific version by <code>pip install fusion-bench==0.1.6</code> or checkout to a specific version by <code>git checkout v0.1.6</code>. If you encounter any issues, please feel free to raise an issue. We are working on the documentation and will update it as soon as possible. Use version &gt;=0.2.0 is recommended.</p> <p>\u6700\u8fd1\u5347\u7ea7\u5230 v0.2.0 \u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e00\u4e9b\u91cd\u5927\u53d8\u66f4\uff0c\u4f7f\u5f97\u6587\u6863\u4e2d\u7684\u4e00\u4e9b\u6307\u4ee4\u53ef\u80fd\u5df2\u8fc7\u65f6\uff0c\u9700\u8981\u66f4\u65b0\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7 <code>pip install fusion-bench==0.1.6</code> \u5b89\u88c5\u7279\u5b9a\u7248\u672c\uff0c\u6216\u8005\u901a\u8fc7 <code>git checkout v0.1.6</code> \u5207\u6362\u5230\u7279\u5b9a\u7248\u672c\u3002 \u5982\u679c\u9047\u5230\u4efb\u4f55\u95ee\u9898\uff0c\u8bf7\u968f\u65f6\u63d0\u51fa\u95ee\u9898\u3002 \u6211\u4eec\u6b63\u5728\u52aa\u529b\u66f4\u65b0\u6587\u6863\uff0c\u5e76\u4f1a\u5c3d\u5feb\u66f4\u65b0\u3002\u5efa\u8bae\u4f7f\u7528 &gt;=0.2.0 \u7248\u672c\u3002</p> <p>Note</p> <ul> <li>Any questions or comments can be directed to the GitHub Issues page for this project.</li> <li>Any contributions or pull requests are welcome. If you find any mistakes or have suggestions for improvements, please feel free to raise an issue or submit a pull request.</li> </ul> <ul> <li>\u4efb\u4f55\u95ee\u9898\u6216\u8bc4\u8bba\u53ef\u4ee5\u76f4\u63a5\u5728\u8be5\u9879\u76ee\u7684GitHub Issues\u9875\u9762\u4e0a\u63d0\u51fa\u3002</li> <li>\u6b22\u8fce\u4efb\u4f55\u8d21\u732e\u6216\u62c9\u53d6\u8bf7\u6c42\u3002\u5982\u679c\u60a8\u53d1\u73b0\u4efb\u4f55\u9519\u8bef\u6216\u6709\u6539\u8fdb\u5efa\u8bae\uff0c\u8bf7\u968f\u65f6\u63d0\u51fa\u95ee\u9898\u6216\u63d0\u4ea4\u62c9\u53d6\u8bf7\u6c42\u3002</li> </ul> <p>Introduction to Deep Model Fusion (The Learn From Model Paradigm)</p> <p>Deep model fusion is a technique that merges, ensemble, or fuse multiple deep neural networks to obtain a unified model. It can be used to improve the performance and robustness of model or to combine the strengths of different models, such as fuse multiple task-specific models to create a multi-task model. For a more detailed introduction to deep model fusion, you can refer to W. Li, 2023, 'Deep Model Fusion: A Survey'.  In this benchmark, we evaluate the performance of different fusion methods on a variety of datasets and tasks. ...</p> <p>\u6df1\u5ea6\u6a21\u578b\u878d\u5408\u7b80\u4ecb\uff08\u4ece\u6a21\u578b\u4e2d\u5b66\u4e60\u7684\u8303\u5f0f\uff09</p> <p>\u6df1\u5ea6\u6a21\u578b\u878d\u5408\u662f\u4e00\u79cd\u5c06\u591a\u4e2a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5408\u5e76\u3001\u96c6\u6210\u6216\u878d\u5408\u4ee5\u83b7\u5f97\u7edf\u4e00\u6a21\u578b\u7684\u6280\u672f\u3002 \u5b83\u53ef\u4ee5\u7528\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u6216\u8005\u7ed3\u5408\u4e0d\u540c\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u4f8b\u5982\u5c06\u591a\u4e2a\u7279\u5b9a\u4efb\u52a1\u7684\u6a21\u578b\u878d\u5408\u6210\u4e00\u4e2a\u591a\u4efb\u52a1\u6a21\u578b\u3002 \u6709\u5173\u6df1\u5ea6\u6a21\u578b\u878d\u5408\u7684\u66f4\u8be6\u7ec6\u4ecb\u7ecd\uff0c\u53ef\u4ee5\u53c2\u8003 W. Li, 2023, 'Deep Model Fusion: A Survey'\u3002 \u5728\u8fd9\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u4e0d\u540c\u878d\u5408\u65b9\u6cd5\u5728\u5404\u79cd\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002...</p> <p> Read More</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#installation","title":"Installation","text":"<p>Install from PyPI:</p> <pre><code>pip install fusion-bench\n\n# you can also install a specific version\n# pip install fusion-bench==0.1.6\n</code></pre> <p>Or install the latest version in development from github repository</p> <pre><code>git clone https://github.com/tanganke/fusion_bench.git\ncd fusion_bench\n\n# checkout to use a specific version. for example, v0.1.6\n# git checkout v0.1.6\n\npip install -e . # install the package in editable mode\n</code></pre> <p>View documentation locally:</p> <pre><code># install mkdocs and the required packages\npip install -r mkdocs-requirements.txt\n\n# serve the documentation, by default it will be available at http://localhost:8000\nmkdocs serve\n</code></pre>"},{"location":"#command-line-interface","title":"Command Line Interface","text":"<p><code>fusion_bench</code> is the command line interface for running the benchmark.  It takes a configuration file as input, which specifies the models, fusion method to be used, and the datasets to be evaluated.  To run the benchmark, you can use the following command:</p> <pre><code>fusion_bench [--config-path CONFIG_PATH] [--config-name CONFIG_NAME] \\\n    OPTION_1=VALUE_1 OPTION_2=VALUE_2 ...\n</code></pre> <p>This program will load the configuration file specified by <code>--config-path</code> and <code>--config-name</code>, and run the fusion algorithm on the model pool. The pseudocode is as follows:</p> <pre><code># instantiate an algorithm, a modelpool object that manages the models, \n# and a taskpool object that manages the tasks (dataset + metrics)\nalgorithm = load_algorithm(config.algorithm)\nmodelpool = load_modelpool(config.modelpool)\ntaskpool = load_taskpool(config.taskpool)\n\n# run the fusion algorithm on the model pool\nmerged_model = algorithm.run(modelpool)\n# evaluate the merged model on the tasks\nreport = taskpool.evaluate(merged_model)\n</code></pre> <p>For detailed information on the options available, you can refer to this page.</p>"},{"location":"#implmentation-of-fusion-algorithms","title":"Implmentation of Fusion Algorithms","text":"<p>Working in progress.</p>"},{"location":"#general-structure-of-fusionbench","title":"General Structure of FusionBench","text":"Framework of FusionBench <p>FusionBench is a pioneering project that provides a comprehensive benchmark for deep model fusion, facilitating the evaluation and comparison of various model fusion techniques. The project is meticulously designed to support rigorous analysis and experimentation in the field of model fusion, offering a versatile and modular codebase tailored for advanced research and development.</p> <p>The general structure of the FusionBench project can be visualized through its modular framework, which is divided into several key components:</p> <ol> <li>Fusion Algorithm: The core component where Model Fusion takes place. It integrates models from the Model Pool and adjusts them according to the specified fusion algorithms. The output is then evaluated for performance and effectiveness.</li> <li>Model Pool: A repository of various pre-trained models that can be accessed and utilized for fusion. This pool serves as the foundation for creating new, fused models by leveraging the strengths of each individual model.</li> <li>Task Pool: A collection of tasks that the fused models are evaluated on. These tasks help in assessing the practical applicability and robustness of the fused models.</li> <li>Models &amp; Warpers, Datasets, and Metrics: These underlying modules include:<ul> <li>Models &amp; Warpers: Tools and scripts for model loading, wrapping, and pre-processing.</li> <li>Datasets: The datasets used for training, validation, and testing the fused models.</li> <li>Metrics: The performance metrics used to evaluate the models, providing a comprehensive understanding of their capabilities.</li> </ul> </li> <li>YAML Configurations: Central to the project's modularity, YAML files are used to configure models, datasets, and metrics, allowing seamless customization and scalability.      This is based on the hydra framework, which allows for easy customization and scalability.      Read More</li> </ol> <p>By organizing these components into a structured and modular codebase, FusionBench ensures flexibility, ease of use, and scalability for researchers and developers. The project not only serves as a benchmark but also as a robust platform for innovation in the realm of deep model fusion.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Comprehensive Benchmark: FusionBench provides a wide range of fusion algorithms, model pools, and tasks for thorough evaluation.</li> <li>Modular Design: The project is structured into separate modules for algorithms, model pools, and task pools, allowing easy extension and customization.</li> <li>Command-line Interface: A flexible CLI tool <code>fusion_bench</code> for running experiments with various configurations.</li> <li>Web UI: An interactive web interface <code>fusion_bench_webui</code> for easier configuration and command generation.</li> <li>Extensive Documentation: Detailed guides, API references, and examples to help users get started quickly.</li> </ul>"},{"location":"#basic-example","title":"Basic Example","text":"<p>Here we provide a basic example to demonstrate the usage of FusionBench. We choose the simple average algorithm as the fusion algorithm, and 4 fine-tuned CLIP-ViT-B/32 models to be merged. We are going to evaluate the merged model on 4 tasks with data corrupted by Gaussian noise to evaluate the robustness of the merged model.</p> <p>We provide an command line interface <code>fusion_bench</code> to run the example. The instruction to run the example is as follows:</p> <pre><code>fusion_bench \\\n    # (1)\n    --config-name clip-vit-base-patch32_robustness_corrupted \\\n    corruption=gaussian_noise \\\n    # (2)\n    method=simple_averaging  \\\n    # (3)\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_robustness_corrupted \\\n    # (4)\n    taskpool=clip-vit-base-patch32_robustness_corrupted\n</code></pre> <ol> <li>Here we specify the main configuration file to be used.      The <code>corruption</code> option specifies the type of data corruption to be applied to the evaluation datasets. In this case, we use Gaussian noise.      In FusionBench, we are currently provide 7 types of data corruptions for imaage classification tasks Standford Cars, EuroSAT, RESISC45 and GTSRB.      The option <code>corrption</code> can be one of: <code>contrast</code>, <code>gaussian_noise</code>, <code>impulse_noise</code>, <code>jpeg_compression</code>, <code>motion_blur</code>, <code>pixelate</code>, <code>spatter</code>.</li> <li>The <code>method</code> option specifies the fusion algorithm to be used. In this case, we use the simple averaging algorithm.</li> <li>Here we specify the model pool to be used.      The model pool is responsible for managing the loading, preprocessing, and saving of the models.     By pass option <code>modelpool=CLIPVisionModelPool/clip-vit-base-patch32_robustness_corrupted</code>, the program instantiate a modelpool object that manages 4 task-specific CLIP-ViT-B/32 models that are fine-tuned on Stanford Cars, EuroSAT, RESISC45, and GTSRB datasets.</li> <li>Here we specify the task pool to be used.      The task pool is responsible for managing the evaluation datasets and metrics.     By pass option <code>taskpool=clip-vit-base-patch32_robustness_corrupted</code>, the program instantiate a taskpool object that manages 4 tasks with data corrupted by Gaussian noise.</li> </ol> <p>The configurations are stored in the <code>configs</code> directory, listed as follows:</p> Method ConfigurationModel Pool ConfigurationTask Pool Configuration <p>The simple averaging algorithm is very straightforward. No additional hyperparameters are required. So the configuration file contains only the name of the algorithm to specify the Python class of the fusion algorithm.</p> config/method/simple_average.yaml<pre><code>name: simple_average # (1)\n</code></pre> <ol> <li>Name of the fusion algorithm. The <code>name</code> field specifies the class of the fusion algorithm.</li> </ol> config/modelpool/clip-vit-base-patch32_robustness_corrupted.yaml<pre><code>type: huggingface_clip_vision # (1)\nmodels: # (2)\n- name: _pretrained_\n    path: openai/clip-vit-base-patch32\n- name: stanford_cars\n    path: tanganke/clip-vit-base-patch32_stanford-cars\n- name: eurosat\n    path: tanganke/clip-vit-base-patch32_eurosat\n- name: resisc45\n    path: tanganke/clip-vit-base-patch32_resisc45\n- name: gtsrb\n    path: tanganke/clip-vit-base-patch32_gtsrb\n\n\n# `corrption` can be one of:\n# contrast, gaussian_noise, impulse_noise, jpeg_compression, motion_blur, pixelate, spatter\ncorruption: ${corruption}\n\n# Other configurations to meet other methods' requirements.\n# For example, test dataset for test-time adaptation training.\n# ...\n</code></pre> <ol> <li>Type of the model pool. The <code>type</code> field specifies the class of the model pool.</li> <li>The <code>models</code> field specifies the models to be used for fusion. In this case, we use 4 task-specific CLIP-ViT-B/32 models that are fine-tuned on Stanford Cars, EuroSAT, RESISC45, and GTSRB datasets.</li> </ol> config/taskpool/clip-vit-base-patch32_robustness_corrupted.yaml<pre><code>type: clip_vit_classification # (1)\nname: clip-vit-robustness_clean\n\n# corrption can be one of:\n# contrast, gaussian_noise, impulse_noise, jpeg_compression, motion_blur, pixelate, spatter\ncorruption: ${corruption}\ndataset_type: huggingface_image_classification\ntasks: # (2)\n- name: stanford_cars\n    dataset:\n    name: tanganke/stanford_cars\n    split: ${taskpool.corruption}\n- name: eurosat\n    dataset:\n    name: tanganke/eurosat\n    split: ${taskpool.corruption}\n- name: resisc45\n    dataset:\n    name: tanganke/resisc45\n    split: ${taskpool.corruption}\n- name: gtsrb\n    dataset:\n    name: tanganke/gtsrb\n    split: ${taskpool.corruption}\n\nclip_model: openai/clip-vit-base-patch32 # (3)\nbatch_size: 128 # (4)\nnum_workers: 16\nfast_dev_run: ${fast_dev_run}\n</code></pre> <ol> <li>Type and name of the task pool. The <code>type</code> field specifies the class of the task pool, and the <code>name</code> field specifies the name of the task pool.</li> <li>The <code>tasks</code> field specifies the tasks to be evaluated. In this case, we evaluate the fused model on 4 tasks: Stanford Cars, EuroSAT, RESISC45, and GTSRB, with data corrupted by <code>${corruption}</code>.</li> <li>Base model used for intializing the classification head. Here, we need the text encoder of CLIP-ViT-B/32 to initialize the classification head.</li> <li>Batch size and number of workers used for data loading.</li> </ol> <p>A flowchart of the FusionBench command line interface is shown below:</p> <ul> <li> <p>Fusion Algorithm Module</p> <p>Implement the fusion algorithms. Receive the model pool and return the fused model.</p> <p> Read More</p> </li> <li> <p>Model Pool Module</p> <p>Magage the models. Responsible for loading, preprocessing, and saving the models.</p> <p> Read More</p> </li> <li> <p>Task Pool Module</p> <p>Manage the tasks. Responsible for loading evaluation datasets and metrics, and evaluating the fused model.</p> <p> Read More</p> </li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you find this benchmark useful, please consider citing our work:</p> <pre><code>@misc{tangFusionBenchComprehensiveBenchmark2024,\n  title = {{{FusionBench}}: {{A Comprehensive Benchmark}} of {{Deep Model Fusion}}},\n  shorttitle = {{{FusionBench}}},\n  author = {Tang, Anke and Shen, Li and Luo, Yong and Hu, Han and Du, Bo and Tao, Dacheng},\n  year = {2024},\n  month = jun,\n  number = {arXiv:2406.03280},\n  eprint = {2406.03280},\n  publisher = {arXiv},\n  url = {http://arxiv.org/abs/2406.03280},\n  archiveprefix = {arxiv},\n  langid = {english},\n  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}\n}\n</code></pre>"},{"location":"introduction_to_model_fusion/","title":"Introduction to Deep Model Fusion (The Learn From Model Paradigm)","text":"<p>Deep model fusion is a technique that merges, ensemble, or fuse multiple deep neural networks to obtain a unified model. It can be used to improve the performance and robustness of model or to combine the strengths of different models, such as fuse multiple task-specific models to create a multi-task model. For a more detailed introduction to deep model fusion, you can refer to W. Li, 2023, 'Deep Model Fusion: A Survey'. In this benchmark, we evaluate the performance of different fusion methods on a variety of datasets and tasks.</p>"},{"location":"introduction_to_model_fusion/#background","title":"Background","text":"<p>The traditional approach of training deep networks with supervised learning requires:</p> <ol> <li>gathering a large dataset relevant to the problem at hand. </li> <li>Once the dataset is collected, manual labeling may be necessary to provide supervision.</li> <li>After the dataset is prepared, Feature Engineering can be conducted to select and transform variables that are most relevant to the predictive model. This step is optional but can significantly improve the accuracy of the model.</li> <li>The next step is Model Training, in which the design model architecture is developed and an optimization algorithm is chosen.</li> </ol> <p>We call this the \"learn from data\" paradigm. This approach has been very successful in many applications, and the size of both the dataset and the model has been increasing rapidly. </p> Hongling Zheng, Li Shen, et al. \"Learn from model beyond fine-tuning: A survey.\"  arXiv:2310.08184 (2023). <p>However, in the past few years, it is obvious that the learning paradigm of deep neural networks has undergone a significant shift. The traditional approach of training deep networks with supervised learning has been complemented with emerging techniques that transfer knowledge from existing models, edit existing models, or leverage unlabeled data to tune the models. Many data-efficient approaches have been proposed, such as model tuning, model distillation, model ensemble, model pruning, model editing and so on.  We can even achieve satisfactory results with just prompt design in a few minutes, without any data or model training.</p> <p>This transformation is driven by the increasing complexity of the models and the increasing cost of data labeling. We call this the \"learn from model\" paradigm.</p> <p>So why do we need to learn from models? What are the advantages of learning from models? By learning from model, we aim to enhance model performance, reduce training costs, improve generalization...</p> <ul> <li>Knowledge transfer across models (accelerate training, reuse sunk training cost)  </li> <li>Knowledge transfer across tasks (Facilitating learning from smaller datasets)</li> <li>Task regularization improve generalization and reduce overfitting</li> <li>Reduced model/parameter maintenance costs</li> <li>Improve the efficiency of the inference phase (one forward inference to obtain multiple task predictions)</li> <li>...</li> </ul>"},{"location":"introduction_to_model_fusion/#applications-of-deep-model-fusion","title":"Applications of Deep Model Fusion","text":""},{"location":"introduction_to_model_fusion/#accelerating-model-training","title":"Accelerating Model Training","text":"<p>Leveraging existing models provides a significant advantage in accelerating the development of new models.  Training a model from scratch is often both costly and time-consuming, so utilizing pre-existing models can substantially speed up this process.  By harnessing the knowledge embedded in these models, we can enhance the training efficiency for new models.</p>"},{"location":"introduction_to_model_fusion/#multi-task-learning","title":"Multi-Task Learning","text":"<p>Another popular application of deep model fusion is to construct multi-task models (FusionBench provides a comprehensive benchmark for multi-task model fusion). Where we aim to collaboratively train one single model with data from multiple tasks to facilitate knowledge transfer. Unlike traditional multi-task learning approaches that train a single model with data from multiple tasks, the multi-task model fusion approach reuse the task-specific models to construct a multi-task model.  This approach can be more data-efficient.</p> <p>Similar to single-task model training, train a multi-task model needs three steps: </p> <ol> <li>Data collection and preparation.</li> <li>Model design and algorithm selection.</li> <li>Model training and evaluation.</li> </ol>  Core steps of multi-task model training  <p>However, the traditional model training strategy has limitations in certain applications.  Particularly when the training data for each task is private and cannot be shared.  In other cases, we may only have access to the models trained on the private data, but not the data itself. In these cases, we need to develop a new training strategy that can train a multi-task model without sharing the private data.  And training a multi-task model from scratch can be very expensive.</p> <p>Model fusion without accessing training data in machine learning has attracted increasing interest due to the practical resource-saving and data privacy issues.  Creating a multi-task model by merging models for distinct tasks has proven to be an economical andscalable approach. By fuse deep neural networks, transfer knowledge, addressing task interference, and utilizing combined knowledge can be achieved without the need for accessing labeled datasets. This is scalable to large models and a wide array of tasks. </p> The general framework of multi-task model fusion"},{"location":"introduction_to_model_fusion/#improving-model-performance","title":"Improving Model Performance","text":"<p>Moreover, the knowledge from existing models can be used to create new models.  For instance, if we have multiple models, we can combine their knowledge through techniques like model ensembling or weight manipulation.  This approach allows us to develop more sophisticated models by integrating the strengths of several pre-trained models.</p>"},{"location":"introduction_to_model_fusion/#scaling-up-models","title":"Scaling Up Models","text":"<p>Additionally, we can upscale existing models to create larger, more powerful models.  This can be achieved by mixing the weights of existing models or by adding new layers to them.  One of the more popular methods for scaling up models recently is the use of Mixture of Experts (MoE) methods.  These methods allow for the scaling of models to very large sizes while maintaining a manageable inference cost.</p>"},{"location":"introduction_to_model_fusion/#formal-definition-and-taxonomies-of-deep-model-fusion","title":"Formal Definition and Taxonomies of Deep Model Fusion","text":"<p>In this section, we provide a formal definition of deep model fusion and introduce several taxonomies to categorize different fusion methods. We define deep model fusion as the process of combining multiple deep neural networks to create a unified model that leverages the knowledge and capabilities of the individual models. We taxonomize these fusion methods based on their underlying principles and mechanisms, providing a structured framework for understanding and comparing different fusion techniques. We categorize deep model fusion methods into three main types: Model Ensemble, Model Merging, and Model Mixing.</p> <p>The initial diagram presented below provides a visual representation of the Model Ensemble technique.  This method involves the training of several independent models, each generating its own set of predictions.  These individual predictions are then aggregated to produce a final, more accurate prediction. The Model Ensemble technique is frequently employed to enhance both the performance and reliability of models.  It achieves this by leveraging the collective intelligence of multiple models, thereby reducing the likelihood of errors that could arise from a single model's predictions. However, it's important to note that this technique does come with its own set of challenges.  The necessity to train and maintain multiple models can lead to increased computational costs.  This is due to the additional resources required for the training process, as well as the storage needed for each model.  Therefore, while the Model Ensemble technique can lead to improved accuracy, it's essential to consider the potential trade-off in terms of computational expense.</p> <p>Moving on, the second diagram represents Model Merging. This technique involves combining multiple models into a single model of the same architecture. By merging these models, we can create a more robust and comprehensive model that leverages the strengths of its constituent models.  This can lead to improved performance and generalization, especially in scenarios where the individual models excel in different areas.</p> <p>Lastly, the third diagram depicts Model Mixing, a technique where the weights of multiple models are combined to form a new model.  The new model typically has a different architecture from the original models, such as a larger size or more complex structure.</p>"},{"location":"supported_algorithms/","title":"Supported algorithms","text":"<p>Here is a table of supported algorithms in the benchmark:</p> Model Merging AlgorithmsModel Mixing AlgorithmsModel Ensemble AlgorithmsOthers Algorithm Name Class Path <code>dummy</code> <code>clip_finetune</code> <code>.classification.clip_finetune.ImageClassificationFineTuningForCLIP</code> <code>TaskVectorCosSimilarity</code> <code>.analysis.task_vector_cos_similarity.TaskVectorCosSimilarity</code> <code>simple_ensemble</code> <code>.ensemble.EnsembleAlgorithm</code> <code>weighted_ensemble</code> <code>.ensemble.WeightedEnsembleAlgorithm</code> <code>max_model_predictor</code> <code>.ensemble.MaxModelPredictorAlgorithm</code> <code>simple_average</code> <code>.simple_average.SimpleAverageAlgorithm</code> <code>weighted_average</code> <code>.weighted_average.weighted_average.WeightedAverageAlgorithm</code> <code>weighted_average_for_llama</code> <code>.weighted_average.llama.WeightedAverageForLLama</code> <code>clip_fisher_merging</code> <code>.fisher_merging.clip_fisher_merging.FisherMergingAlgorithmForCLIP</code> <code>gpt2_fisher_merging</code> <code>.fisher_merging.gpt2_fisher_merging.FisherMergingAlgorithmForGPT2</code> <code>clip_regmean</code> <code>.regmean.clip_regmean.RegMeanAlgorithmForCLIP</code> <code>gpt2_regmean</code> <code>.regmean.gpt2_regmean.RegMeanAlgorithmForGPT2</code> <code>task_arithmetic</code> <code>.task_arithmetic.TaskArithmeticAlgorithm</code> <code>ties_merging</code> <code>.ties_merging.ties_merging.TiesMergingAlgorithm</code> <code>clip_task_wise_adamerging</code> <code>.adamerging.clip_task_wise_adamerging.CLIPTaskWiseAdaMergingAlgorithm</code> <code>clip_layer_wise_adamerging</code> <code>.adamerging.clip_layer_wise_adamerging.CLIPLayerWiseAdaMergingAlgorithm</code> <code>singular_projection_merging</code> <code>fusion_bench.method.smile_upscaling.singular_projection_merging.SingularProjectionMergingAlgorithm</code> <code>pwe_moe_ls_for_clip</code> <code>.pwe_moe.clip_pwe_moe.PWEMoELinearScalarizationForCLIP</code> <code>pwe_moe_epo_for_clip</code> <code>.pwe_moe.clip_pwe_moe.PWEMoExactParetoOptimalForCLIP</code> <code>clip_concrete_task_arithmetic</code> <code>.concrete_subspace.clip_concrete_task_arithmetic.ConcreteTaskArithmeticAlgorithmForCLIP</code> <code>clip_concrete_task_wise_adamerging</code> <code>.concrete_subspace.clip_concrete_adamerging.ConcreteTaskWiseAdaMergingForCLIP</code> <code>clip_concrete_layer_wise_adamerging</code> <code>.concrete_subspace.clip_concrete_adamerging.ConcreteLayerWiseAdaMergingForCLIP</code> <code>mixtral_moe_merging</code> <code>.mixture_of_experts.mixtral_merging.MixtralMoEMergingAlgorithm</code> <code>mixtral_for_causal_lm_merging</code> <code>.mixture_of_experts.mixtral_merging.MixtralForCausalLMMergingAlgorithm</code> <code>clip_weight_ensembling_moe</code> <code>.we_moe.clip_we_moe.CLIPWeightEnsemblingMoEAlgorithm</code> Algorithm Name Class Description Depth Upscaling DepthUpscalingAlgorithm Depth upscaling algorithm that concatenates the layers of model.  MoE Upscaling MixtralUpscalingAlgorithm, MixtralForCausalLMUpscalingAlgorithm Mixture of Experts upscaling algorithm that merges the models.  SMILE Upscaling SmileUpscalingAlgorithm Upscale models to sparse low-rank MoE model.  Algorithm Name Class Description Simple Ensemble SimpleEnsembleAlgorithm Simple ensemble algorithm that averages the predictions of multiple models. Weighted Ensemble WeightedEnsembleAlgorithm Ensemble algorithm that averages the predictions of multiple models with given weights. <p>These algorithms are not directly related to model fusion, but they are used in the benchmark for other purposes.</p> Algorithm Name Class Description Dummy Algorithm DummyAlgorithm Return model as it is. <p>You can find the implementation of these algorithms in the corresponding files.</p>"},{"location":"algorithms/","title":"Introduction to Algorithm Module","text":"<p>The <code>Fusion Algorithm</code> module is a core component of the FusionBench project, dedicated to the implementation and execution of various model fusion techniques.  This module provides the mechanisms necessary to combine multiple models from the Model Pool, enabling nuanced and optimized model merging operations.</p>"},{"location":"algorithms/#key-points-of-the-fusion-algorithm-module","title":"Key Points of the <code>Fusion Algorithm</code> Module","text":"<ul> <li>Adaptive Fusion: The module supports advanced fusion techniques, such as AdaMerging, that adaptively learn the best coefficients for model merging using sophisticated methods like entropy minimization.</li> <li>Algorithm Configuration: Algorithms are defined and loaded based on configuration files, ensuring flexibility and ease of experimentation. This modular approach allows researchers to switch between different fusion methods seamlessly.</li> <li>Model Integration: It facilitates the integration of multiple models, combining their strengths and mitigating individual weaknesses. The result is a single, merged model that ideally performs better than any individual model alone or has multitasking capability.</li> <li>Evaluation Support: Once the model fusion process is completed, the merged model can interface with the TaskPool to evaluate the performance of the merged model across various tasks, providing a comprehensive assessment of its capabilities.</li> </ul>"},{"location":"algorithms/#example-capabilities","title":"Example Capabilities","text":"<ul> <li>Entropy Minimization: Some algorithms in this module utilize entropy minimization on unlabeled test samples to refine merging coefficients, ensuring that the fusion process is data-driven and optimized.</li> <li>Layer-wise and Task-wise Fusion: It allows both layer-wise and task-wise model fusion, where merging coefficients can be learned for individual layers or entire tasks, respectively.</li> </ul>"},{"location":"algorithms/#code-integration","title":"Code Integration","text":"<p>The module is typically invoked through a configuration-driven approach in CLI scripts, enabling users to specify fusion algorithms and parameters via YAML configuration files. This method ensures reproducibility and ease of use. For more information, see the document of fusion_bench CLI.</p> <p><code>ModelFusionAlgorithm</code> is the base class for all fusion algorithms in the Fusion Algorithm module.  It provides a common interface for different fusion techniques, allowing for seamless integration and execution of various algorithms.</p>"},{"location":"algorithms/#example-usage","title":"Example Usage","text":"<p>Implement your own model fusion algorithm:</p> <pre><code>from fusion_bench.method import BaseModelFusionAlgorithm\nfrom fusion_bench.modelpool import BaseModelPool\n\nclass DerivedModelFusionAlgorithm(BaseModelFusionAlgorithm):\n    \"\"\"\n    An example of a derived model fusion algorithm.\n    \"\"\"\n\n    # _config_mapping maps the attribution to the corresponding key in the configuration file.\n    _config_mapping = BaseModelFusionAlgorithm._config_mapping | {\n        \"hyperparam_attr_1\": \"hyperparam_1\",\n        \"hyperparam_attr_2\": \"hyperparam_2\",\n    }\n\n    def __init__(self, hyperparam_1, hyperparam_2, **kwargs):\n        self.hyperparam_attr_1 = hyperparam_1\n        self.hyperparam_attr_2 = hyperparam_2\n        super().__init__(**kwargs)\n\n    def run(self, modelpool: BaseModelPool):\n        # implement the fusion algorithm here\n        raise NotImplementedError(\n            \"DerivedModelFusionAlgorithm.run() is not implemented.\"\n        )\n</code></pre> <p>We provide a simple example to illustrate how the algorithm is used in the FusionBench as follows:</p> <pre><code>import logging\nfrom typing import Dict, Optional\nfrom omegaconf import DictConfig\n\nfrom fusion_bench.utils import instantiate\n\nlog = logging.getLogger(__name__)\n\ndef run_model_fusion(\n    method_config: DictConfig,\n    modelpool_config: DictConfig,\n    taskpool_config: Optional[DictConfig] = None,\n    seed: Optional[int] = None,\n    print_config: bool = True,\n    **kwargs\n):\n    \"\"\"\n    Run the model fusion process.\n\n    Args:\n        method_config: Configuration for the fusion method.\n        modelpool_config: Configuration for the model pool.\n        taskpool_config: Configuration for the task pool (optional).\n    \"\"\"\n    # Instantiate components: modelpool, method, and taskpool\n    modelpool = instantiate(modelpool_config)\n    method = instantiate(method_config)\n    taskpool = None\n    if taskpool_config is not None:\n        taskpool = instantiate(taskpool_config)\n\n    # Run fusion\n    merged_model = method.run(modelpool)\n\n    # Evaluate if taskpool is provided\n    if taskpool is not None:\n        report = taskpool.evaluate(merged_model)\n</code></pre> <p>In summary, the Fusion Algorithm module is vital for the model merging operations within FusionBench, leveraging sophisticated techniques to ensure optimal fusion and performance evaluation of deep learning models. This capability makes it an indispensable tool for researchers and practitioners focusing on model fusion strategies.</p>"},{"location":"algorithms/#references","title":"References","text":""},{"location":"algorithms/#fusion_bench.method.BaseAlgorithm","title":"<code>BaseAlgorithm</code>","text":"<p>               Bases: <code>BaseYAMLSerializableModel</code></p> <p>Base class for model fusion algorithms.</p> <p>This class provides a template for implementing model fusion algorithms. Subclasses must implement the <code>run</code> method to define the fusion logic.</p> Source code in <code>fusion_bench/method/base_algorithm.py</code> <pre><code>class BaseAlgorithm(BaseYAMLSerializableModel):\n    \"\"\"\n    Base class for model fusion algorithms.\n\n    This class provides a template for implementing model fusion algorithms.\n    Subclasses must implement the `run` method to define the fusion logic.\n    \"\"\"\n\n    _program = None\n\n    @abstractmethod\n    def run(self, modelpool: BaseModelPool):\n        \"\"\"\n        Fuse the models in the given model pool.\n\n        This method must be implemented by subclasses to define the fusion logic.\n\n        Examples:\n            &gt;&gt;&gt; algorithm = SimpleAverageAlgorithm()\n            &gt;&gt;&gt; modelpool = ModelPool()\n            &gt;&gt;&gt; merged_model = algorithm.run(modelpool)\n\n        Args:\n            modelpool (BaseModelPool): The pool of models to fuse.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"algorithms/#fusion_bench.method.BaseAlgorithm.run","title":"<code>run(modelpool)</code>  <code>abstractmethod</code>","text":"<p>Fuse the models in the given model pool.</p> <p>This method must be implemented by subclasses to define the fusion logic.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; algorithm = SimpleAverageAlgorithm()\n&gt;&gt;&gt; modelpool = ModelPool()\n&gt;&gt;&gt; merged_model = algorithm.run(modelpool)\n</code></pre> <p>Parameters:</p> Source code in <code>fusion_bench/method/base_algorithm.py</code> <pre><code>@abstractmethod\ndef run(self, modelpool: BaseModelPool):\n    \"\"\"\n    Fuse the models in the given model pool.\n\n    This method must be implemented by subclasses to define the fusion logic.\n\n    Examples:\n        &gt;&gt;&gt; algorithm = SimpleAverageAlgorithm()\n        &gt;&gt;&gt; modelpool = ModelPool()\n        &gt;&gt;&gt; merged_model = algorithm.run(modelpool)\n\n    Args:\n        modelpool (BaseModelPool): The pool of models to fuse.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"algorithms/#fusion_bench.method.BaseAlgorithm.run(modelpool)","title":"<code>modelpool</code>","text":"(<code>BaseModelPool</code>)           \u2013            <p>The pool of models to fuse.</p>"},{"location":"algorithms/#fusion_bench.method.BaseModelFusionAlgorithm","title":"<code>BaseModelFusionAlgorithm = BaseAlgorithm</code>  <code>module-attribute</code>","text":"<p>Alias for <code>BaseAlgorithm</code>.</p>"},{"location":"algorithms/adamerging/","title":"AdaMerging","text":"Task Vector, Task Arithmetic, and AdaMerging. Credit to <sup>1</sup> <p>In the complex landscape of multi-task learning, AdaMerging has emerged as a potent method for adaptively merging model parameters to optimize performance across tasks. Unlike traditional fixed-coefficient methods, AdaMerging autonomously learns merging coefficients, offering a more refined and responsive approach<sup>1</sup>. </p> <p>The cornerstone of AdaMerging lies in its adaptive nature, where it learns the coefficients for merging either on a task-wise or layer-wise basis. This adaptability is driven by an entropy minimization strategy applied to unlabeled test samples as a surrogate objective function, which serves to refine the merging coefficients for optimal performance.</p> <p>Task-wise AdaMerging is formulated as:</p> \\[ \\theta = \\theta_0 + \\sum_{i=1}^{n} \\lambda_i \\tau_i \\] <p>where \\(\\lambda_i\\) represents the merging coefficient for the \\(i\\)-th task, and \\(\\tau_i\\) denotes the task vector for the \\(i\\)-th task.</p> <p>On the other hand, Layer-wise AdaMerging is articulated as:</p> \\[ \\theta^l = \\theta_0^l + \\sum_{i=1}^{n} \\lambda^{l}_{i} \\tau^{l}_{i} \\] <p>where the merging coefficient \\(\\lambda^{l}_{i}\\) and task vector \\(\\tau^{l}_{i}\\) are specific to each layer \\(l\\) of the model.</p> <p>By leveraging this adaptive learning approach, AdaMerging significantly enhances the model's ability to generalize across tasks and layers, resulting in a more robust and finely-tuned performance profile. The method\u2019s reliance on entropy minimization ensures that the merging process continually seeks the most informative and stable configuration, adapting to the specific needs of the dataset and tasks at hand.</p>"},{"location":"algorithms/adamerging/#adamerging-analysis","title":"AdaMerging Analysis","text":"<p>Task-wise Coefficients.  The below Figure shows the changes during the iteration process of merging coefficient optimization of each task vector in Task-wise AdaMerging and AdaMerging++, which is shown every ten steps. We consistently observe that the merging coefficients of each task vector are inconsistent. When the number of tasks is relatively large, it is obviously undesirable to grid search the coefficients of each task, but our AdaMerging avoids this manual search process.</p>  Model merging coefficients \\(\\{\u03bb_k\\}_{k=1}^K\\) change with respect to training steps on ViT-B/32: (a) Task-wise AdaMerging; (b) Task-wise AdaMerging++. Each line represents the change process of the coefficient \\(\u03bb_k\\) of a task vector \\(T_k (k \\in \\{1, 2, . . . , K\\})\\).  <p>Layer-wise Coefficients. The following Figure shows the merging coefficients learned by Layer-wise AdaMerging and AdaMerging++ on ViT-B/32 respectively. We observed that:  </p> <ol> <li>The coefficients learned by each layer of each task vector are different, which shows that the importance of each layer in the model merging process is different. </li> <li>The coefficients learned by shallow layers are generally smaller than those of deep layers, which indicates that shallow layers rely more on the weights of the pre-trained model rather than the weights provided by task vectors, while the deep layers rely more on the weights provided by the task vectors. This may be since the shallow layer learns general features, which are cross-task, while the deep layer learns task-specific features <sup>2</sup>. This finding is also consistent with routing analysis in <sup>3</sup>.</li> </ol>  Learned model merging coefficients \\(\\{\u03bb_l^k\\}^{K,L}_{k=1,l=1}\\) of Layer-wise AdaMerging (Above) and AdaMerging++ (Below) on ViT-B/32.  The \\(k\\)-th row represents the \\(k\\)-th task vector, the \\(l\\)-th column represents the \\(l\\)-th layer, and the intersection point represents the coefficient \\(\u03bb^l_k\\)."},{"location":"algorithms/adamerging/#code-integration","title":"Code Integration","text":"<p>Merge CLIP-ViT-B/32 models from eight downstream image classification tasks:</p> <pre><code>fusion_bench \\\n    method=adamerging \\\n        method.name=clip_layer_wise_adamerging \\\n        method.save_merging_weights=merging_weights.pt \\\n    modelpool=clip-vit-base-patch32_TA8 \\\n    taskpool=clip-vit-classification_TA8 \\\n    fabric.loggers.root_dir=outputs/logs/ViT-B-32 \\\n    fabric.loggers.name=clip_layer_wise_adamerging_adam\n</code></pre> <p>Part of the output:</p> <pre><code>Profiler Report\n\n----------------------------------------------------------------------------------------------------------------------------------\n|  Action                       |  Mean duration (s)    |  Num calls            |  Total time (s)       |  Percentage %         |\n----------------------------------------------------------------------------------------------------------------------------------\n|  Total                        |  -                    |  26001                |  724.65               |  100 %                |\n----------------------------------------------------------------------------------------------------------------------------------\n|  backward pass                |  0.060172             |  8000                 |  481.38               |  66.429               |\n|  forward pass                 |  0.016124             |  8000                 |  128.99               |  17.801               |\n|  data loading                 |  0.0063443            |  8000                 |  50.754               |  7.004                |\n|  merging weights              |  0.050735             |  1000                 |  50.735               |  7.0013               |\n|  construct the wrapped model  |  7.2558               |  1                    |  7.2558               |  1.0013               |\n|  optimizer step               |  0.00098186           |  1000                 |  0.98186              |  0.13549              |\n----------------------------------------------------------------------------------------------------------------------------------\n</code></pre>"},{"location":"algorithms/adamerging/#reference","title":"Reference","text":""},{"location":"algorithms/adamerging/#task-wise-adamerging","title":"Task-Wise AdaMerging","text":""},{"location":"algorithms/adamerging/#fusion_bench.method.adamerging.task_wise_adamerging","title":"<code>task_wise_adamerging</code>","text":""},{"location":"algorithms/adamerging/#fusion_bench.method.adamerging.task_wise_adamerging.TaskWiseAdaMergingAlgorithm","title":"<code>TaskWiseAdaMergingAlgorithm</code>","text":"<p>               Bases: <code>ModelFusionAlgorithm</code></p> Source code in <code>fusion_bench/method/adamerging/task_wise_adamerging.py</code> <pre><code>class TaskWiseAdaMergingAlgorithm(ModelFusionAlgorithm):\n    _fabric: L.Fabric = None\n\n    def __init__(self, algorithm_config: DictConfig):\n        super().__init__(algorithm_config)\n\n        if self._fabric is None and torch.cuda.is_available():\n            self._fabric = L.Fabric(devices=self.config.get(\"devices\", 1))\n            self._fabric.launch()\n\n    @torch.no_grad()\n    def construct_task_wise_merged_model(self, modelpool: ModelPool):\n        if self.config.weights is None:\n            task_wise_weight = get_task_wise_weights(\n                num_models=len(modelpool.model_names),\n                init_values=self.config.init_values,\n            )\n        else:\n            if isinstance(self.config.weights, str):\n                # self.config.weights is a path to a .np or .pt file\n                if self.config.weights.endswith(\".pt\"):\n                    task_wise_weight = torch.load(\n                        self.config.weights, map_location=\"cpu\"\n                    ).detach_()\n                elif self.config.weights.endswith(\".np\"):\n                    task_wise_weight = torch.from_numpy(\n                        np.load(self.config.weights)\n                    ).detach_()\n                else:\n                    raise ValueError(f\"Unsupported file format: {self.config.weights}\")\n            else:\n                try:\n                    task_wise_weight = torch.tensor(\n                        list(self.config.weights), dtype=torch.float32\n                    )\n                except ValueError:\n                    raise ValueError(\n                        f\"Unsupported weights format: {self.config.weights}\"\n                    )\n\n        pretrained_model = modelpool.load_model(\"_pretrained_\")\n        finetuned_models = [\n            modelpool.load_model(name) for name in modelpool.model_names\n        ]\n\n        module = TaskWiseMergedModel(\n            task_wise_weight=task_wise_weight,\n            pretrained_model=pretrained_model,\n            finetuned_models=finetuned_models,\n            clamp_weights=self.config.clamp_weights,\n            tie_weights=self.config.tie_weights,\n            strict=self.config.strict,\n        )\n        return module\n\n    def run(self, modelpool: ModelPool):\n        log.info(\"Fusing models using task-wise adaptive merging.\")\n        self.modelpool = modelpool\n\n        module = self.construct_task_wise_merged_model(modelpool)\n\n        if self.config.weights is not None:\n            # skip the test-time adaptation\n            return module.merge_and_unload()\n        else:\n            module = self.test_time_adaptation(module)\n            if self.config.get(\"save_merging_weights\", False):\n                torch.save(module.merge_weight, self.config.save_merging_weights)\n            return module.merge_and_unload()\n\n    def on_test_time_adaptation_start(self):\n        pass\n\n    @abstractmethod\n    def get_shuffled_test_loader_iter(self, task: str) -&gt; DataLoader:\n        pass\n\n    @abstractmethod\n    def compute_logits(self, module: nn.Module, batch, task: str) -&gt; Tensor:\n        \"\"\"\n        Compute the logits for the given batch and task.\n\n        Args:\n            module (nn.Module): The model module.\n            batch (tuple): A batch of input data.\n            task (str): The name of the task.\n\n        Returns:\n            Tensor: The classification logits for the batch.\n        \"\"\"\n        pass\n\n    def test_time_adaptation(self, module: TaskWiseMergedModel):\n        self.on_test_time_adaptation_start()\n\n        # configure optimizer\n        if self.config.optimizer == \"adam\":\n            optimizer = torch.optim.Adam([module.merge_weight], lr=self.config.lr)\n        else:\n            raise ValueError(f\"Unsupported optimizer: {self.config.optimizer}\")\n\n        if self._fabric is not None:\n            module, optimizer = self._fabric.setup(module, optimizer)\n\n        module.train()\n        module.merge_weights()\n\n        if self.config.get(\"fast_dev_run\", False):\n            log.info(\"Running fast_dev_run, only one step\")\n            pbar = tqdm(\n                range(1),\n                \"AdaMerging Test-time adaptation\",\n                dynamic_ncols=True,\n            )\n        else:\n            pbar = tqdm(\n                range(self.config.max_steps),\n                \"AdaMerging Test-time adaptation\",\n                dynamic_ncols=True,\n            )\n        for step_idx in pbar:\n            for task in self.modelpool.model_names:\n                batch = next(self.get_shuffled_test_loader_iter(task))\n                logits = self.compute_logits(module, batch, task)\n                assert (\n                    logits.dim() == 2\n                ), f\"Expected logits to be 2D, got {logits.dim()}\"\n                loss = entropy_loss(logits)\n                # .backward() accumulates when .zero_grad() wasn't called\n                # this can save memory\n                self._fabric.backward(loss, retain_graph=True)\n\n            optimizer.step()\n            optimizer.zero_grad()\n            module.merge_weights()\n\n        return module\n</code></pre>"},{"location":"algorithms/adamerging/#fusion_bench.method.adamerging.task_wise_adamerging.TaskWiseAdaMergingAlgorithm.compute_logits","title":"<code>compute_logits(module, batch, task)</code>  <code>abstractmethod</code>","text":"<p>Compute the logits for the given batch and task.</p> <p>Parameters:</p> <ul> <li> <code>module</code> \u00b6              (<code>Module</code>)           \u2013            <p>The model module.</p> </li> <li> <code>batch</code> \u00b6              (<code>tuple</code>)           \u2013            <p>A batch of input data.</p> </li> <li> <code>task</code> \u00b6              (<code>str</code>)           \u2013            <p>The name of the task.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>Tensor</code> )          \u2013            <p>The classification logits for the batch.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/task_wise_adamerging.py</code> <pre><code>@abstractmethod\ndef compute_logits(self, module: nn.Module, batch, task: str) -&gt; Tensor:\n    \"\"\"\n    Compute the logits for the given batch and task.\n\n    Args:\n        module (nn.Module): The model module.\n        batch (tuple): A batch of input data.\n        task (str): The name of the task.\n\n    Returns:\n        Tensor: The classification logits for the batch.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"algorithms/adamerging/#fusion_bench.method.adamerging.task_wise_adamerging.entropy_loss","title":"<code>entropy_loss(logits)</code>","text":"<p>Compute the entropy loss of a set of logits.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>Tensor</code> )          \u2013            <p>The entropy loss of the logits.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/task_wise_adamerging.py</code> <pre><code>def entropy_loss(logits: Tensor) -&gt; Tensor:\n    \"\"\"\n    Compute the entropy loss of a set of logits.\n\n    Args:\n        logits (Tensor): The logits to compute the entropy loss of.\n\n    Returns:\n        Tensor: The entropy loss of the logits.\n    \"\"\"\n    probs = torch.softmax(logits, dim=-1)\n    return -torch.sum(probs * torch.log(probs + 1e-8), dim=-1).mean()\n</code></pre>"},{"location":"algorithms/adamerging/#fusion_bench.method.adamerging.task_wise_adamerging.entropy_loss(logits)","title":"<code>logits</code>","text":"(<code>Tensor</code>)           \u2013            <p>The logits to compute the entropy loss of.</p>"},{"location":"algorithms/adamerging/#fusion_bench.method.adamerging.clip_task_wise_adamerging","title":"<code>clip_task_wise_adamerging</code>","text":""},{"location":"algorithms/adamerging/#fusion_bench.method.adamerging.clip_task_wise_adamerging.CLIPTaskWiseAdaMergingAlgorithm","title":"<code>CLIPTaskWiseAdaMergingAlgorithm</code>","text":"<p>               Bases: <code>TaskWiseAdaMergingAlgorithm</code></p> <p>A class for task-wise adaptive merging of CLIP models.</p> <p>This class extends the TaskWiseAdaMergingAlgorithm to provide specific functionality for CLIP models, including loading datasets, constructing zero-shot classification heads, and computing logits.</p> <p>Attributes:</p> <ul> <li> <code>modelpool</code>               (<code>CLIPVisionModelPool</code>)           \u2013            <p>The model pool containing CLIP models.</p> </li> <li> <code>_clip_processor</code>               (<code>CLIPProcessor</code>)           \u2013            <p>The CLIP processor for preparing inputs.</p> </li> <li> <code>zeroshot_weights</code>               (<code>dict</code>)           \u2013            <p>A dictionary to store zero-shot weights for each task.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/clip_task_wise_adamerging.py</code> <pre><code>class CLIPTaskWiseAdaMergingAlgorithm(TaskWiseAdaMergingAlgorithm):\n    \"\"\"\n    A class for task-wise adaptive merging of CLIP models.\n\n    This class extends the TaskWiseAdaMergingAlgorithm to provide specific\n    functionality for CLIP models, including loading datasets, constructing\n    zero-shot classification heads, and computing logits.\n\n    Attributes:\n        modelpool (CLIPVisionModelPool): The model pool containing CLIP models.\n        _clip_processor (CLIPProcessor): The CLIP processor for preparing inputs.\n        zeroshot_weights (dict): A dictionary to store zero-shot weights for each task.\n    \"\"\"\n\n    modelpool: CLIPVisionModelPool = None\n    _clip_processor: CLIPProcessor = None\n    zeroshot_weights = {}\n\n    def __init__(self, algorithm_config: DictConfig):\n        super().__init__(algorithm_config)\n\n    @functools.cache\n    def get_test_dataset(self, task: str):\n        \"\"\"\n        Load the test dataset for the task.\n        This method is cached, so the dataset is loaded only once.\n\n        Args:\n            task (str): The name of the task.\n\n        Returns:\n            CLIPDataset: The test dataset for the task.\n        \"\"\"\n        log.info(f\"Loading test dataset: {task}\")\n        dataset = self.modelpool.load_test_dataset(task)\n        dataset = CLIPDataset(dataset, self._clip_processor)\n        return dataset\n\n    @functools.cache\n    def get_shuffled_test_loader_iter(self, task: str):\n        \"\"\"\n        Get an iterator over the shuffled test DataLoader for the task.\n\n        Args:\n            task (str): The name of the task.\n\n        Returns:\n            iterator: An iterator over the shuffled test DataLoader.\n        \"\"\"\n        loader = DataLoader(\n            self.get_test_dataset(task),\n            batch_size=self.config.batch_size,\n            shuffle=True,\n            num_workers=self.config.num_workers,\n            pin_memory=True,\n        )\n        if self._fabric is not None:\n            loader = self._fabric.setup_dataloaders(loader)\n        return iter(InfiniteDataLoader(loader))\n\n    def on_test_time_adaptation_start(self):\n        \"\"\"\n        Prepare for test-time adaptation.\n\n        This method loads the CLIP processor and constructs the zero-shot\n        classification head for each task.\n        \"\"\"\n        clip_model_config = self.modelpool.get_model_config(\"_pretrained_\")\n        pretrained_path = (\n            clip_model_config.pretrained_model_name_or_path\n            if hasattr(clip_model_config, \"pretrained_model_name_or_path\")\n            else clip_model_config.path\n        )\n\n        with timeit_context(\"Loading CLIP processor and pretrained CLIP model.\"):\n            self._clip_processor = CLIPProcessor.from_pretrained(pretrained_path)\n            clip_model: CLIPModel = CLIPModel.from_pretrained(pretrained_path)\n\n            clip_classifier = HFCLIPClassifier(clip_model, self._clip_processor)\n            self.visual_projection = clip_model.visual_projection.requires_grad_(False)\n            self.logit_scale_exp = clip_model.logit_scale.exp()\n            if self._fabric is not None:\n                self.visual_projection = self._fabric.to_device(self.visual_projection)\n                self.logit_scale_exp = self._fabric.to_device(self.logit_scale_exp)\n\n        for task in self.modelpool.model_names:\n            cache_file = os.path.join(\n                self.config.cache_dir,\n                f\"{os.path.basename(pretrained_path)}_{task}_zeroshot_weights.pt\",\n            )\n            if os.path.exists(cache_file):\n                log.info(f\"Loading cached zeroshot weights for task: {task}\")\n                zeroshot_weights = torch.load(cache_file, map_location=\"cpu\")\n            else:\n                log.info(f\"Construct zero shot classification head for task: {task}\")\n                classnames, templates = get_classnames_and_templates(task)\n                clip_classifier.set_classification_task(classnames, templates)\n                zeroshot_weights = clip_classifier.zeroshot_weights\n                log.info(f\"save zeroshot weights to {cache_file}\")\n                torch.save(zeroshot_weights, cache_file)\n            self.zeroshot_weights[task] = zeroshot_weights\n            if self._fabric is not None:\n                self.zeroshot_weights[task] = self._fabric.to_device(\n                    self.zeroshot_weights[task]\n                )\n\n    def compute_logits(self, module, batch, task: str) -&gt; Tensor:\n        \"\"\"\n        Compute the logits for the given batch and task.\n\n        This method computes the image embeddings, normalizes them, and calculates\n        the cosine similarity with the text embeddings to produce classification logits.\n\n        Args:\n            module (nn.Module): The model module.\n            batch (tuple): A batch of input data.\n            task (str): The name of the task.\n\n        Returns:\n            Tensor: The classification logits for the batch.\n        \"\"\"\n        images, _ = batch\n        text_embeds = self.zeroshot_weights[task]\n\n        image_embeds = module(images)[1]\n        image_embeds = self.visual_projection(image_embeds)\n\n        # normalize embeddings\n        image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n\n        # cosine similarity\n        logits_per_text = (\n            torch.matmul(text_embeds, image_embeds.t()) * self.logit_scale_exp\n        )\n        logits_per_image = logits_per_text.t()\n\n        return logits_per_image\n</code></pre>"},{"location":"algorithms/adamerging/#fusion_bench.method.adamerging.clip_task_wise_adamerging.CLIPTaskWiseAdaMergingAlgorithm.compute_logits","title":"<code>compute_logits(module, batch, task)</code>","text":"<p>Compute the logits for the given batch and task.</p> <p>This method computes the image embeddings, normalizes them, and calculates the cosine similarity with the text embeddings to produce classification logits.</p> <p>Parameters:</p> <ul> <li> <code>module</code> \u00b6              (<code>Module</code>)           \u2013            <p>The model module.</p> </li> <li> <code>batch</code> \u00b6              (<code>tuple</code>)           \u2013            <p>A batch of input data.</p> </li> <li> <code>task</code> \u00b6              (<code>str</code>)           \u2013            <p>The name of the task.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>Tensor</code> )          \u2013            <p>The classification logits for the batch.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/clip_task_wise_adamerging.py</code> <pre><code>def compute_logits(self, module, batch, task: str) -&gt; Tensor:\n    \"\"\"\n    Compute the logits for the given batch and task.\n\n    This method computes the image embeddings, normalizes them, and calculates\n    the cosine similarity with the text embeddings to produce classification logits.\n\n    Args:\n        module (nn.Module): The model module.\n        batch (tuple): A batch of input data.\n        task (str): The name of the task.\n\n    Returns:\n        Tensor: The classification logits for the batch.\n    \"\"\"\n    images, _ = batch\n    text_embeds = self.zeroshot_weights[task]\n\n    image_embeds = module(images)[1]\n    image_embeds = self.visual_projection(image_embeds)\n\n    # normalize embeddings\n    image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n\n    # cosine similarity\n    logits_per_text = (\n        torch.matmul(text_embeds, image_embeds.t()) * self.logit_scale_exp\n    )\n    logits_per_image = logits_per_text.t()\n\n    return logits_per_image\n</code></pre>"},{"location":"algorithms/adamerging/#fusion_bench.method.adamerging.clip_task_wise_adamerging.CLIPTaskWiseAdaMergingAlgorithm.get_shuffled_test_loader_iter","title":"<code>get_shuffled_test_loader_iter(task)</code>  <code>cached</code>","text":"<p>Get an iterator over the shuffled test DataLoader for the task.</p> <p>Parameters:</p> <ul> <li> <code>task</code> \u00b6              (<code>str</code>)           \u2013            <p>The name of the task.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>iterator</code>          \u2013            <p>An iterator over the shuffled test DataLoader.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/clip_task_wise_adamerging.py</code> <pre><code>@functools.cache\ndef get_shuffled_test_loader_iter(self, task: str):\n    \"\"\"\n    Get an iterator over the shuffled test DataLoader for the task.\n\n    Args:\n        task (str): The name of the task.\n\n    Returns:\n        iterator: An iterator over the shuffled test DataLoader.\n    \"\"\"\n    loader = DataLoader(\n        self.get_test_dataset(task),\n        batch_size=self.config.batch_size,\n        shuffle=True,\n        num_workers=self.config.num_workers,\n        pin_memory=True,\n    )\n    if self._fabric is not None:\n        loader = self._fabric.setup_dataloaders(loader)\n    return iter(InfiniteDataLoader(loader))\n</code></pre>"},{"location":"algorithms/adamerging/#fusion_bench.method.adamerging.clip_task_wise_adamerging.CLIPTaskWiseAdaMergingAlgorithm.get_test_dataset","title":"<code>get_test_dataset(task)</code>  <code>cached</code>","text":"<p>Load the test dataset for the task. This method is cached, so the dataset is loaded only once.</p> <p>Parameters:</p> <ul> <li> <code>task</code> \u00b6              (<code>str</code>)           \u2013            <p>The name of the task.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>CLIPDataset</code>          \u2013            <p>The test dataset for the task.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/clip_task_wise_adamerging.py</code> <pre><code>@functools.cache\ndef get_test_dataset(self, task: str):\n    \"\"\"\n    Load the test dataset for the task.\n    This method is cached, so the dataset is loaded only once.\n\n    Args:\n        task (str): The name of the task.\n\n    Returns:\n        CLIPDataset: The test dataset for the task.\n    \"\"\"\n    log.info(f\"Loading test dataset: {task}\")\n    dataset = self.modelpool.load_test_dataset(task)\n    dataset = CLIPDataset(dataset, self._clip_processor)\n    return dataset\n</code></pre>"},{"location":"algorithms/adamerging/#fusion_bench.method.adamerging.clip_task_wise_adamerging.CLIPTaskWiseAdaMergingAlgorithm.on_test_time_adaptation_start","title":"<code>on_test_time_adaptation_start()</code>","text":"<p>Prepare for test-time adaptation.</p> <p>This method loads the CLIP processor and constructs the zero-shot classification head for each task.</p> Source code in <code>fusion_bench/method/adamerging/clip_task_wise_adamerging.py</code> <pre><code>def on_test_time_adaptation_start(self):\n    \"\"\"\n    Prepare for test-time adaptation.\n\n    This method loads the CLIP processor and constructs the zero-shot\n    classification head for each task.\n    \"\"\"\n    clip_model_config = self.modelpool.get_model_config(\"_pretrained_\")\n    pretrained_path = (\n        clip_model_config.pretrained_model_name_or_path\n        if hasattr(clip_model_config, \"pretrained_model_name_or_path\")\n        else clip_model_config.path\n    )\n\n    with timeit_context(\"Loading CLIP processor and pretrained CLIP model.\"):\n        self._clip_processor = CLIPProcessor.from_pretrained(pretrained_path)\n        clip_model: CLIPModel = CLIPModel.from_pretrained(pretrained_path)\n\n        clip_classifier = HFCLIPClassifier(clip_model, self._clip_processor)\n        self.visual_projection = clip_model.visual_projection.requires_grad_(False)\n        self.logit_scale_exp = clip_model.logit_scale.exp()\n        if self._fabric is not None:\n            self.visual_projection = self._fabric.to_device(self.visual_projection)\n            self.logit_scale_exp = self._fabric.to_device(self.logit_scale_exp)\n\n    for task in self.modelpool.model_names:\n        cache_file = os.path.join(\n            self.config.cache_dir,\n            f\"{os.path.basename(pretrained_path)}_{task}_zeroshot_weights.pt\",\n        )\n        if os.path.exists(cache_file):\n            log.info(f\"Loading cached zeroshot weights for task: {task}\")\n            zeroshot_weights = torch.load(cache_file, map_location=\"cpu\")\n        else:\n            log.info(f\"Construct zero shot classification head for task: {task}\")\n            classnames, templates = get_classnames_and_templates(task)\n            clip_classifier.set_classification_task(classnames, templates)\n            zeroshot_weights = clip_classifier.zeroshot_weights\n            log.info(f\"save zeroshot weights to {cache_file}\")\n            torch.save(zeroshot_weights, cache_file)\n        self.zeroshot_weights[task] = zeroshot_weights\n        if self._fabric is not None:\n            self.zeroshot_weights[task] = self._fabric.to_device(\n                self.zeroshot_weights[task]\n            )\n</code></pre>"},{"location":"algorithms/adamerging/#fusion_bench.method.adamerging.clip_task_wise_adamerging.InfiniteDataLoader","title":"<code>InfiniteDataLoader</code>","text":"<p>A wrapper class for DataLoader to create an infinite data loader. This is useful in case we are only interested in the number of steps and not the number of epochs.</p> <p>This class wraps a DataLoader and provides an iterator that resets when the end of the dataset is reached, creating an infinite loop.</p> <p>Attributes:</p> <ul> <li> <code>data_loader</code>               (<code>DataLoader</code>)           \u2013            <p>The DataLoader to wrap.</p> </li> <li> <code>data_iter</code>               (<code>iterator</code>)           \u2013            <p>An iterator over the DataLoader.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/clip_task_wise_adamerging.py</code> <pre><code>class InfiniteDataLoader:\n    \"\"\"\n    A wrapper class for DataLoader to create an infinite data loader.\n    This is useful in case we are only interested in the number of steps and not the number of epochs.\n\n    This class wraps a DataLoader and provides an iterator that resets\n    when the end of the dataset is reached, creating an infinite loop.\n\n    Attributes:\n        data_loader (DataLoader): The DataLoader to wrap.\n        data_iter (iterator): An iterator over the DataLoader.\n    \"\"\"\n\n    def __init__(self, data_loader):\n        self.data_loader = data_loader\n        self.data_iter = iter(data_loader)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        try:\n            data = next(self.data_iter)\n        except StopIteration:\n            self.data_iter = iter(self.data_loader)  # Reset the data loader\n            data = next(self.data_iter)\n        return data\n</code></pre>"},{"location":"algorithms/adamerging/#layer-wise-adamerging","title":"Layer-Wise AdaMerging","text":"<ol> <li> <p>(ICLR 2024) AdaMerging: Adaptive Model Merging for Multi-Task Learning. https://openreview.net/pdf?id=nZP6NgD3QY\u00a0\u21a9</p> </li> <li> <p>Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? Advances in neural information processing systems, 27, 2014.\u00a0\u21a9</p> </li> <li> <p>A. Tang, L. Shen, Y. Luo, N. Yin, L. Zhang, and D. Tao, \u201cMerging Multi-Task Models via Weight-Ensembling Mixture of Experts,\u201d ICML 2024. doi: 10.48550/arXiv.2402.00433.\u00a0\u21a9</p> </li> </ol>"},{"location":"algorithms/adamerging/#fusion_bench.method.adamerging.layer_wise_adamerging","title":"<code>layer_wise_adamerging</code>","text":""},{"location":"algorithms/adamerging/#fusion_bench.method.adamerging.layer_wise_adamerging.LayerWiseAdaMergingAlgorithm","title":"<code>LayerWiseAdaMergingAlgorithm</code>","text":"<p>               Bases: <code>ModelFusionAlgorithm</code>, <code>LightningFabricMixin</code>, <code>SimpleProfilerMixin</code></p> Source code in <code>fusion_bench/method/adamerging/layer_wise_adamerging.py</code> <pre><code>class LayerWiseAdaMergingAlgorithm(\n    ModelFusionAlgorithm,\n    LightningFabricMixin,\n    SimpleProfilerMixin,\n):\n    _program: \"FabricModelFusionProgram\"\n    \"\"\"The program that this algorithm is running on.\"\"\"\n\n    \"\"\"\n    Implements the Layer-Wise AdaMerging Algorithm.\n\n    This class merges the layers of a pretrained model with those of several fine-tuned models.\n    The merging is controlled by layer-wise weights, which can be initialized based on a provided configuration or loaded from a file.\n    \"\"\"\n\n    def __init__(self, algorithm_config: DictConfig):\n        \"\"\"\n        Initialize the LayerWiseAdaMergingAlgorithm with the given configuration.\n\n        Args:\n            algorithm_config (DictConfig): The configuration for the algorithm.\n        \"\"\"\n        super().__init__(algorithm_config)\n\n    @torch.no_grad()\n    def construct_layer_wise_merged_model(self, modelpool: \"ModelPool\"):\n        \"\"\"\n        Constructs a wrapped layer-wise merged model from model pool.\n\n        This method creates a new wrapped model by merging the layers of a pretrained model with those of several fine-tuned models.\n        The merging is controlled by layer-wise weights, which is a `torch.Tensor` of the shape `(num_models, num_layers)`.\n        The merging weights can be initialized based on a provided configuration or loaded from a file.\n\n        Args:\n            modelpool (ModelPool): An object containing the pretrained model and fine-tuned models to be merged.\n\n        Returns:\n            LayerWiseMergedModel: An instance of the merged model with layer-wise weights applied.\n        \"\"\"\n        pretrained_model = modelpool.load_model(\"_pretrained_\")\n        finetuned_models = [\n            modelpool.load_model(name) for name in modelpool.model_names\n        ]\n\n        # initialize layer-wise weights using the provided configuration `init_values` or load from file if `weights` is provided\n        if self.config.weights is None:\n            layer_wise_weight = get_layer_wise_weights(\n                num_models=len(modelpool.model_names),\n                num_layers=len(\n                    tuple(\n                        filter(lambda p: p.requires_grad, pretrained_model.parameters())\n                    )\n                ),\n                init_values=self.config.init_values,\n            )\n        else:\n            if isinstance(self.config.weights, str):\n                # self.config.weights is a path to a saved tensor\n                layer_wise_weight = load_tensor_from_file(self.config.weights)\n            else:\n                raise ValueError(f\"Unsupported weights format: {self.config.weights}\")\n\n        module = LayerWiseMergedModel(\n            layer_wise_weight=layer_wise_weight,\n            pretrained_model=pretrained_model,\n            finetuned_models=finetuned_models,\n            clamp_weights=self.config.clamp_weights,\n            tie_weights=self.config.tie_weights,\n            strict=self.config.strict,\n        )\n        print(f\"{layer_wise_weight.size()=}, {layer_wise_weight.numel()=}\")\n        return module\n\n    @rank_zero_only\n    def save_merging_weights(self, file_path: str, merging_weights: torch.Tensor):\n        \"\"\"\n        Save the merging weights to a file.\n\n        Args:\n            file_path (str): The path to save the merging weights.\n            merging_weights (torch.Tensor): The merging weights to save.\n        \"\"\"\n        if self.fabric.is_global_zero and self.config.get(\n            \"save_merging_weights\", False\n        ):\n            if isinstance(file_path, str) and not file_path.startswith((\"/\", \".\")):\n                # if the file path is not absolute or relative to current working directory, save it in the log directory\n                save_path = os.path.join(self.log_dir, file_path)\n            else:\n                save_path = file_path\n            log.info(f\"saving merging weights to {save_path}.\")\n            if os.path.dirname(save_path):\n                os.makedirs(os.path.dirname(save_path), exist_ok=True)\n            torch.save(merging_weights.detach().cpu(), save_path)\n\n    def run(self, modelpool: ModelPool, **kwargs):\n        \"\"\"\n        Run the Layer-Wise AdaMerging Algorithm.\n\n        This method constructs the wrapped model and performs test-time adaptation if necessary.\n\n        Args:\n            modelpool (ModelPool): The model pool containing the pretrained and fine-tuned models.\n\n        Returns:\n            LayerWiseMergedModel: The merged model after test-time adaptation.\n        \"\"\"\n        log.info(\"Fusing models using layer-wise adaptive merging.\")\n        self.modelpool = modelpool\n        self.log_hyperparams(self.config)\n\n        with self.profile(\"construct the wrapped model\"):\n            module = self.construct_layer_wise_merged_model(modelpool)\n\n        if self.config.weights is not None:\n            # skip the test-time adaptation\n            return module.merge_and_unload()\n        else:\n            with self.profile(\"test-time adaptation\"):\n                module = self.test_time_adaptation(module)\n            if self.config.get(\"save_merging_weights\", False):\n                self.save_merging_weights(\n                    self.config.save_merging_weights, module.merge_weight\n                )\n            return module.merge_and_unload()\n\n    def on_test_time_adaptation_start(self):\n        \"\"\"\n        Something to do before the test-time adaptation starts. Such as setting up the task-specific heads.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_shuffled_test_loader_iter(self, task: str) -&gt; DataLoader:\n        \"\"\"\n        Loader of test dataset for test-time adaptation. labels are not needed.\n\n        Args:\n            task (str): The name of the task.\n\n        Returns:\n            DataLoader: The data loader for the test dataset.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def compute_logits(self, module, images: Tensor, task: str) -&gt; Tensor:\n        \"\"\"\n        Compute the logits for the given images and task.\n\n        Args:\n            module: The model module.\n            images (Tensor): The input images.\n            task (str): The name of the task.\n\n        Returns:\n            Tensor: The computed logits.\n        \"\"\"\n        pass\n\n    def test_time_adaptation(self, module: \"LayerWiseMergedModel[TorchModelType]\"):\n        \"\"\"\n        Perform test-time adaptation on the merged model.\n\n        This method adapts the merging weights during test-time to improve performance.\n\n        Args:\n            module (LayerWiseMergedModel): The merged model.\n\n        Returns:\n            LayerWiseMergedModel: The adapted merged model.\n        \"\"\"\n        self.on_test_time_adaptation_start()\n\n        # configure optimizer\n        if self.config.optimizer == \"adam\":\n            optimizer = torch.optim.Adam([module.merge_weight], lr=self.config.lr)\n            print(f\"{optimizer=}\")\n            module, optimizer = self.fabric.setup(module, optimizer)\n        else:\n            raise ValueError(f\"Unsupported optimizer: {self.config.optimizer}\")\n\n        module.train()\n        module.merge_weights()\n        for step_idx in (\n            pbar := tqdm(\n                range(self.config.max_steps if not self.is_debug_mode else 1),\n                (\"[DEBUG MODE] \" if self.is_debug_mode else \"\")\n                + \"AdaMerging Test-time adaptation\",\n                dynamic_ncols=True,\n            )\n        ):\n            # default behavior for first-order optimizers\n            for task in self.modelpool.model_names:\n                with self.profile(\"data loading\"):\n                    batch = next(self.get_shuffled_test_loader_iter(task))\n                with self.profile(\"forward pass\"):\n                    logits = self.compute_logits(module, batch[0], task)\n                    loss = entropy_loss(logits)\n                with self.profile(\"backward pass\"):\n                    self.fabric.backward(loss, retain_graph=True)\n\n            with self.profile(\"optimizer step\"):\n                optimizer.step()\n                optimizer.zero_grad()\n            with self.profile(\"merging weights\"):\n                module.merge_weights()\n\n            metrics = {\n                \"train/loss\": loss.item(),\n                \"train/weight_max\": module.merge_weight.max().item(),\n                \"train/weight_min\": module.merge_weight.min().item(),\n                \"train/weight_mean\": module.merge_weight.mean().item(),\n            }\n            self.fabric.log_dict(metrics, step=step_idx)\n            pbar.set_postfix(metrics)\n\n        log.info(get_memory_usage(f\"after adamerging, the memory usage of GPU is:\"))\n        self.print_profile_summary()\n        return module\n</code></pre>"},{"location":"algorithms/adamerging/#fusion_bench.method.adamerging.layer_wise_adamerging.LayerWiseAdaMergingAlgorithm.__init__","title":"<code>__init__(algorithm_config)</code>","text":"<p>Initialize the LayerWiseAdaMergingAlgorithm with the given configuration.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_config</code> \u00b6              (<code>DictConfig</code>)           \u2013            <p>The configuration for the algorithm.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/layer_wise_adamerging.py</code> <pre><code>def __init__(self, algorithm_config: DictConfig):\n    \"\"\"\n    Initialize the LayerWiseAdaMergingAlgorithm with the given configuration.\n\n    Args:\n        algorithm_config (DictConfig): The configuration for the algorithm.\n    \"\"\"\n    super().__init__(algorithm_config)\n</code></pre>"},{"location":"algorithms/adamerging/#fusion_bench.method.adamerging.layer_wise_adamerging.LayerWiseAdaMergingAlgorithm.compute_logits","title":"<code>compute_logits(module, images, task)</code>  <code>abstractmethod</code>","text":"<p>Compute the logits for the given images and task.</p> <p>Parameters:</p> <ul> <li> <code>module</code> \u00b6          \u2013            <p>The model module.</p> </li> <li> <code>images</code> \u00b6              (<code>Tensor</code>)           \u2013            <p>The input images.</p> </li> <li> <code>task</code> \u00b6              (<code>str</code>)           \u2013            <p>The name of the task.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>Tensor</code> )          \u2013            <p>The computed logits.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/layer_wise_adamerging.py</code> <pre><code>@abstractmethod\ndef compute_logits(self, module, images: Tensor, task: str) -&gt; Tensor:\n    \"\"\"\n    Compute the logits for the given images and task.\n\n    Args:\n        module: The model module.\n        images (Tensor): The input images.\n        task (str): The name of the task.\n\n    Returns:\n        Tensor: The computed logits.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"algorithms/adamerging/#fusion_bench.method.adamerging.layer_wise_adamerging.LayerWiseAdaMergingAlgorithm.construct_layer_wise_merged_model","title":"<code>construct_layer_wise_merged_model(modelpool)</code>","text":"<p>Constructs a wrapped layer-wise merged model from model pool.</p> <p>This method creates a new wrapped model by merging the layers of a pretrained model with those of several fine-tuned models. The merging is controlled by layer-wise weights, which is a <code>torch.Tensor</code> of the shape <code>(num_models, num_layers)</code>. The merging weights can be initialized based on a provided configuration or loaded from a file.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code> \u00b6              (<code>ModelPool</code>)           \u2013            <p>An object containing the pretrained model and fine-tuned models to be merged.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LayerWiseMergedModel</code>          \u2013            <p>An instance of the merged model with layer-wise weights applied.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/layer_wise_adamerging.py</code> <pre><code>@torch.no_grad()\ndef construct_layer_wise_merged_model(self, modelpool: \"ModelPool\"):\n    \"\"\"\n    Constructs a wrapped layer-wise merged model from model pool.\n\n    This method creates a new wrapped model by merging the layers of a pretrained model with those of several fine-tuned models.\n    The merging is controlled by layer-wise weights, which is a `torch.Tensor` of the shape `(num_models, num_layers)`.\n    The merging weights can be initialized based on a provided configuration or loaded from a file.\n\n    Args:\n        modelpool (ModelPool): An object containing the pretrained model and fine-tuned models to be merged.\n\n    Returns:\n        LayerWiseMergedModel: An instance of the merged model with layer-wise weights applied.\n    \"\"\"\n    pretrained_model = modelpool.load_model(\"_pretrained_\")\n    finetuned_models = [\n        modelpool.load_model(name) for name in modelpool.model_names\n    ]\n\n    # initialize layer-wise weights using the provided configuration `init_values` or load from file if `weights` is provided\n    if self.config.weights is None:\n        layer_wise_weight = get_layer_wise_weights(\n            num_models=len(modelpool.model_names),\n            num_layers=len(\n                tuple(\n                    filter(lambda p: p.requires_grad, pretrained_model.parameters())\n                )\n            ),\n            init_values=self.config.init_values,\n        )\n    else:\n        if isinstance(self.config.weights, str):\n            # self.config.weights is a path to a saved tensor\n            layer_wise_weight = load_tensor_from_file(self.config.weights)\n        else:\n            raise ValueError(f\"Unsupported weights format: {self.config.weights}\")\n\n    module = LayerWiseMergedModel(\n        layer_wise_weight=layer_wise_weight,\n        pretrained_model=pretrained_model,\n        finetuned_models=finetuned_models,\n        clamp_weights=self.config.clamp_weights,\n        tie_weights=self.config.tie_weights,\n        strict=self.config.strict,\n    )\n    print(f\"{layer_wise_weight.size()=}, {layer_wise_weight.numel()=}\")\n    return module\n</code></pre>"},{"location":"algorithms/adamerging/#fusion_bench.method.adamerging.layer_wise_adamerging.LayerWiseAdaMergingAlgorithm.get_shuffled_test_loader_iter","title":"<code>get_shuffled_test_loader_iter(task)</code>  <code>abstractmethod</code>","text":"<p>Loader of test dataset for test-time adaptation. labels are not needed.</p> <p>Parameters:</p> <ul> <li> <code>task</code> \u00b6              (<code>str</code>)           \u2013            <p>The name of the task.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code> (              <code>DataLoader</code> )          \u2013            <p>The data loader for the test dataset.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/layer_wise_adamerging.py</code> <pre><code>@abstractmethod\ndef get_shuffled_test_loader_iter(self, task: str) -&gt; DataLoader:\n    \"\"\"\n    Loader of test dataset for test-time adaptation. labels are not needed.\n\n    Args:\n        task (str): The name of the task.\n\n    Returns:\n        DataLoader: The data loader for the test dataset.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"algorithms/adamerging/#fusion_bench.method.adamerging.layer_wise_adamerging.LayerWiseAdaMergingAlgorithm.on_test_time_adaptation_start","title":"<code>on_test_time_adaptation_start()</code>","text":"<p>Something to do before the test-time adaptation starts. Such as setting up the task-specific heads.</p> Source code in <code>fusion_bench/method/adamerging/layer_wise_adamerging.py</code> <pre><code>def on_test_time_adaptation_start(self):\n    \"\"\"\n    Something to do before the test-time adaptation starts. Such as setting up the task-specific heads.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"algorithms/adamerging/#fusion_bench.method.adamerging.layer_wise_adamerging.LayerWiseAdaMergingAlgorithm.run","title":"<code>run(modelpool, **kwargs)</code>","text":"<p>Run the Layer-Wise AdaMerging Algorithm.</p> <p>This method constructs the wrapped model and performs test-time adaptation if necessary.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code> \u00b6              (<code>ModelPool</code>)           \u2013            <p>The model pool containing the pretrained and fine-tuned models.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LayerWiseMergedModel</code>          \u2013            <p>The merged model after test-time adaptation.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/layer_wise_adamerging.py</code> <pre><code>def run(self, modelpool: ModelPool, **kwargs):\n    \"\"\"\n    Run the Layer-Wise AdaMerging Algorithm.\n\n    This method constructs the wrapped model and performs test-time adaptation if necessary.\n\n    Args:\n        modelpool (ModelPool): The model pool containing the pretrained and fine-tuned models.\n\n    Returns:\n        LayerWiseMergedModel: The merged model after test-time adaptation.\n    \"\"\"\n    log.info(\"Fusing models using layer-wise adaptive merging.\")\n    self.modelpool = modelpool\n    self.log_hyperparams(self.config)\n\n    with self.profile(\"construct the wrapped model\"):\n        module = self.construct_layer_wise_merged_model(modelpool)\n\n    if self.config.weights is not None:\n        # skip the test-time adaptation\n        return module.merge_and_unload()\n    else:\n        with self.profile(\"test-time adaptation\"):\n            module = self.test_time_adaptation(module)\n        if self.config.get(\"save_merging_weights\", False):\n            self.save_merging_weights(\n                self.config.save_merging_weights, module.merge_weight\n            )\n        return module.merge_and_unload()\n</code></pre>"},{"location":"algorithms/adamerging/#fusion_bench.method.adamerging.layer_wise_adamerging.LayerWiseAdaMergingAlgorithm.save_merging_weights","title":"<code>save_merging_weights(file_path, merging_weights)</code>","text":"<p>Save the merging weights to a file.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code> \u00b6              (<code>str</code>)           \u2013            <p>The path to save the merging weights.</p> </li> <li> <code>merging_weights</code> \u00b6              (<code>Tensor</code>)           \u2013            <p>The merging weights to save.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/layer_wise_adamerging.py</code> <pre><code>@rank_zero_only\ndef save_merging_weights(self, file_path: str, merging_weights: torch.Tensor):\n    \"\"\"\n    Save the merging weights to a file.\n\n    Args:\n        file_path (str): The path to save the merging weights.\n        merging_weights (torch.Tensor): The merging weights to save.\n    \"\"\"\n    if self.fabric.is_global_zero and self.config.get(\n        \"save_merging_weights\", False\n    ):\n        if isinstance(file_path, str) and not file_path.startswith((\"/\", \".\")):\n            # if the file path is not absolute or relative to current working directory, save it in the log directory\n            save_path = os.path.join(self.log_dir, file_path)\n        else:\n            save_path = file_path\n        log.info(f\"saving merging weights to {save_path}.\")\n        if os.path.dirname(save_path):\n            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        torch.save(merging_weights.detach().cpu(), save_path)\n</code></pre>"},{"location":"algorithms/adamerging/#fusion_bench.method.adamerging.layer_wise_adamerging.LayerWiseAdaMergingAlgorithm.test_time_adaptation","title":"<code>test_time_adaptation(module)</code>","text":"<p>Perform test-time adaptation on the merged model.</p> <p>This method adapts the merging weights during test-time to improve performance.</p> <p>Parameters:</p> <ul> <li> <code>module</code> \u00b6              (<code>LayerWiseMergedModel</code>)           \u2013            <p>The merged model.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LayerWiseMergedModel</code>          \u2013            <p>The adapted merged model.</p> </li> </ul> Source code in <code>fusion_bench/method/adamerging/layer_wise_adamerging.py</code> <pre><code>def test_time_adaptation(self, module: \"LayerWiseMergedModel[TorchModelType]\"):\n    \"\"\"\n    Perform test-time adaptation on the merged model.\n\n    This method adapts the merging weights during test-time to improve performance.\n\n    Args:\n        module (LayerWiseMergedModel): The merged model.\n\n    Returns:\n        LayerWiseMergedModel: The adapted merged model.\n    \"\"\"\n    self.on_test_time_adaptation_start()\n\n    # configure optimizer\n    if self.config.optimizer == \"adam\":\n        optimizer = torch.optim.Adam([module.merge_weight], lr=self.config.lr)\n        print(f\"{optimizer=}\")\n        module, optimizer = self.fabric.setup(module, optimizer)\n    else:\n        raise ValueError(f\"Unsupported optimizer: {self.config.optimizer}\")\n\n    module.train()\n    module.merge_weights()\n    for step_idx in (\n        pbar := tqdm(\n            range(self.config.max_steps if not self.is_debug_mode else 1),\n            (\"[DEBUG MODE] \" if self.is_debug_mode else \"\")\n            + \"AdaMerging Test-time adaptation\",\n            dynamic_ncols=True,\n        )\n    ):\n        # default behavior for first-order optimizers\n        for task in self.modelpool.model_names:\n            with self.profile(\"data loading\"):\n                batch = next(self.get_shuffled_test_loader_iter(task))\n            with self.profile(\"forward pass\"):\n                logits = self.compute_logits(module, batch[0], task)\n                loss = entropy_loss(logits)\n            with self.profile(\"backward pass\"):\n                self.fabric.backward(loss, retain_graph=True)\n\n        with self.profile(\"optimizer step\"):\n            optimizer.step()\n            optimizer.zero_grad()\n        with self.profile(\"merging weights\"):\n            module.merge_weights()\n\n        metrics = {\n            \"train/loss\": loss.item(),\n            \"train/weight_max\": module.merge_weight.max().item(),\n            \"train/weight_min\": module.merge_weight.min().item(),\n            \"train/weight_mean\": module.merge_weight.mean().item(),\n        }\n        self.fabric.log_dict(metrics, step=step_idx)\n        pbar.set_postfix(metrics)\n\n    log.info(get_memory_usage(f\"after adamerging, the memory usage of GPU is:\"))\n    self.print_profile_summary()\n    return module\n</code></pre>"},{"location":"algorithms/adamerging/#fusion_bench.method.adamerging.clip_layer_wise_adamerging","title":"<code>clip_layer_wise_adamerging</code>","text":"<p>Example Usage:</p> <pre><code>fusion_bench     method=adamerging         method.name=clip_layer_wise_adamerging         method.save_merging_weights=merging_weights.pt     modelpool=clip-vit-base-patch32_TA8     taskpool=clip-vit-classification_TA8     fabric.loggers.root_dir=outputs/logs/ViT-B-32     fabric.loggers.name=clip_layer_wise_adamerging_adam\n</code></pre>"},{"location":"algorithms/adamerging/#fusion_bench.method.adamerging.clip_layer_wise_adamerging.CLIPLayerWiseAdaMergingAlgorithm","title":"<code>CLIPLayerWiseAdaMergingAlgorithm</code>","text":"<p>               Bases: <code>CLIPClassificationMixin</code>, <code>LayerWiseAdaMergingAlgorithm</code></p> Source code in <code>fusion_bench/method/adamerging/clip_layer_wise_adamerging.py</code> <pre><code>class CLIPLayerWiseAdaMergingAlgorithm(\n    CLIPClassificationMixin,\n    LayerWiseAdaMergingAlgorithm,\n):\n    def on_test_time_adaptation_start(self):\n        \"\"\"\n        Here we load the CLIP processor and construct the zero-shot classification head for each task.\n        \"\"\"\n        self.setup_zero_shot_classification_head()\n\n    @functools.cache\n    def get_shuffled_test_loader_iter(self, task: str):\n        return super().get_shuffled_test_loader_iter(\n            task,\n            batch_size=self.config.batch_size,\n            num_workers=self.config.num_workers,\n        )\n</code></pre>"},{"location":"algorithms/adamerging/#fusion_bench.method.adamerging.clip_layer_wise_adamerging.CLIPLayerWiseAdaMergingAlgorithm.on_test_time_adaptation_start","title":"<code>on_test_time_adaptation_start()</code>","text":"<p>Here we load the CLIP processor and construct the zero-shot classification head for each task.</p> Source code in <code>fusion_bench/method/adamerging/clip_layer_wise_adamerging.py</code> <pre><code>def on_test_time_adaptation_start(self):\n    \"\"\"\n    Here we load the CLIP processor and construct the zero-shot classification head for each task.\n    \"\"\"\n    self.setup_zero_shot_classification_head()\n</code></pre>"},{"location":"algorithms/concrete_subspace/","title":"Concrete Subspace Learning","text":"(a) Framework overview. Our proposed framework comprises two main steps: first, establishing a common subspace for task vectors across various tasks using a shared mask, and second, merging the models within this shared subspace. (b) Mask sampling. Here we illustrate the procedure for sampling discrete binary masks and our differentiable Concrete mask. It's important to note that while a Concrete mask can also be binarized, this binarization process is non-differentiable."},{"location":"algorithms/concrete_subspace/#contrete-masking","title":"Contrete Masking","text":""},{"location":"algorithms/concrete_subspace/#the-gumbel-max-trick","title":"The Gumbel-Max Trick","text":"<p>Consider a discrete categorical distribution parameterized by logits \\(\\mathbf{x} = (x_1, \\dots, x_n) \\in \\mathbb{R}^{n}\\), where \\(x_i\\) is the logit of the \\(i\\)-th category. The Gumbel-Max trick <sup>1</sup><sup>2</sup><sup>3</sup> states a reparameterization trick to sample from the categorical distribution by sampling from the standard Gumbel distribution \\(\\text{Gumbel}(\\mu=0,\\beta=1)\\) and taking the argmax of the sum of the Gumbel random variables and the logits.</p> <p>This trick proceeds as follows: sample \\(n\\) Gumbel random variables \\(g_1, \\dots, g_n\\) independently from the standard Gumbel distribution \\(\\text{Gumbel}(\\mu=0,\\beta=1)\\) (We can draw a random sample \\(u\\) from a unifrom distribution on the interval \\((0,1)\\) and then transform it into a Gumbel-distributed variable \\(g\\) using the formula \\(g=-\\log(-\\log u)\\).), find the index \\(i\\) of that maximizes \\(x_i + g_i\\), then we have</p> \\[   {\\arg\\max}_{i\\in[n]} (x_i + g_i) \\sim \\text{Categorical}(\\text{softmax}(\\mathbf{x})). \\] <p>If we represent the categorical distribution as a one-hot vector \\(\\mathbf{y} = (y_1, \\dots, y_n) \\in \\{0,1\\}^n\\), where \\(y_i=1\\) indicates that the \\(i\\)-th category is sampled and for all \\(j\\neq i\\), \\(y_j=0\\), then we have</p> \\[   \\mathbb{P}(y_k=1) = \\mathbb{P}\\left({\\arg\\max}_{i\\in[n]} (x_i + g_i) = k\\right) = \\frac{\\exp(x_k)}{\\sum_{i=1}^n \\exp(x_i)}. \\]"},{"location":"algorithms/concrete_subspace/#continuous-relaxation-of-the-discrete-categorical-distribution","title":"Continuous Relaxation of the discrete Categorical Distribution","text":"<p>Since the derivative of the \\({\\arg\\max}\\) function is not defined, we cannot backpropagate the gradients through it. To address this issue,  (Maddison et al., 2017)<sup>4</sup> proposed to use a continuous relaxation of the discrete categorical distribution. A CONCRETE random variable (CONtinuous relaxation of disCRETE random variable) relax the condition that the one-hot vector \\(\\mathbf{y}\\) must be located at the vertices of the \\((n-1)\\)-dimensional simplex \\(\\Delta^{n-1}\\), and instead, it allows \\(\\mathbf{y}\\) to be located anywhere inside the simplex \\(\\Delta^{n-1}\\), i.e. \\(\\{ y\\in \\mathbb{R}^n | y_i \\in [0,1], \\sum_{i=1}^n y_i =1 \\}\\).</p> <p>To sample a Concrete random variable \\(\\mathbf{y}\\) from a distribution that is parameterized by a temperature hyperparameter \\(\\lambda &gt; 0\\) and a vector of logits \\(\\mathbf{x} = (x_1, \\dots, x_n) \\in \\mathbb{R}^{n}\\), we have</p> \\[   \\mathbf{y} = \\text{softmax}\\left(\\frac{\\mathbf{x} + \\mathbf{g}}{\\lambda}\\right), \\quad   y_i = \\frac{\\exp\\left((x_i + g_i)/{\\lambda}\\right)}{\\sum_{j=1}^n \\exp\\left(({x_j + g_j})/{\\lambda}\\right)} \\quad \\text{for} \\,\\, i\\in[n]. \\] <p>where \\(\\mathbf{g} = (g_1, \\dots, g_n)\\) is a vector of Gumbel random variables that are independently sampled from the standard Gumbel distribution \\(\\text{Gumbel}(\\mu=0,\\beta=1)\\).</p>"},{"location":"algorithms/concrete_subspace/#concrete-masking","title":"Concrete Masking","text":"<p>A subspace mask \\(\\mathbf{m}\\) is a binary vector that identifies a subspace of the parameter space. For a neural network parametrized by \\(\\theta\\), we can use a subspace mask \\(\\mathbf{m}\\) to identify a subspace of the parameter space \\(\\mathbf{\\theta}\\) by setting the parameters that are not in the subspace to zero, i.e. \\(\\mathbf{\\theta} \\circ \\mathbf{m}\\), where \\(\\circ\\) denotes the element-wise product. We can draw a random sample \\(\\mathbf{m}\\) from a Bernoulli distribution \\(\\text{Bernoulli}(\\mathbf{p}=\\sigma(\\mathbf{x}))\\), where \\(\\mathbf{p}\\) is the probability (\\(\\mathbf{x}\\) denotes the logits) of each parameter being activated. However, the discrete Bernoulli distribution is not differentiable, so we cannot backpropagate the gradients through it to optimize the parameters \\(\\mathbf{p}\\) or \\(\\mathbf{x}\\).</p> <p>To address this issue, we introduce the Concrete mask which can be drawn from a continuous relaxation of Bernoulli distribution. Before we introduce the Concrete mask, we first review the Gumbel-Max trick in the two-class case.</p> <p>Let \\(p_0\\) and \\(p_1\\) denote the unnormalized probabilities of a Bernoulli random variable being 0 and 1, respectively, with \\(x\\) representing the logits. Then, the probability of the event \\(m=1\\) is given by</p> \\[   \\mathbb{P}(m=1) = \\frac{p_1}{p_0 + p_1} = \\sigma(x), \\] <p>where \\(\\sigma\\) denotes the sigmoid function. In the context of the Gumbel-Max trick, the occurrence of the event \\(m=1\\) is determined by the condition \\(g_1 + \\log p_1 &gt; g_0 + \\log p_0\\), where \\(g_0\\) and \\(g_1\\) are two independent standard Gumbel random variables. Thus we have</p> \\[   \\mathbb{P}(m=1) = \\mathbb{P}(g_1 + \\log p_1 &gt; g_0 + \\log p_0)   = \\mathbb{P}\\left((g_1 - g_0) + (\\log p_1 - \\log p_0)&gt; 0\\right). \\] <p>Because the difference of two standard Gumbel random variables is a Logistic random variable, we can replace \\(g_1 - g_0\\) by \\(\\log u - \\log(1-u)\\) where \\(u\\) is a random variable sampled from a uniform distribution on the interval \\((0,1)\\). Substitute this into Eq.(\\ref{eq:appendix_P_m_1}) and express the probability in terms of the logits \\(x\\) to simplify the expression, we have</p> \\[   \\mathbb{P}(m=1) = \\mathbb{P}\\left(\\log \\frac{u}{1-u} + \\log \\frac{\\sigma(x)}{1-\\sigma(x)} &gt; 0\\right), \\quad u \\sim \\text{Uniform}(0,1). \\] <p>The binary Concrete distribution offers a continuous relaxation of the discrete Bernoulli random variables, which is beneficial for gradient-based optimization as it allows for the backpropagation of gradients even through the sampling process. Instead of making a hard decision as the above equation, we use a temperature parameter \\(\\lambda\\) to control the steepness of the sigmoid function, and hence control how close our 'soft' decisions are to being 'hard' decisions. The continuous version of the Bernoulli random variable is then given by</p> \\[   \\hat{m} = \\sigma\\left(\\left(\\log \\frac{u}{1 - u} + \\log \\frac{\\sigma(x)}{1 - \\sigma(x)}\\right) / \\lambda\\right). \\] <p>As the temperature \\(\\lambda\\) approaches zero, the sigmoid function becomes a step function, and the Concrete random variable \\(\\hat{m}\\) becomes a Bernoulli random variable, as shown in the following Figure. In the limit when \\(\\lambda \\to 0\\), this results in sampling \\(m=1\\) if \\(\\log \\frac{\\sigma(x)}{1 - \\sigma(x)} &gt; -\\log \\frac{u}{1 - u}\\), consistent with the original Gumbel-Max trick. The binary Concrete distribution thus provides a differentiable approximation to Bernoulli random variables. We can further binarize the Concrete mask by setting the entries with values greater than 0.5 to 1 and the rest to 0.</p>       The sigmoid function \\(\\sigma(\\cdot/\\lambda)\\) with different temperatures \\(\\lambda\\)."},{"location":"algorithms/concrete_subspace/#method-analysis","title":"Method Analysis","text":""},{"location":"algorithms/concrete_subspace/#concrete-adamerging","title":"Concrete AdaMerging","text":"Performance comparison between AdaMerging and Concrete AdaMerging. Here we show the whole process of applying AdaMerging and Concrete AdaMerging to CLIP-ViT-B/32, the y-axes are shared by these two subfigures: (a) shows the performance of the merged model during the meta-learning phase of the Concrete AdaMerging; (b) illustrates the comparison between AdaMerging with and without the Concrete mask."},{"location":"algorithms/concrete_subspace/#code-integration","title":"Code Integration","text":"<p>Merging CLIP models on eight image classification tasks, using the concrete task arithmetic algorithm</p> <pre><code># tensorboard logs and learned checkpoints of the shared mask can be found at https://huggingface.co/tanganke/clip-vit-base-patch32_concrete-task-arithmetic_tblogs\nfusion_bench \\\n    fabric.loggers.name=ViT-B-32/concrete_task_arithmetic \\\n    method=concrete_subspace/clip_concrete_task_arithmetic \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre> <p>results</p> <pre><code>{\n    \"svhn\": {\n        \"accuracy\": 0.903003990650177,\n        \"loss\": 0.37700024247169495\n    },\n    \"stanford_cars\": {\n        \"accuracy\": 0.6326327323913574,\n        \"loss\": 1.2553859949111938\n    },\n    \"resisc45\": {\n        \"accuracy\": 0.7558730244636536,\n        \"loss\": 1.017554759979248\n    },\n    \"eurosat\": {\n        \"accuracy\": 0.9407407641410828,\n        \"loss\": 0.20871955156326294\n    },\n    \"gtsrb\": {\n        \"accuracy\": 0.8285035490989685,\n        \"loss\": 0.5861473679542542\n    },\n    \"mnist\": {\n        \"accuracy\": 0.9800000190734863,\n        \"loss\": 0.08148527890443802\n    },\n    \"dtd\": {\n        \"accuracy\": 0.5249999761581421,\n        \"loss\": 2.2731478214263916\n    },\n    \"sun397\": {\n        \"accuracy\": 0.6421158909797668,\n        \"loss\": 1.4108904600143433\n    }\n}\n</code></pre> <p>Concrete AdaMerging (Layer-wise)</p> <pre><code># tensorboard logs and learned checkpoints of the shared mask can be found at https://huggingface.co/tanganke/clip-vit-base-patch32_concrete-layer-wise_adamerging_tblogs\nfusion_bench \\\n    fabric.loggers.name=ViT-B-32/clip_concrete_layer_wise_adamerging \\\n    method=concrete_subspace/clip_concrete_layer_wise_adamerging \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre>"},{"location":"algorithms/concrete_subspace/#further-reading","title":"Further Reading","text":"<ul> <li> <p>       X. Yi, S. Zheng, L. Wang, X. Wang, and L. He, \u201cA safety realignment framework via subspace-oriented model fusion for large language models.\u201d arXiv, May 14, 2024. doi: 10.48550/arXiv.2405.09055.</p> <p>The paper introduces a safety realignment framework for large language models via subspace-oriented model fusion (SOMF, the authors learn a shared mask on the weight space of large language model), which combines safeguard capabilities of initially aligned models with fine-tuned models to ensure safety without compromising performance on downstream tasks.</p> </li> </ul> <ol> <li> <p>E. J. Gumbel. Statistical Theory of Extreme Values and Some Practical Applications. A Series of Lectures. Technical Report PB175818, National Bureau of Standards, Washington, D. C. Applied Mathematics Div., 1954. URL https://ntrl.ntis.gov/NTRL/dashboard/searchResults/titleDetail/PB175818.xhtml.\u00a0\u21a9</p> </li> <li> <p>R. Duncan Luce. Individual Choice Behavior. Individual Choice Behavior. John Wiley, Oxford, England, 1959\u00a0\u21a9</p> </li> <li> <p>Chris J Maddison, Daniel Tarlow, and Tom Minka. A* sampling. Advances in neural information processing systems, 27, 2014.\u00a0\u21a9</p> </li> <li> <p>Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables, March 2017. URL http://arxiv.org/abs/1611.00712.\u00a0\u21a9</p> </li> </ol>"},{"location":"algorithms/depth_upscaling/","title":"Depth Upscaling","text":""},{"location":"algorithms/depth_upscaling/#usage","title":"Usage","text":"<p>The <code>DepthUpscalingAlgorithm</code> is used to upscale the depth of PyTorch models. Here's a basic guide on how to use it:</p> <p>First, import the necessary modules:</p> <pre><code>from omegaconf import DictConfig\nfrom torch import nn\nfrom fusion_bench.method.depth_upscaling import DepthUpscalingAlgorithm\nfrom fusion_bench.modelpool import to_modelpool\n</code></pre> <p>Create an instance of <code>DepthUpscalingAlgorithm</code> by passing a configuration dictionary.  This dictionary should contain the name of the method (\"depth_upscaling\") and a list of layer indices that determine the upscaling pattern.</p> <pre><code>method_config = {\"name\": \"depth_upscaling\", \"layer_indices\": [0, 1, 1, 0]}\nalgorithm = DepthUpscalingAlgorithm(DictConfig(method_config))\n</code></pre> <p>Assume we have a list of PyTorch models (<code>nn.ModuleList</code> instances) that we want to upscale. Here, we're creating a list of linear models as an example:</p> <pre><code>model = nn.ModuleList([nn.Linear(10, 10) for _ in range(2)])\n</code></pre> <p>Then, we can the model to the <code>run</code> method of our algorithm:</p> <pre><code>upscaled_model = algorithm.run(model)\n</code></pre> <p>The <code>run</code> method will return an upscaled model. The type of the returned model will be the same as the input models (in this case, <code>nn.ModuleList</code>), and its length will be determined by the layer indices specified in the method configuration.</p>"},{"location":"algorithms/depth_upscaling/#examples","title":"Examples","text":"<p>Here we provide an example of how to use the <code>DepthUpscalingAlgorithm</code> to upscale the depth of a Mistral model <sup>1</sup>.</p>  Credit to \"SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling\" <pre><code>from omegaconf import DictConfig\nfrom torch import nn\nfrom transformers import AutoModelForCausalLM, MistralConfig, MistralForCausalLM\nfrom fusion_bench.method.depth_upscaling import DepthUpscalingAlgorithm\n\n# create a Mistral model\n# here we randomly initialize the model for demonstration purposes\n# in practice, you would load a pretrained model\nmodel_config = MistralConfig(\n    # https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/config.json\n    **{\n        \"architectures\": [\"MistralForCausalLM\"],\n        \"bos_token_id\": 1,\n        \"eos_token_id\": 2,\n        \"hidden_act\": \"silu\",\n        \"hidden_size\": 4096,\n        \"initializer_range\": 0.02,\n        \"intermediate_size\": 14336,\n        \"max_position_embeddings\": 32768,\n        \"model_type\": \"mistral\",\n        \"num_attention_heads\": 32,\n        \"num_hidden_layers\": 32,\n        \"num_key_value_heads\": 8,\n        \"rms_norm_eps\": 1e-05,\n        \"rope_theta\": 10000.0,\n        \"sliding_window\": 4096,\n        \"tie_word_embeddings\": False,\n        \"torch_dtype\": \"bfloat16\",\n        \"transformers_version\": \"4.34.0.dev0\",\n        \"use_cache\": True,\n        \"vocab_size\": 32000,\n    }\n)\nprint('creating model')\nmodel: MistralForCausalLM = AutoModelForCausalLM.from_config(model_config)\n\nmethod_config = {\n    \"name\": \"depth_upscaling\",\n    \"layer_indices\": [\"range(0,24)\", \"range(8,32)\"],\n}\nalgorithm = DepthUpscalingAlgorithm(DictConfig(method_config))\nprint('upscaling model')\nupscaled_model = algorithm.run(model.model.layers)\n\n# substitute the model with the upscaled model\nmodel.model.layers = upscaled_model\n</code></pre>"},{"location":"algorithms/depth_upscaling/#code-integration","title":"Code Integration","text":"<p>The <code>DepthUpscalingAlgorithm</code> is integrated into the <code>fusion_bench</code> package. You can use it by specifying <code>\"depth_upscaling\"</code> as the method name in the command line or configuration file.</p> config/method/depth_upscaling.yaml<pre><code>name: depth_upscaling\n# this should be a list of integers or string, indicating the sequence of layers. If the entry is an integer, it will use the n-th layer of the model. If the entry is a string, it will use the layers specified by the string. The string should be a valid python expression that evaluates to a list of integers.\n# for example, [\"range(0,12)\", \"range(6,12)\"] will use the first 12 layers and the last 6 layers of the model to construct the new model\n# [0, 2, 4, \"range(6,12)\"] will use the 1st, 3rd, 5th, and the 7th to 12th layers of the model to construct the new model\nlayer_indices: null\n</code></pre> <p>You can then run the <code>fusion_bench</code> command with the specified configuration file:</p> <pre><code>fusion_bench method=depth_upscaling ...\n</code></pre>"},{"location":"algorithms/depth_upscaling/#references","title":"References","text":"<ol> <li> <p>SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling \u21a9</p> </li> </ol>"},{"location":"algorithms/depth_upscaling/#fusion_bench.method.depth_upscaling.DepthUpscalingAlgorithm","title":"<code>DepthUpscalingAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code></p> <p>Implements the Depth Upscaling Algorithm.</p> <ul> <li>Kim et al. SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling. http://arxiv.org/abs/2312.15166</li> </ul> <p>This class extends the <code>BaseModelFusionAlgorithm</code> to handle depth upscaling of models. It supports upscaling the depth of a model by duplicating specified layers.</p> <p>Parameters:</p> <ul> <li> </li> <li> </li> </ul> Source code in <code>fusion_bench/method/depth_upscaling/depth_upscaling.py</code> <pre><code>class DepthUpscalingAlgorithm(BaseAlgorithm):\n    R\"\"\"\n    Implements the Depth Upscaling Algorithm.\n\n    - Kim et al. SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling. http://arxiv.org/abs/2312.15166\n\n    This class extends the `BaseModelFusionAlgorithm` to handle depth upscaling of models.\n    It supports upscaling the depth of a model by duplicating specified layers.\n\n    Args:\n        layer_indices (list): List of layer indices to duplicate.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"layer_indices\": \"layer_indices\",\n    }\n\n    def __init__(self, layer_indices: Union[str, List[int]], **kwargs):\n        self.layer_indices = layer_indices\n        super().__init__(**kwargs)\n\n    @torch.no_grad()\n    def run(self, modelpool: nn.ModuleList | BaseModelPool) -&gt; nn.ModuleList:\n        \"\"\"\n        Executes the depth upscaling algorithm on a given model pool.\n\n        This method checks the type of the model pool, ensures that it contains only one model, and verifies that the model is an instance of `nn.ModuleList`.\n\n        Args:\n            modelpool (nn.ModuleList | ModelPool): The pool of models to upscale. Must contain only one model.\n\n        Returns:\n            nn.ModuleList: The upscaled model.\n\n        Raises:\n            AssertionError: If the model pool contains more than one model or if the model is not an instance of `nn.ModuleList`.\n            ValueError: If an invalid layer specification is provided in the configuration.\n        \"\"\"\n        # check the modelpool type\n        if isinstance(modelpool, BaseModelPool):\n            assert len(modelpool) == 1, \"DepthUpscaling only support one model\"\n            model = modelpool.load_model(modelpool.model_names[0])\n            assert isinstance(\n                model, nn.ModuleList\n            ), f\"The model should be a `nn.ModuleList`, but got {type(model)}\"\n        elif isinstance(modelpool, nn.ModuleList):\n            model = modelpool\n        else:\n            raise AssertionError(\n                f\"Invalid modelpool type: {type(modelpool)}. Expected `ModelPool` or `nn.ModuleList`.\"\n            )\n\n        # parse the layers\n        layer_indices = self.layer_indices\n        parsed_layer_indices = []\n        for layer in layer_indices:\n            if isinstance(layer, int):\n                parsed_layer_indices.append(layer)\n            elif isinstance(layer, str):\n                parsed_layer_indices.extend(eval(layer))\n            else:\n                raise ValueError(\"Invalid layer specification: {}\".format(layer))\n\n        # create a new model with the specified layers\n        new_model = nn.ModuleList(\n            [\n                deepcopy(model[i])\n                for i in tqdm(\n                    parsed_layer_indices, desc=\"constructing depth-upscaled model\"\n                )\n            ]\n        )\n\n        return new_model\n</code></pre>"},{"location":"algorithms/depth_upscaling/#fusion_bench.method.depth_upscaling.DepthUpscalingAlgorithm(layer_indices)","title":"<code>layer_indices</code>","text":"(<code>list</code>)           \u2013            <p>List of layer indices to duplicate.</p>"},{"location":"algorithms/depth_upscaling/#fusion_bench.method.depth_upscaling.DepthUpscalingAlgorithm(**kwargs)","title":"<code>**kwargs</code>","text":"\u2013            <p>Additional keyword arguments.</p>"},{"location":"algorithms/depth_upscaling/#fusion_bench.method.depth_upscaling.DepthUpscalingAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Executes the depth upscaling algorithm on a given model pool.</p> <p>This method checks the type of the model pool, ensures that it contains only one model, and verifies that the model is an instance of <code>nn.ModuleList</code>.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>ModuleList</code>           \u2013            <p>nn.ModuleList: The upscaled model.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>AssertionError</code>             \u2013            <p>If the model pool contains more than one model or if the model is not an instance of <code>nn.ModuleList</code>.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If an invalid layer specification is provided in the configuration.</p> </li> </ul> Source code in <code>fusion_bench/method/depth_upscaling/depth_upscaling.py</code> <pre><code>@torch.no_grad()\ndef run(self, modelpool: nn.ModuleList | BaseModelPool) -&gt; nn.ModuleList:\n    \"\"\"\n    Executes the depth upscaling algorithm on a given model pool.\n\n    This method checks the type of the model pool, ensures that it contains only one model, and verifies that the model is an instance of `nn.ModuleList`.\n\n    Args:\n        modelpool (nn.ModuleList | ModelPool): The pool of models to upscale. Must contain only one model.\n\n    Returns:\n        nn.ModuleList: The upscaled model.\n\n    Raises:\n        AssertionError: If the model pool contains more than one model or if the model is not an instance of `nn.ModuleList`.\n        ValueError: If an invalid layer specification is provided in the configuration.\n    \"\"\"\n    # check the modelpool type\n    if isinstance(modelpool, BaseModelPool):\n        assert len(modelpool) == 1, \"DepthUpscaling only support one model\"\n        model = modelpool.load_model(modelpool.model_names[0])\n        assert isinstance(\n            model, nn.ModuleList\n        ), f\"The model should be a `nn.ModuleList`, but got {type(model)}\"\n    elif isinstance(modelpool, nn.ModuleList):\n        model = modelpool\n    else:\n        raise AssertionError(\n            f\"Invalid modelpool type: {type(modelpool)}. Expected `ModelPool` or `nn.ModuleList`.\"\n        )\n\n    # parse the layers\n    layer_indices = self.layer_indices\n    parsed_layer_indices = []\n    for layer in layer_indices:\n        if isinstance(layer, int):\n            parsed_layer_indices.append(layer)\n        elif isinstance(layer, str):\n            parsed_layer_indices.extend(eval(layer))\n        else:\n            raise ValueError(\"Invalid layer specification: {}\".format(layer))\n\n    # create a new model with the specified layers\n    new_model = nn.ModuleList(\n        [\n            deepcopy(model[i])\n            for i in tqdm(\n                parsed_layer_indices, desc=\"constructing depth-upscaled model\"\n            )\n        ]\n    )\n\n    return new_model\n</code></pre>"},{"location":"algorithms/depth_upscaling/#fusion_bench.method.depth_upscaling.DepthUpscalingAlgorithm.run(modelpool)","title":"<code>modelpool</code>","text":"(<code>ModuleList | ModelPool</code>)           \u2013            <p>The pool of models to upscale. Must contain only one model.</p>"},{"location":"algorithms/dummy/","title":"Dummy Algorithm","text":"<p>The Dummy Algorithm is a simple algorithm that does not perform any fusion operation. Instead, it returns a pretrained model if one is available in the model pool. If no pretrained model is available, it returns the first model in the model pool. This algorithm is useful for testing and debugging purposes, as it allows you to quickly check if the model pool is set up correctly and the fusion process is working as expected.</p>"},{"location":"algorithms/dummy/#usage","title":"Usage","text":"<p>To use the Dummy Algorithm, you need to specify <code>\"dummy\"</code> as the algorithm name.</p> <pre><code>fusion_bench method=dummy ...\n</code></pre>"},{"location":"algorithms/dummy/#implementation","title":"Implementation","text":"<p>The implementation of the Dummy Algorithm is straightforward. Here is the main method of the <code>DummyAlgorithm</code> class:</p>"},{"location":"algorithms/dummy/#fusion_bench.method.dummy.DummyAlgorithm","title":"<code>DummyAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code></p> Source code in <code>fusion_bench/method/dummy.py</code> <pre><code>class DummyAlgorithm(BaseAlgorithm):\n    def run(self, modelpool: BaseModelPool):\n        \"\"\"\n        This method returns the pretrained model from the model pool.\n        If the pretrained model is not available, it returns the first model from the model pool.\n\n        Args:\n            modelpool (BaseModelPool): The pool of models to fuse.\n\n        Raises:\n            AssertionError: If the model is not found in the model pool.\n        \"\"\"\n        if isinstance(modelpool, nn.Module):\n            return modelpool\n        elif not isinstance(modelpool, BaseModelPool):\n            modelpool = BaseModelPool(modelpool)\n\n        model = modelpool.load_pretrained_or_first_model()\n\n        assert model is not None, \"Model is not found in the model pool.\"\n        return model\n</code></pre>"},{"location":"algorithms/fisher_merging/","title":"(Diagonal) Fisher Merging","text":"<p>The Fisher merging algorithm <sup>1</sup> is a per-parameter weighed averaging method that assigns weights to the models based on the Fisher information matrix of the models on some labeled data. The Fisher information matrix \\(F_\\theta\\) of a model with parameters \\(\\theta\\) can be expressed as:</p> \\[ F_\\theta = \\mathbb{E}_{x \\sim p(x)} \\left[ \\nabla_\\theta \\log p(y|x, \\theta) \\nabla_\\theta \\log p(y|x, \\theta)^T \\right] \\] <p>where \\(p(x)\\) is the data distribution, \\(p(y|x, \\theta)\\) is the model's output distribution, for example, the softmax output of a classification model, and \\(\\nabla_\\theta\\) is the gradient with respect to the model's parameters \\(\\theta\\). The Fisher information matrix can be used to estimate the importance of each parameter in the model and thus assign weights to the models based on their Fisher information.  In addition, the Fisher information matrix can be used to estimate the similarity between tasks, which can be useful in auxiliary-task learning and multi-task learning scenarios <sup>2</sup>.</p> <p>As the full Fisher information matrix is often computationally expensive to compute and memory-intensive to store, we approximate using the diagonal Fisher information matrix, which is the diagonal of the full Fisher information matrix. The diagonal Fisher information matrix can be computed as:</p> \\[ \\hat{F}_\\theta = \\mathbb{E}_{x \\sim p(x)} \\left[ \\left(\\nabla_\\theta \\log p(y|x, \\theta)\\right)^2 \\right] \\] <p>Assuming we have \\(n\\) models with parameters \\(\\theta_i\\) and diagonal Fisher information matrices \\(\\hat{F}_{\\theta_i}\\), the Fisher merging algorithm computes the merged model's parameters \\(\\theta\\) as follows:</p> \\[ \\theta^{(j)} = \\frac{\\sum_{i=1}^{n} \\hat{F}_{\\theta_i}^{(j)} \\theta_i^{(j)}}{\\sum_{i=1}^{n} \\hat{F}_{\\theta_i}^{(j)}} \\] <p>where \\(\\theta_i\\) are the parameters of the individual models, \\(\\hat{F}_{\\theta_i}\\) are the diagonal Fisher information matrices of the individual models, and \\(j\\) indexes the parameters of the models. The Fisher merging algorithm can be considered a per-weight weighed averaging method, where the weights are determined by the Fisher information of each parameter in the models.</p>"},{"location":"algorithms/fisher_merging/#code-integration","title":"Code Integration","text":"<p>Example of merging eight CLIP-ViT-B/32 models using Fisher merging:</p> <pre><code>fusion_bench method=clip_fisher_merging \\\n  modelpool=clip-vit-base-patch32_TA8 \\\n  taskpool=clip-vit-classification_TA8\n</code></pre> <p>Merge eight CLIP-ViT-L/14 models using Fisher merging:</p> <pre><code>fusion_bench \\\n  method=clip_fisher_merging \\\n    method.batch_size=8 method.num_workers=4 \\\n  modelpool=clip-vit-large-patch14_TA8 \\\n  taskpool=clip-vit-classification_TA8 \\\n    taskpool.clip_model=openai/clip-vit-large-patch14\n</code></pre> <p>Merge GPT-2 models for text classification tasks:</p> <pre><code>fusion_bench \\\n  method=gpt2_fisher_merging \\\n    method.num_fisher_examples=512 method.batch_size=8 \\\n  modelpool=gpt-2_glue \\\n  taskpool=gpt-2_glue\n</code></pre>"},{"location":"algorithms/fisher_merging/#references","title":"References","text":"<ol> <li> <p>M. Matena, C. Raffel. \"Merging Models with Fisher-Weighted Averaging\" http://arxiv.org/abs/2111.09832\u00a0\u21a9</p> </li> <li> <p>C. Wu, et al. \"Pi-Tuning: Transferring Multimodal Foundation Models with Optimal Multi-task Interpolation\". https://github.com/TencentARC/pi-Tuning\u00a0\u21a9</p> </li> </ol>"},{"location":"algorithms/fisher_merging/#fusion_bench.method.fisher_merging.fisher_merging.FisherMergingAlgorithm","title":"<code>FisherMergingAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code></p> <p>Implements the Fisher Merging Algorithm.</p> <p>This class extends the BaseModelFusionAlgorithm to handle merging of models using Fisher weights. It supports excluding certain parameters, normalizing Fisher weights, and setting a minimal value for Fisher weights.</p> <p>Methods:</p> <ul> <li> <code>run</code>             \u2013              <p>BaseModelPool) -&gt; nn.Module: Executes the Fisher merging process on the model pool and returns the merged model.</p> </li> </ul> Source code in <code>fusion_bench/method/fisher_merging/fisher_merging.py</code> <pre><code>class FisherMergingAlgorithm(BaseAlgorithm):\n    \"\"\"\n    Implements the Fisher Merging Algorithm.\n\n    This class extends the BaseModelFusionAlgorithm to handle merging of models using Fisher weights.\n    It supports excluding certain parameters, normalizing Fisher weights, and setting a minimal value for Fisher weights.\n\n    Methods:\n        run(modelpool: BaseModelPool) -&gt; nn.Module:\n            Executes the Fisher merging process on the model pool and returns the merged model.\n    \"\"\"\n\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"exclude_param_names_regex\": \"exclude_param_names_regex\",\n        \"normalize_fisher_weight\": \"normalize_fisher_weight\",\n        \"minimal_fisher_weight\": \"minimal_fisher_weight\",\n        \"num_fisher_examples\": \"num_fisher_examples\",\n    }\n\n    def __init__(\n        self,\n        *,\n        exclude_param_names_regex: list,\n        normalize_fisher_weight: bool,\n        minimal_fisher_weight: float,\n        num_fisher_examples: int,\n    ):\n        super().__init__()\n        self.exclude_param_names_regex = exclude_param_names_regex\n        self.normalize_fisher_weight = normalize_fisher_weight\n        self.minimal_fisher_weight = minimal_fisher_weight\n        self.num_fisher_examples = num_fisher_examples\n\n    def run(self, modelpool: BaseModelPool) -&gt; nn.Module:\n        \"\"\"\n        Run the Fisher Merging Algorithm.\n\n        This method constructs the wrapped model and performs test-time adaptation if necessary.\n\n        Args:\n            modelpool (BaseModelPool): The model pool containing the pretrained and fine-tuned models.\n\n        Returns:\n            nn.Module: The merged model after test-time adaptation.\n        \"\"\"\n        log.info(\"Running Fisher Merging Algorithm\")\n        if isinstance(modelpool, (dict, list, tuple)):\n            modelpool = BaseModelPool(modelpool)\n\n        assert len(modelpool) &gt; 0, \"model pool is empty\"\n        assert (\n            modelpool.has_pretrained\n        ), \"no pretrained model (base model) in the model pool\"\n\n        self.modelpool = modelpool\n        self.on_fisher_merging_start()\n\n        # dictionary of list, where key is the parameter name,\n        # value is a list of the corresponding parameters of all the models that need to be merged\n        models_to_merge_param_dict = defaultdict(list)\n\n        # list of dictionaries with length len(models_to_merge),\n        # each dictionary records the fisher weights (matrix or vector) of parameters for each model that needs to be merged\n        models_to_merge_fisher_weights_list = []\n\n        param_names_to_merge = None\n\n        for name, model in modelpool.named_models():\n            param_dict = model.state_dict()\n            if param_names_to_merge is None:\n                param_names_to_merge = get_param_names_to_merge(\n                    input_param_names=list(param_dict.keys()),\n                    exclude_param_names_regex=self.config.get(\n                        \"exclude_param_names_regex\", []\n                    ),\n                )\n\n            for param_name in param_names_to_merge:\n                models_to_merge_param_dict[param_name].append(param_dict[param_name])\n\n            model_to_merge_fisher_weights = self.get_fisher_weights(\n                model_name=name,\n                model=model,\n                train_dataset=modelpool.load_train_dataset(name),\n                param_names_to_merge=param_names_to_merge,\n            )\n\n            models_to_merge_fisher_weights_list.append(model_to_merge_fisher_weights)\n\n        merged_params = merging_with_fisher_weights(\n            models_to_merge_param_dict=models_to_merge_param_dict,\n            models_to_merge_fisher_weights_list=models_to_merge_fisher_weights_list,\n            fisher_scaling_coefficients=torch.ones(len(modelpool)) / len(modelpool),\n            normalize_fisher_weight=self.config.get(\"normalize_fisher_weight\", True),\n            minimal_fisher_weight=self.config.get(\"minimal_fisher_weight\", 1e-6),\n        )\n\n        merged_model = modelpool.load_model(\"_pretrained_\")\n        merged_model.load_state_dict(merged_params, strict=False)\n        return merged_model\n\n    def get_fisher_weights(\n        self,\n        model_name: str,\n        model: nn.Module,\n        train_dataset,\n        param_names_to_merge: List[str],\n    ) -&gt; Dict[str, Tensor]:\n        \"\"\"\n        Compute the Fisher weights for the given model and training dataset.\n\n        Args:\n            model_name (str): The name of the model.\n            model (nn.Module): The model module.\n            train_dataset: The training dataset.\n            param_names_to_merge (List[str]): List of parameter names to merge.\n\n        Returns:\n            Dict[str, Tensor]: The computed Fisher weights for each parameter.\n        \"\"\"\n        # this function is used to compute fisher weights for a model\n        # it should be implemented in the subclass\n        raise NotImplementedError\n\n    def on_fisher_merging_start(self):\n        \"\"\"\n        Setup the zero-shot classification head before starting the Fisher merging process.\n        \"\"\"\n        # this function is used to initialize some variables before running fisher merging\n        pass\n</code></pre>"},{"location":"algorithms/fisher_merging/#fusion_bench.method.fisher_merging.fisher_merging.FisherMergingAlgorithm.get_fisher_weights","title":"<code>get_fisher_weights(model_name, model, train_dataset, param_names_to_merge)</code>","text":"<p>Compute the Fisher weights for the given model and training dataset.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Tensor]</code>           \u2013            <p>Dict[str, Tensor]: The computed Fisher weights for each parameter.</p> </li> </ul> Source code in <code>fusion_bench/method/fisher_merging/fisher_merging.py</code> <pre><code>def get_fisher_weights(\n    self,\n    model_name: str,\n    model: nn.Module,\n    train_dataset,\n    param_names_to_merge: List[str],\n) -&gt; Dict[str, Tensor]:\n    \"\"\"\n    Compute the Fisher weights for the given model and training dataset.\n\n    Args:\n        model_name (str): The name of the model.\n        model (nn.Module): The model module.\n        train_dataset: The training dataset.\n        param_names_to_merge (List[str]): List of parameter names to merge.\n\n    Returns:\n        Dict[str, Tensor]: The computed Fisher weights for each parameter.\n    \"\"\"\n    # this function is used to compute fisher weights for a model\n    # it should be implemented in the subclass\n    raise NotImplementedError\n</code></pre>"},{"location":"algorithms/fisher_merging/#fusion_bench.method.fisher_merging.fisher_merging.FisherMergingAlgorithm.get_fisher_weights(model_name)","title":"<code>model_name</code>","text":"(<code>str</code>)           \u2013            <p>The name of the model.</p>"},{"location":"algorithms/fisher_merging/#fusion_bench.method.fisher_merging.fisher_merging.FisherMergingAlgorithm.get_fisher_weights(model)","title":"<code>model</code>","text":"(<code>Module</code>)           \u2013            <p>The model module.</p>"},{"location":"algorithms/fisher_merging/#fusion_bench.method.fisher_merging.fisher_merging.FisherMergingAlgorithm.get_fisher_weights(train_dataset)","title":"<code>train_dataset</code>","text":"\u2013            <p>The training dataset.</p>"},{"location":"algorithms/fisher_merging/#fusion_bench.method.fisher_merging.fisher_merging.FisherMergingAlgorithm.get_fisher_weights(param_names_to_merge)","title":"<code>param_names_to_merge</code>","text":"(<code>List[str]</code>)           \u2013            <p>List of parameter names to merge.</p>"},{"location":"algorithms/fisher_merging/#fusion_bench.method.fisher_merging.fisher_merging.FisherMergingAlgorithm.on_fisher_merging_start","title":"<code>on_fisher_merging_start()</code>","text":"<p>Setup the zero-shot classification head before starting the Fisher merging process.</p> Source code in <code>fusion_bench/method/fisher_merging/fisher_merging.py</code> <pre><code>def on_fisher_merging_start(self):\n    \"\"\"\n    Setup the zero-shot classification head before starting the Fisher merging process.\n    \"\"\"\n    # this function is used to initialize some variables before running fisher merging\n    pass\n</code></pre>"},{"location":"algorithms/fisher_merging/#fusion_bench.method.fisher_merging.fisher_merging.FisherMergingAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Run the Fisher Merging Algorithm.</p> <p>This method constructs the wrapped model and performs test-time adaptation if necessary.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>nn.Module: The merged model after test-time adaptation.</p> </li> </ul> Source code in <code>fusion_bench/method/fisher_merging/fisher_merging.py</code> <pre><code>def run(self, modelpool: BaseModelPool) -&gt; nn.Module:\n    \"\"\"\n    Run the Fisher Merging Algorithm.\n\n    This method constructs the wrapped model and performs test-time adaptation if necessary.\n\n    Args:\n        modelpool (BaseModelPool): The model pool containing the pretrained and fine-tuned models.\n\n    Returns:\n        nn.Module: The merged model after test-time adaptation.\n    \"\"\"\n    log.info(\"Running Fisher Merging Algorithm\")\n    if isinstance(modelpool, (dict, list, tuple)):\n        modelpool = BaseModelPool(modelpool)\n\n    assert len(modelpool) &gt; 0, \"model pool is empty\"\n    assert (\n        modelpool.has_pretrained\n    ), \"no pretrained model (base model) in the model pool\"\n\n    self.modelpool = modelpool\n    self.on_fisher_merging_start()\n\n    # dictionary of list, where key is the parameter name,\n    # value is a list of the corresponding parameters of all the models that need to be merged\n    models_to_merge_param_dict = defaultdict(list)\n\n    # list of dictionaries with length len(models_to_merge),\n    # each dictionary records the fisher weights (matrix or vector) of parameters for each model that needs to be merged\n    models_to_merge_fisher_weights_list = []\n\n    param_names_to_merge = None\n\n    for name, model in modelpool.named_models():\n        param_dict = model.state_dict()\n        if param_names_to_merge is None:\n            param_names_to_merge = get_param_names_to_merge(\n                input_param_names=list(param_dict.keys()),\n                exclude_param_names_regex=self.config.get(\n                    \"exclude_param_names_regex\", []\n                ),\n            )\n\n        for param_name in param_names_to_merge:\n            models_to_merge_param_dict[param_name].append(param_dict[param_name])\n\n        model_to_merge_fisher_weights = self.get_fisher_weights(\n            model_name=name,\n            model=model,\n            train_dataset=modelpool.load_train_dataset(name),\n            param_names_to_merge=param_names_to_merge,\n        )\n\n        models_to_merge_fisher_weights_list.append(model_to_merge_fisher_weights)\n\n    merged_params = merging_with_fisher_weights(\n        models_to_merge_param_dict=models_to_merge_param_dict,\n        models_to_merge_fisher_weights_list=models_to_merge_fisher_weights_list,\n        fisher_scaling_coefficients=torch.ones(len(modelpool)) / len(modelpool),\n        normalize_fisher_weight=self.config.get(\"normalize_fisher_weight\", True),\n        minimal_fisher_weight=self.config.get(\"minimal_fisher_weight\", 1e-6),\n    )\n\n    merged_model = modelpool.load_model(\"_pretrained_\")\n    merged_model.load_state_dict(merged_params, strict=False)\n    return merged_model\n</code></pre>"},{"location":"algorithms/fisher_merging/#fusion_bench.method.fisher_merging.fisher_merging.FisherMergingAlgorithm.run(modelpool)","title":"<code>modelpool</code>","text":"(<code>BaseModelPool</code>)           \u2013            <p>The model pool containing the pretrained and fine-tuned models.</p>"},{"location":"algorithms/isotropic_merging/","title":"Isotropic Merging","text":""},{"location":"algorithms/isotropic_merging/#code-integration","title":"Code Integration","text":"<p>Merge CLIP-ViT-B/32 models on eight image classification tasks using ISO-C, with a scaling factor of 1.5:</p> <pre><code>fusion_bench \\\n    fabric.loggers.name=iso_c \\\n    method=isotropic_merging/iso_c \\\n    method.scaling_factor=1.5 \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre> <p>Merge CLIP-ViT-B/32 models on eight image classification tasks using ISO-CTS, with a scaling factor of 1.5:</p> <pre><code>fusion_bench \\\n    fabric.loggers.name=iso_cts \\\n    method=isotropic_merging/iso_cts \\\n    method.scaling_factor=1.5 \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre>"},{"location":"algorithms/layer_recombination/","title":"Layer Recombination","text":""},{"location":"algorithms/max-model_predictor/","title":"Max-Model Predictor","text":"<p>The max-model predictor algorithm is a type of ensemble method. Formally, a max-model predictor is defined as follows:</p> <p>Definition (Max-Model Predictor) <sup>1</sup> Given a set of predictors \\(H = \\{h_1, h_2, \\ldots, h_n\\}\\), with \\(h_i: \\mathcal{X} \\times \\mathcal{Y}_i \\mapsto \\mathbb{R}\\), the max-model predictor \\(h_H\\) is defined as:</p> \\[h_H(x,y) = \\max_{h_i\\in H} h_i(x,y).\\] <p>Take the flu detection problem as an example <sup>1</sup>.  Doctors want to build a learning model to detect what type of virus one patient is affected based on her symptoms, for appropriate treatment. However, the types of influenza diverse geographically (Rejmanek et al., 2015), which means the distribution of patient records collected by a hospital in California may be different from those in Florida. In an extreme case, some types are unknown to the other hospital. Assume there are 4 types of influenza in the United States. In California, 2 of 4 are commonly detected, while in Florida 3 of 4 types are often detected. We assume in the two states, doctors separately trained two models \\(h_{CA}\\) and \\(h_{FL}\\) which work locally well in California and Florida respectively. However, a direct ensemble of the two local models may not work well on all the patients. Let \\(h_{US}\\) denote the ideal global model trained on the combination of local datasets. When we input a patient record \\(x\\), each model outputs its prediction as shown in the following table:</p> <p>Table: Example of flu detection on a patient \\(x\\) affected with type 2 flu. \u201c\u2212\u201d means this model is not able to predict the corresponding class. Taking the maximal score as prediction, \\(h_{FL}\\) is consistent with \\(h_{US}\\), but the combination of two local models \\(h_{CA,FL}\\) is not since \\(3/4 &gt; 4/7\\).</p> Type 1 2 3 4 \\(h_{US}(x)\\) 2/10 4/10 1/10 3/10 \\(h_{CA}(x)\\) - - 1/4 3/4 \\(h_{FL}(x)\\) 2/7 4/7 1/7 - \\(h_{\\{CA,FL\\}}(x)\\) 2/7 4/7 1/4 3/4 The illustration of running our method on the flu example."},{"location":"algorithms/max-model_predictor/#example","title":"Example","text":"<p>Here is an example of how to use the Max-Model Predictor Algorithm:</p> <pre><code>from fusion_bench.method import MaxModelPredictorAlgorithm\nfrom fusion_bench.modelpool import ModelPool\n\n# Instantiate the MaxPredictorAlgorithm\nalgorithm = MaxModelPredictorAlgorithm()\n\n# Assume we have a ModelPool instance that contains the models we want to ensemble.\nmodelpool = ModelPool(...) # or a list of nn.Module\n\n# Run the algorithm on the model pool.\nmax_model_predictor : nn.Module = algorithm.run(modelpool)\n</code></pre>"},{"location":"algorithms/max-model_predictor/#code-integration","title":"Code Integration","text":"<p>Configuration template for the Max Predictor Algorithm:</p> config/method/max_model_predictor.yaml<pre><code>name: max_model_predictor\n</code></pre> <p>To create a max predictor ensemble of models for a specific task, you can use the following command:</p> <pre><code>fusion_bench method=max_model_predictor \\\n  modelpool=&lt;modelpool_name&gt; \\\n  taskpool=&lt;taskpool_name&gt;\n</code></pre> <ol> <li> <p>Zhu et.al. ICML 2019. Heterogeneous model reuse via optimizing multiparty multiclass margin\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"algorithms/model_recombination/","title":"Model Recombination","text":"Credit to FedMR"},{"location":"algorithms/model_recombination/#usage","title":"Usage","text":"<p><code>ModelRecombinationAlgorithm</code> is a class used to recombine models in a model pool. Here's how to use it:</p> <p>First, import the necessary modules:</p> <pre><code>from fusion_bench.method import ModelRecombinationAlgorithm\nfrom fusion_bench.modelpool import ModelPool, to_modelpool\nfrom torch import nn\n</code></pre> <p>Create an instance of <code>ModelRecombinationAlgorithm</code>:</p> <pre><code>model_recombination = ModelRecombinationAlgorithm()\n</code></pre> <p>Create a model pool using the <code>to_modelpool</code> function. This function takes a list of models or a dict of models and converts it into a <code>ModelPool</code>:</p> <pre><code>models = [nn.Linear(10, 10) for _ in range(3)]\nmodelpool = to_modelpool(models)\n</code></pre> <p>Use the <code>run</code> method of the <code>ModelRecombinationAlgorithm</code> instance to recombine the models in the model pool:</p> <pre><code>new_modelpool = model_recombination.run(modelpool, return_modelpool=True)\n</code></pre> <p>The <code>run</code> method takes two arguments:</p> <ul> <li><code>modelpool</code>: The model pool to recombine.</li> <li><code>return_modelpool</code> (optional): A boolean indicating whether to return the entire model pool or just the first model. Defaults to <code>True</code>.</li> </ul> <p>If <code>return_modelpool</code> is <code>True</code>, the <code>run</code> method returns a new <code>ModelPool</code> with the recombined models. If <code>False</code>, it returns the first model from the new model pool.</p> <pre><code>new_model = model_recombination.run(modelpool, return_modelpool=False)\n</code></pre> <p>You can check the type of the returned value to ensure that the <code>run</code> method worked correctly:</p> <pre><code>assert isinstance(new_modelpool, ModelPool)\nassert isinstance(new_model, nn.Module)\n</code></pre>"},{"location":"algorithms/model_recombination/#code-integration","title":"Code Integration","text":"<p>Configuration template for the model recombination algorithm:</p> config/method/model_recombination.yaml<pre><code>name: model_recombination\n# if `return_model_pool` is not null, the argument `return_modelpool` passed to the `run` method will be ignored.\nreturn_modelpool: null\n</code></pre> <p>Construct a model recombination using our CLI tool <code>fusion_bench</code>:</p> <pre><code>fusion_bench \\\n    method=model_recombination \\\n        method.return_modelpool=false \\\n    modelpool=... \\\n    taskpool=...\n</code></pre>"},{"location":"algorithms/model_recombination/#references","title":"References","text":""},{"location":"algorithms/model_recombination/#fusion_bench.method.model_recombination.ModelRecombinationAlgorithm","title":"<code>ModelRecombinationAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code></p> <p>Model recombination recombinates the layers of the given models, to create a new set of models.</p> Source code in <code>fusion_bench/method/model_recombination.py</code> <pre><code>class ModelRecombinationAlgorithm(BaseAlgorithm):\n    \"\"\"\n    Model recombination recombinates the layers of the given models, to create a new set of models.\n    \"\"\"\n\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"return_modelpool\": \"return_modelpool\",\n    }\n\n    def __init__(self, return_modelpool: bool, **kwargs):\n        self.return_modelpool = return_modelpool\n        super().__init__(**kwargs)\n\n    @torch.no_grad()\n    def run(\n        self,\n        modelpool: BaseModelPool,\n        return_modelpool: bool = True,\n    ) -&gt; Union[nn.Module, BaseModelPool]:\n        \"\"\"\n        Executes the model recombination algorithm on a given model pool.\n\n        This method loads models from the model pool, determines their type, and applies the appropriate recombination method.\n        It then creates a new model pool with the recombined models. Depending on the `return_modelpool` flag, it either returns\n        the entire new model pool or just the first model from it.\n\n        - If the models in the model pool are of type `nn.ModuleList`, the recombination method `recombine_modellist` is used. Where each module in the list is shuffled across the models.\n        - If the models are of type `nn.ModuleDict`, the recombination method `recombine_modeldict` is used. Where each module in the dictionary is shuffled across the models.\n        - If the models are of type `nn.Module`, the recombination method `recombine_state_dict` is used. Where the state dictionaries of the models are shuffled across the models.\n\n        Args:\n            modelpool (BaseModelPool): The pool of models to recombine.\n            return_modelpool (bool, optional): Flag indicating whether to return the entire model pool or just the first model. Defaults to True. If this algorithm is initialized with config, the value of `return_modelpool` in the config will be used and this argument passed to the method will be ignored.\n\n        Returns:\n            Union[nn.Module, BaseModelPool]: The recombined model pool or the first model from the recombined pool, depending on the `return_modelpool` flag.\n\n        Raises:\n            ValueError: If the models in the model pool are of an unsupported type.\n        \"\"\"\n        # If the config has a return_modelpool flag, use that, otherwise use the argument\n        if self.config.get(\"return_modelpool\", None) is not None:\n            return_modelpool = self.config.return_modelpool\n        # check the modelpool type\n        if not isinstance(modelpool, BaseModelPool):\n            modelpool = BaseModelPool(modelpool)\n\n        log.info(f\"Running model recombination algorithm with {len(modelpool)} models\")\n\n        # TODO: optimize the `recombine_*` functions, if `return_modelpool` is False, we don't need to create the new modelpool, just the first model\n        models = [modelpool.load_model(m) for m in modelpool.model_names]\n        if isinstance(models[0], nn.ModuleList):\n            new_models = recombine_modellist(models)\n        elif isinstance(models[0], nn.ModuleDict):\n            new_models = recombine_modeldict(models)\n        elif isinstance(models[0], nn.Module):\n            new_models = recombine_state_dict(models)\n        else:\n            raise ValueError(f\"Unsupported model type {type(models[0])}\")\n\n        new_modelpool = BaseModelPool(\n            {n: m for n, m in zip(modelpool.model_names, new_models)}\n        )\n        if return_modelpool:\n            return new_modelpool\n        else:\n            return new_modelpool.load_model(new_modelpool.model_names[0])\n</code></pre>"},{"location":"algorithms/model_recombination/#fusion_bench.method.model_recombination.ModelRecombinationAlgorithm.run","title":"<code>run(modelpool, return_modelpool=True)</code>","text":"<p>Executes the model recombination algorithm on a given model pool.</p> <p>This method loads models from the model pool, determines their type, and applies the appropriate recombination method. It then creates a new model pool with the recombined models. Depending on the <code>return_modelpool</code> flag, it either returns the entire new model pool or just the first model from it.</p> <ul> <li>If the models in the model pool are of type <code>nn.ModuleList</code>, the recombination method <code>recombine_modellist</code> is used. Where each module in the list is shuffled across the models.</li> <li>If the models are of type <code>nn.ModuleDict</code>, the recombination method <code>recombine_modeldict</code> is used. Where each module in the dictionary is shuffled across the models.</li> <li>If the models are of type <code>nn.Module</code>, the recombination method <code>recombine_state_dict</code> is used. Where the state dictionaries of the models are shuffled across the models.</li> </ul> <p>Parameters:</p> <ul> <li> </li> <li> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[Module, BaseModelPool]</code>           \u2013            <p>Union[nn.Module, BaseModelPool]: The recombined model pool or the first model from the recombined pool, depending on the <code>return_modelpool</code> flag.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the models in the model pool are of an unsupported type.</p> </li> </ul> Source code in <code>fusion_bench/method/model_recombination.py</code> <pre><code>@torch.no_grad()\ndef run(\n    self,\n    modelpool: BaseModelPool,\n    return_modelpool: bool = True,\n) -&gt; Union[nn.Module, BaseModelPool]:\n    \"\"\"\n    Executes the model recombination algorithm on a given model pool.\n\n    This method loads models from the model pool, determines their type, and applies the appropriate recombination method.\n    It then creates a new model pool with the recombined models. Depending on the `return_modelpool` flag, it either returns\n    the entire new model pool or just the first model from it.\n\n    - If the models in the model pool are of type `nn.ModuleList`, the recombination method `recombine_modellist` is used. Where each module in the list is shuffled across the models.\n    - If the models are of type `nn.ModuleDict`, the recombination method `recombine_modeldict` is used. Where each module in the dictionary is shuffled across the models.\n    - If the models are of type `nn.Module`, the recombination method `recombine_state_dict` is used. Where the state dictionaries of the models are shuffled across the models.\n\n    Args:\n        modelpool (BaseModelPool): The pool of models to recombine.\n        return_modelpool (bool, optional): Flag indicating whether to return the entire model pool or just the first model. Defaults to True. If this algorithm is initialized with config, the value of `return_modelpool` in the config will be used and this argument passed to the method will be ignored.\n\n    Returns:\n        Union[nn.Module, BaseModelPool]: The recombined model pool or the first model from the recombined pool, depending on the `return_modelpool` flag.\n\n    Raises:\n        ValueError: If the models in the model pool are of an unsupported type.\n    \"\"\"\n    # If the config has a return_modelpool flag, use that, otherwise use the argument\n    if self.config.get(\"return_modelpool\", None) is not None:\n        return_modelpool = self.config.return_modelpool\n    # check the modelpool type\n    if not isinstance(modelpool, BaseModelPool):\n        modelpool = BaseModelPool(modelpool)\n\n    log.info(f\"Running model recombination algorithm with {len(modelpool)} models\")\n\n    # TODO: optimize the `recombine_*` functions, if `return_modelpool` is False, we don't need to create the new modelpool, just the first model\n    models = [modelpool.load_model(m) for m in modelpool.model_names]\n    if isinstance(models[0], nn.ModuleList):\n        new_models = recombine_modellist(models)\n    elif isinstance(models[0], nn.ModuleDict):\n        new_models = recombine_modeldict(models)\n    elif isinstance(models[0], nn.Module):\n        new_models = recombine_state_dict(models)\n    else:\n        raise ValueError(f\"Unsupported model type {type(models[0])}\")\n\n    new_modelpool = BaseModelPool(\n        {n: m for n, m in zip(modelpool.model_names, new_models)}\n    )\n    if return_modelpool:\n        return new_modelpool\n    else:\n        return new_modelpool.load_model(new_modelpool.model_names[0])\n</code></pre>"},{"location":"algorithms/model_recombination/#fusion_bench.method.model_recombination.ModelRecombinationAlgorithm.run(modelpool)","title":"<code>modelpool</code>","text":"(<code>BaseModelPool</code>)           \u2013            <p>The pool of models to recombine.</p>"},{"location":"algorithms/model_recombination/#fusion_bench.method.model_recombination.ModelRecombinationAlgorithm.run(return_modelpool)","title":"<code>return_modelpool</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Flag indicating whether to return the entire model pool or just the first model. Defaults to True. If this algorithm is initialized with config, the value of <code>return_modelpool</code> in the config will be used and this argument passed to the method will be ignored.</p>"},{"location":"algorithms/model_recombination/#fusion_bench.method.model_recombination.recombine_modellist","title":"<code>recombine_modellist(models)</code>","text":"Source code in <code>fusion_bench/method/model_recombination.py</code> <pre><code>def recombine_modellist(models: List[nn.ModuleList]):\n    num_models = len(models)\n    num_layers = len(models[0])\n\n    new_models = [[] for _ in range(num_models)]\n    for layer_idx in range(num_layers):\n        shuffled_layers = [m[layer_idx] for m in models]\n        random.shuffle(shuffled_layers)\n        for model_idx in range(num_models):\n            new_models[model_idx].append(shuffled_layers[model_idx])\n    new_models = [nn.ModuleList(m) for m in new_models]\n    return new_models\n</code></pre>"},{"location":"algorithms/model_recombination/#fusion_bench.method.model_recombination.recombine_modeldict","title":"<code>recombine_modeldict(models)</code>","text":"Source code in <code>fusion_bench/method/model_recombination.py</code> <pre><code>def recombine_modeldict(models: List[nn.ModuleDict]):\n    num_models = len(models)\n\n    new_models = [{} for _ in range(num_models)]\n    for layer_name in models[0].keys():\n        shuffled_layers = [m[layer_name] for m in models]\n        random.shuffle(shuffled_layers)\n        for model_idx in range(num_models):\n            new_models[model_idx][layer_name] = shuffled_layers[model_idx]\n    new_models = [nn.ModuleDict(m) for m in new_models]\n    return new_models\n</code></pre>"},{"location":"algorithms/model_recombination/#fusion_bench.method.model_recombination.recombine_state_dict","title":"<code>recombine_state_dict(models)</code>","text":"Source code in <code>fusion_bench/method/model_recombination.py</code> <pre><code>def recombine_state_dict(models: List[nn.Module]):\n    num_models = len(models)\n    state_dicts = [model.state_dict() for model in models]\n    new_state_dict = [{} for _ in range(num_models)]\n    for key in state_dicts[0].keys():\n        shuffled_layers = [state_dict[key] for state_dict in state_dicts]\n        random.shuffle(shuffled_layers)\n        for model_idx in range(num_models):\n            new_state_dict[model_idx][key] = shuffled_layers[model_idx]\n    for model_idx in range(num_models):\n        models[model_idx].load_state_dict(new_state_dict[model_idx])\n    return models\n</code></pre>"},{"location":"algorithms/moe_based_merging/","title":"MoE-based Model Model Merging","text":""},{"location":"algorithms/moe_based_merging/#code-intergration","title":"Code Intergration","text":"<p>Here we provides instructions on how to use the <code>fusion_bench</code> command-line interface to merge models using a Mixture of Experts (MoE) approach.</p> <p>The first code block is a YAML configuration file for the merging method. The <code>name</code> field specifies the name of the merging method. The <code>num_experts</code> field specifies the number of experts to use in the merging process. The <code>experts_per_token</code> field specifies the number of experts to use per token. The <code>save_checkpoint</code> field specifies the path where the merged model will be saved.</p> config/method/mixtral_moe_merging.yaml<pre><code>name: mixtral_for_causal_lm_moe_merging\n\nexperts_per_token: 2\n# path to save the merged model, if provided\nsave_checkpoint: null\n</code></pre> <p>The second code block is another YAML configuration file, this time for the model pool. The <code>type</code> field specifies the type of model pool to use. The <code>models</code> field is a list of models to include in the pool. Each model should have a <code>name</code> and a <code>path</code>, and the model is loaded from the path.</p> config/modelpool/mixtral_moe_merging.yaml<pre><code>type: AutoModelForCausalLMPool\n# each model should have a name and a path, and the model is loaded from the path\n# this is equivalent to `AutoModelForCausalLM.from_pretrained(path)`\nmodels:\n  - name: _pretrained_\n    path: path_to_your_pretrained_model\n  - name: expert_1\n    path: path_to_your_expert_model_1\n  - name: expert_2\n    path: path_to_your_expert_model_2\n  - name: expert_3\n    path: path_to_your_expert_model_3\n  - name: expert_4\n    path: path_to_your_expert_model_4\n</code></pre> <p>Finally, the third code block is a bash command that runs the <code>fusion_bench</code> command-line interface with the specified method, model pool, and task pool. The <code>method</code> argument specifies the merging method to use. The <code>modelpool</code> argument specifies the model pool to use. The <code>modelpool.models.0.path</code> argument specifies the path to the pretrained model to use. The <code>taskpool</code> argument specifies the task pool to use. In this case, a dummy task pool is used that does nothing but print the parameter counts of the merged model.</p> <pre><code>fusion_bench \\\n    method=mixtral_moe_merging \\\n    modelpool=mixtral_moe_merging \\\n    taskpool=dummy # this is a dummy taskpool that does nothing but print the parameter counts of the merged model\n</code></pre> <p>This guide provides a step-by-step process for merging models using the <code>fusion_bench</code> command-line interface. By following these instructions, you can merge your own models and save them for future use.</p>"},{"location":"algorithms/moe_based_merging/#references","title":"References","text":""},{"location":"algorithms/moe_based_merging/#fusion_bench.method.mixture_of_experts.mixtral_merging","title":"<code>mixtral_merging</code>","text":""},{"location":"algorithms/moe_based_merging/#fusion_bench.method.mixture_of_experts.mixtral_merging.MixtralForCausalLMMergingAlgorithm","title":"<code>MixtralForCausalLMMergingAlgorithm</code>","text":"<p>               Bases: <code>MixtralForCausalLMUpscalingAlgorithm</code></p> <p>This class is responsible for merging models into a <code>MixtralForCausalLM</code>.</p> Source code in <code>fusion_bench/method/mixture_of_experts/mixtral_merging.py</code> <pre><code>class MixtralForCausalLMMergingAlgorithm(MixtralForCausalLMUpscalingAlgorithm):\n    \"\"\"\n    This class is responsible for merging models into a `MixtralForCausalLM`.\n    \"\"\"\n\n    @torch.no_grad()\n    def run(self, modelpool: BaseModelPool) -&gt; MixtralForCausalLM:\n        \"\"\"\n        Runs the merging process. It first upscales the models to MixtralForCausalLM,\n        then substitutes the experts of the MixtralForCausalLM with the models from the modelpool.\n\n        Args:\n            modelpool (ModelPool): The pool of models to be merged. Each model in the pool will be treated as an expert, and should be a `MistralForCausalLM` or `LlamaForCausalLM`.\n\n        Returns:\n            MixtralForCausalLM: The merged model.\n        \"\"\"\n        with open_dict(self.config):\n            self.config.num_experts = len(modelpool)\n\n        # firstly, we upscale the models to MixtralForCausalLM\n        mixtral_model = super()._run(modelpool)\n\n        # then we substitute the experts of the MixtralForCausalLM with the models from the modelpool\n        for model_idx, model_name in enumerate(modelpool.model_names):\n            expert_model: MistralForCausalLM | LlamaForCausalLM = modelpool.load_model(\n                model_name\n            )\n            _substitute_experts(model_idx, expert_model.model, mixtral_model.model)\n\n        if self.config.get(\"save_checkpoint\", None) is not None:\n            mixtral_model.save_pretrained(self.config.save_checkpoint)\n        return mixtral_model\n</code></pre>"},{"location":"algorithms/moe_based_merging/#fusion_bench.method.mixture_of_experts.mixtral_merging.MixtralForCausalLMMergingAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Runs the merging process. It first upscales the models to MixtralForCausalLM, then substitutes the experts of the MixtralForCausalLM with the models from the modelpool.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code> \u00b6              (<code>ModelPool</code>)           \u2013            <p>The pool of models to be merged. Each model in the pool will be treated as an expert, and should be a <code>MistralForCausalLM</code> or <code>LlamaForCausalLM</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MixtralForCausalLM</code> (              <code>MixtralForCausalLM</code> )          \u2013            <p>The merged model.</p> </li> </ul> Source code in <code>fusion_bench/method/mixture_of_experts/mixtral_merging.py</code> <pre><code>@torch.no_grad()\ndef run(self, modelpool: BaseModelPool) -&gt; MixtralForCausalLM:\n    \"\"\"\n    Runs the merging process. It first upscales the models to MixtralForCausalLM,\n    then substitutes the experts of the MixtralForCausalLM with the models from the modelpool.\n\n    Args:\n        modelpool (ModelPool): The pool of models to be merged. Each model in the pool will be treated as an expert, and should be a `MistralForCausalLM` or `LlamaForCausalLM`.\n\n    Returns:\n        MixtralForCausalLM: The merged model.\n    \"\"\"\n    with open_dict(self.config):\n        self.config.num_experts = len(modelpool)\n\n    # firstly, we upscale the models to MixtralForCausalLM\n    mixtral_model = super()._run(modelpool)\n\n    # then we substitute the experts of the MixtralForCausalLM with the models from the modelpool\n    for model_idx, model_name in enumerate(modelpool.model_names):\n        expert_model: MistralForCausalLM | LlamaForCausalLM = modelpool.load_model(\n            model_name\n        )\n        _substitute_experts(model_idx, expert_model.model, mixtral_model.model)\n\n    if self.config.get(\"save_checkpoint\", None) is not None:\n        mixtral_model.save_pretrained(self.config.save_checkpoint)\n    return mixtral_model\n</code></pre>"},{"location":"algorithms/moe_based_merging/#fusion_bench.method.mixture_of_experts.mixtral_merging.MixtralMoEMergingAlgorithm","title":"<code>MixtralMoEMergingAlgorithm</code>","text":"<p>               Bases: <code>MixtralUpscalingAlgorithm</code></p> <p>This class is responsible for merging models into a MixtralModel.</p> Source code in <code>fusion_bench/method/mixture_of_experts/mixtral_merging.py</code> <pre><code>class MixtralMoEMergingAlgorithm(MixtralUpscalingAlgorithm):\n    \"\"\"\n    This class is responsible for merging models into a MixtralModel.\n    \"\"\"\n\n    @torch.no_grad()\n    def run(self, modelpool: BaseModelPool) -&gt; MixtralModel:\n        \"\"\"\n        Runs the merging process.\n\n        Args:\n            modelpool (ModelPool): The pool of models to be merged. Each model in the pool will be treated as an expert, and should be a `MistralModel` or `LlamaModel`.\n\n        Returns:\n            MixtralModel: The merged model.\n        \"\"\"\n        with open_dict(self.config):\n            self.config.num_experts = len(modelpool)\n\n        # firstly, we upscale the models to MixtralModel\n        mixtral_model = super()._run(modelpool)\n\n        # then we substitute the experts of the MixtralModel with the models from the modelpool\n        for model_idx, model_name in enumerate(modelpool.model_names):\n            expert_model: MistralModel | LlamaModel = modelpool.load_model(model_name)\n            _substitute_experts(model_idx, expert_model, mixtral_model)\n\n        if self.config.get(\"save_checkpoint\", None) is not None:\n            mixtral_model.save_pretrained(self.config.save_checkpoint)\n        return mixtral_model\n</code></pre>"},{"location":"algorithms/moe_based_merging/#fusion_bench.method.mixture_of_experts.mixtral_merging.MixtralMoEMergingAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Runs the merging process.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code> \u00b6              (<code>ModelPool</code>)           \u2013            <p>The pool of models to be merged. Each model in the pool will be treated as an expert, and should be a <code>MistralModel</code> or <code>LlamaModel</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MixtralModel</code> (              <code>MixtralModel</code> )          \u2013            <p>The merged model.</p> </li> </ul> Source code in <code>fusion_bench/method/mixture_of_experts/mixtral_merging.py</code> <pre><code>@torch.no_grad()\ndef run(self, modelpool: BaseModelPool) -&gt; MixtralModel:\n    \"\"\"\n    Runs the merging process.\n\n    Args:\n        modelpool (ModelPool): The pool of models to be merged. Each model in the pool will be treated as an expert, and should be a `MistralModel` or `LlamaModel`.\n\n    Returns:\n        MixtralModel: The merged model.\n    \"\"\"\n    with open_dict(self.config):\n        self.config.num_experts = len(modelpool)\n\n    # firstly, we upscale the models to MixtralModel\n    mixtral_model = super()._run(modelpool)\n\n    # then we substitute the experts of the MixtralModel with the models from the modelpool\n    for model_idx, model_name in enumerate(modelpool.model_names):\n        expert_model: MistralModel | LlamaModel = modelpool.load_model(model_name)\n        _substitute_experts(model_idx, expert_model, mixtral_model)\n\n    if self.config.get(\"save_checkpoint\", None) is not None:\n        mixtral_model.save_pretrained(self.config.save_checkpoint)\n    return mixtral_model\n</code></pre>"},{"location":"algorithms/moe_based_upscaling/","title":"MoE-based Model Model Upscaling (Sparse Upcycling)","text":"<p>Sparse upcycling is a technique used to initialize a sparsely activated Mixture-of-Experts (MoE) model from a dense checkpoint. This approach leverages previously incurred training costs to improve the performance of large models while reducing the computational expense. In the process, dense Transformer blocks are partially replaced with MoE blocks, where the MLPs in a Transformer block are replaced by multiple experts. The experts are chosen based on routing probabilities determined by a router. The initialized MoE model is then further trained to recover the performance. This method results in improved performance for both language and vision models while using only a fraction of the original dense pretraining cost <sup>1</sup>.</p>"},{"location":"algorithms/moe_based_upscaling/#examples","title":"Examples","text":"<p>Here\u2019s an example demonstrating how to upscale a pre-trained Mistral model to a Mixtral model:</p> <pre><code>import os\n\nfrom omegaconf import DictConfig\nfrom transformers import MistralForCausalLM\n\nfrom fusion_bench.method.mixture_of_experts.mixtral_upcycling import (\n    MixtralForCausalLMUpscalingAlgorithm,\n)\nfrom fusion_bench.utils import print_parameters\n\n# Load a pre-trained Mistral model\npretrained_model = MistralForCausalLM.from_pretrained(\n    os.path.expanduser(\"path_to_mistral_model\")\n)\nprint(\"Pretrained model:\")\nprint_parameters(pretrained_model)\n# Output:\n# Pretrained model:\n# trainable params: 7.24B || all params: 7.24B || trainable%: 100.0000\n\n# Define the configuration for Mixtral\nconfig = {\n    \"num_experts\": 4,  # Number of expert channels\n    \"experts_per_token\": 2,  # Experts to choose per token\n}\n\n# Initialize the upscaling algorithm\nupscaling_for_causal_lm_algorithm = MixtralForCausalLMUpscalingAlgorithm(\n    DictConfig(config)\n)\n\n# Run the upscaling process to get a Mixtral model\nmixtral_for_causal_lm_model = upscaling_for_causal_lm_algorithm.run(pretrained_model)\n\nprint(\"Mixtral model:\")\nprint_parameters(mixtral_for_causal_lm_model)\n# Outputs:\n# Mixtral model:\n# trainable params: 24.15B || all params: 24.15B || trainable%: 100.0000\n\n# Save the upscaled Mixtral model\nmixtral_for_causal_lm_model.save_pretrained(\"path_to_save_mixtral_model\")\n</code></pre> <p>A Jupyter notebook example is also available at our repo.</p>"},{"location":"algorithms/moe_based_upscaling/#code-integration","title":"Code Integration","text":"<p>This is a guide on how to use the <code>fusion_bench</code> command-line interface to upscale a Mistral model to a Mixtral model.</p> <p>The first code block is a YAML configuration file for the upscaling method. The name field specifies the name of the upscaling method. The <code>num_experts</code> field specifies the number of experts to use in the upscaling process. The <code>experts_per_token</code> field specifies the number of experts to use per token. The <code>save_checkpoint</code> field specifies the path where the upscaled model will be saved, if provided.</p> config/method/mixtral_moe_upscaling.yaml<pre><code>name: mixtral_for_causal_lm_moe_upscaling # or \"mixtral_moe_upscaling\"\n\nnum_experts: 4\nexperts_per_token: 2\n# path to save the upscaled model\nsave_checkpoint: null\n</code></pre> <p>The second code block is another YAML configuration file, this time for the model pool. The <code>type</code> field specifies the type of model pool to use. The <code>models</code> field is a list of models to include in the pool. Each model should have a <code>name</code> and a <code>path</code>, and the model is loaded from the <code>path</code>.</p> config/modelpool/mixtral_moe_upscaling.yaml<pre><code>type: AutoModelForCausalLMPool\n# each model should have a name and a path, and the model is loaded from the path\n# this is equivalent to `AutoModelForCausalLM.from_pretrained(path)`\nmodels:\n  - name: _pretrained_\n    path: path_to_your_pretrained_model\n</code></pre> <p>Finally, the third code block is a bash command that runs the fusion_bench command-line interface with the specified method, model pool, and task pool. The method argument specifies the upscaling method to use. The modelpool argument specifies the model pool to use. The modelpool.models.0.path argument specifies the path to the pretrained model to use. The taskpool argument specifies the task pool to use. In this case, a dummy task pool is used that does nothing but print the parameter counts of the merged model.</p> <pre><code>fusion_bench \\\n    method=mixtral_moe_upscaling \\\n    modelpool=mixtral_moe_upscaling \\\n        modelpool.models.0.path=path_to_your_pretrained_model \\\n    taskpool=dummy # this is a dummy taskpool that does nothing but print the parameter counts of the merged model\n</code></pre>"},{"location":"algorithms/moe_based_upscaling/#references","title":"References","text":"<ol> <li> <p>Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints. http://arxiv.org/abs/2212.05055\u00a0\u21a9</p> </li> </ol>"},{"location":"algorithms/moe_based_upscaling/#fusion_bench.method.mixture_of_experts.mixtral_upcycling","title":"<code>mixtral_upcycling</code>","text":""},{"location":"algorithms/moe_based_upscaling/#fusion_bench.method.mixture_of_experts.mixtral_upcycling.MixtralForCausalLMUpscalingAlgorithm","title":"<code>MixtralForCausalLMUpscalingAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code></p> <p>This class is responsible for upscaling a model to a MixtralForCausalLM. It inherits from the ModelFusionAlgorithm class.</p> Source code in <code>fusion_bench/method/mixture_of_experts/mixtral_upcycling.py</code> <pre><code>class MixtralForCausalLMUpscalingAlgorithm(BaseAlgorithm):\n    \"\"\"\n    This class is responsible for upscaling a model to a MixtralForCausalLM.\n    It inherits from the ModelFusionAlgorithm class.\n    \"\"\"\n\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"num_experts\": \"num_experts\",\n        \"experts_per_token\": \"experts_per_token\",\n        \"save_checkpoint\": \"save_checkpoint\",\n    }\n\n    def __init__(\n        self,\n        num_experts: int,\n        experts_per_token: int,\n        save_checkpoint: str,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the MixtralForCausalLMUpscalingAlgorithm.\n\n        Args:\n            num_experts (int): The number of experts in the Mixtral model.\n            experts_per_token (int): The number of experts per token.\n            save_checkpoint (str): The path to save the checkpoint.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        self.num_experts = num_experts\n        self.experts_per_token = experts_per_token\n        self.save_checkpoint = save_checkpoint\n        super().__init__(**kwargs)\n\n    @torch.no_grad()\n    def _run(\n        self, modelpool: BaseModelPool | LlamaForCausalLM | MistralForCausalLM\n    ) -&gt; MixtralForCausalLM:\n        \"\"\"\n        Internal method to run the upscaling process.\n\n        Args:\n            modelpool (BaseModelPool | LlamaForCausalLM | MistralForCausalLM): The model to be upscaled.\n\n        Returns:\n            MixtralForCausalLM: The upscaled model.\n        \"\"\"\n        if isinstance(modelpool, BaseModelPool):\n            assert modelpool.has_pretrained, \"ModelPool must have pretrained model.\"\n            pretrained_model = modelpool.load_model(\"_pretrained_\")\n        elif isinstance(modelpool, (LlamaForCausalLM, MistralForCausalLM)):\n            pretrained_model = modelpool\n        else:\n            raise ValueError(\"Invalid modelpool type\")\n\n        mixtral_config = _convert_config_to_mixtral(\n            pretrained_model.config,\n            self.config.num_experts,\n            self.config.experts_per_token,\n        )\n\n        with ContextManagers([no_init_weights(True)]):\n            for _ in tqdm(range(1), desc=\"Initializing Mixtral model\"):\n                mixtral_model = MixtralForCausalLM(mixtral_config)\n        upscale_to_mixtral_for_causal_lm(pretrained_model, mixtral_model)\n\n        return mixtral_model\n\n    @torch.no_grad()\n    def run(\n        self, modelpool: BaseModelPool | LlamaForCausalLM | MistralForCausalLM\n    ) -&gt; MixtralForCausalLM:\n        \"\"\"\n        Runs the upscaling process.\n\n        Args:\n            modelpool (ModelPool | LlamaForCausalLM | MistralForCausalLM): The model to be upscaled.\n\n        Returns:\n            MixtralForCausalLM: The upscaled model.\n        \"\"\"\n        mixtral_model = self._run(modelpool)\n\n        if self.config.get(\"save_checkpoint\", None) is not None:\n            mixtral_model.save_pretrained(self.config.save_checkpoint)\n        return mixtral_model\n</code></pre>"},{"location":"algorithms/moe_based_upscaling/#fusion_bench.method.mixture_of_experts.mixtral_upcycling.MixtralForCausalLMUpscalingAlgorithm.__init__","title":"<code>__init__(num_experts, experts_per_token, save_checkpoint, **kwargs)</code>","text":"<p>Initialize the MixtralForCausalLMUpscalingAlgorithm.</p> <p>Parameters:</p> <ul> <li> <code>num_experts</code> \u00b6              (<code>int</code>)           \u2013            <p>The number of experts in the Mixtral model.</p> </li> <li> <code>experts_per_token</code> \u00b6              (<code>int</code>)           \u2013            <p>The number of experts per token.</p> </li> <li> <code>save_checkpoint</code> \u00b6              (<code>str</code>)           \u2013            <p>The path to save the checkpoint.</p> </li> <li> <code>**kwargs</code> \u00b6          \u2013            <p>Additional keyword arguments.</p> </li> </ul> Source code in <code>fusion_bench/method/mixture_of_experts/mixtral_upcycling.py</code> <pre><code>def __init__(\n    self,\n    num_experts: int,\n    experts_per_token: int,\n    save_checkpoint: str,\n    **kwargs,\n):\n    \"\"\"\n    Initialize the MixtralForCausalLMUpscalingAlgorithm.\n\n    Args:\n        num_experts (int): The number of experts in the Mixtral model.\n        experts_per_token (int): The number of experts per token.\n        save_checkpoint (str): The path to save the checkpoint.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    self.num_experts = num_experts\n    self.experts_per_token = experts_per_token\n    self.save_checkpoint = save_checkpoint\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"algorithms/moe_based_upscaling/#fusion_bench.method.mixture_of_experts.mixtral_upcycling.MixtralForCausalLMUpscalingAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Runs the upscaling process.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code> \u00b6              (<code>ModelPool | LlamaForCausalLM | MistralForCausalLM</code>)           \u2013            <p>The model to be upscaled.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MixtralForCausalLM</code> (              <code>MixtralForCausalLM</code> )          \u2013            <p>The upscaled model.</p> </li> </ul> Source code in <code>fusion_bench/method/mixture_of_experts/mixtral_upcycling.py</code> <pre><code>@torch.no_grad()\ndef run(\n    self, modelpool: BaseModelPool | LlamaForCausalLM | MistralForCausalLM\n) -&gt; MixtralForCausalLM:\n    \"\"\"\n    Runs the upscaling process.\n\n    Args:\n        modelpool (ModelPool | LlamaForCausalLM | MistralForCausalLM): The model to be upscaled.\n\n    Returns:\n        MixtralForCausalLM: The upscaled model.\n    \"\"\"\n    mixtral_model = self._run(modelpool)\n\n    if self.config.get(\"save_checkpoint\", None) is not None:\n        mixtral_model.save_pretrained(self.config.save_checkpoint)\n    return mixtral_model\n</code></pre>"},{"location":"algorithms/moe_based_upscaling/#fusion_bench.method.mixture_of_experts.mixtral_upcycling.MixtralUpscalingAlgorithm","title":"<code>MixtralUpscalingAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code></p> <p>This class is responsible for upscaling a model to a MixtralModel. It inherits from the ModelFusionAlgorithm class.</p> Source code in <code>fusion_bench/method/mixture_of_experts/mixtral_upcycling.py</code> <pre><code>class MixtralUpscalingAlgorithm(BaseAlgorithm):\n    \"\"\"\n    This class is responsible for upscaling a model to a MixtralModel.\n    It inherits from the ModelFusionAlgorithm class.\n    \"\"\"\n\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"num_experts\": \"num_experts\",\n        \"experts_per_token\": \"experts_per_token\",\n        \"save_checkpoint\": \"save_checkpoint\",\n    }\n\n    def __init__(\n        self,\n        num_experts: int,\n        experts_per_token: int,\n        save_checkpoint: str,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the MixtralUpscalingAlgorithm.\n\n        Args:\n            num_experts (int): The number of experts in the Mixtral model.\n            experts_per_token (int): The number of experts per token.\n            save_checkpoint (str): The path to save the checkpoint.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        self.num_experts = num_experts\n        self.experts_per_token = experts_per_token\n        self.save_checkpoint = save_checkpoint\n        super().__init__(**kwargs)\n\n    @torch.no_grad()\n    def _run(\n        self, modelpool: BaseModelPool | LlamaModel | MistralModel\n    ) -&gt; MixtralModel:\n        \"\"\"\n        Internal method to run the upscaling process.\n\n        Args:\n            modelpool (BaseModelPool | LlamaModel | MistralModel): The model to be upscaled.\n\n        Returns:\n            MixtralModel: The upscaled model.\n        \"\"\"\n        if isinstance(modelpool, BaseModelPool):\n            assert modelpool.has_pretrained, \"ModelPool must have pretrained model.\"\n            pretrained_model = modelpool.load_model(\"_pretrained_\")\n        elif isinstance(modelpool, (LlamaModel, MistralModel)):\n            pretrained_model = modelpool\n        else:\n            raise ValueError(\"Invalid modelpool type\")\n\n        mixtral_config = _convert_config_to_mixtral(\n            pretrained_model.config,\n            self.config.num_experts,\n            self.config.experts_per_token,\n        )\n\n        with ContextManagers([no_init_weights(True)]):\n            for _ in tqdm(range(1), desc=\"Initializing Mixtral model\"):\n                mixtral_model = MixtralModel(mixtral_config)\n        upscale_to_mixtral_model(pretrained_model, mixtral_model)\n\n        return mixtral_model\n\n    @torch.no_grad()\n    def run(self, modelpool: BaseModelPool | LlamaModel | MistralModel) -&gt; MixtralModel:\n        \"\"\"\n        Runs the upscaling process.\n\n        Args:\n            modelpool (ModelPool | LlamaModel | MistralModel): The model to be upscaled.\n\n        Returns:\n            MixtralModel: The upscaled model.\n        \"\"\"\n        mixtral_model = self._run(modelpool)\n\n        if self.config.get(\"save_checkpoint\", None) is not None:\n            mixtral_model.save_pretrained(self.config.save_checkpoint)\n        return mixtral_model\n</code></pre>"},{"location":"algorithms/moe_based_upscaling/#fusion_bench.method.mixture_of_experts.mixtral_upcycling.MixtralUpscalingAlgorithm.__init__","title":"<code>__init__(num_experts, experts_per_token, save_checkpoint, **kwargs)</code>","text":"<p>Initialize the MixtralUpscalingAlgorithm.</p> <p>Parameters:</p> <ul> <li> <code>num_experts</code> \u00b6              (<code>int</code>)           \u2013            <p>The number of experts in the Mixtral model.</p> </li> <li> <code>experts_per_token</code> \u00b6              (<code>int</code>)           \u2013            <p>The number of experts per token.</p> </li> <li> <code>save_checkpoint</code> \u00b6              (<code>str</code>)           \u2013            <p>The path to save the checkpoint.</p> </li> <li> <code>**kwargs</code> \u00b6          \u2013            <p>Additional keyword arguments.</p> </li> </ul> Source code in <code>fusion_bench/method/mixture_of_experts/mixtral_upcycling.py</code> <pre><code>def __init__(\n    self,\n    num_experts: int,\n    experts_per_token: int,\n    save_checkpoint: str,\n    **kwargs,\n):\n    \"\"\"\n    Initialize the MixtralUpscalingAlgorithm.\n\n    Args:\n        num_experts (int): The number of experts in the Mixtral model.\n        experts_per_token (int): The number of experts per token.\n        save_checkpoint (str): The path to save the checkpoint.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    self.num_experts = num_experts\n    self.experts_per_token = experts_per_token\n    self.save_checkpoint = save_checkpoint\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"algorithms/moe_based_upscaling/#fusion_bench.method.mixture_of_experts.mixtral_upcycling.MixtralUpscalingAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Runs the upscaling process.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code> \u00b6              (<code>ModelPool | LlamaModel | MistralModel</code>)           \u2013            <p>The model to be upscaled.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MixtralModel</code> (              <code>MixtralModel</code> )          \u2013            <p>The upscaled model.</p> </li> </ul> Source code in <code>fusion_bench/method/mixture_of_experts/mixtral_upcycling.py</code> <pre><code>@torch.no_grad()\ndef run(self, modelpool: BaseModelPool | LlamaModel | MistralModel) -&gt; MixtralModel:\n    \"\"\"\n    Runs the upscaling process.\n\n    Args:\n        modelpool (ModelPool | LlamaModel | MistralModel): The model to be upscaled.\n\n    Returns:\n        MixtralModel: The upscaled model.\n    \"\"\"\n    mixtral_model = self._run(modelpool)\n\n    if self.config.get(\"save_checkpoint\", None) is not None:\n        mixtral_model.save_pretrained(self.config.save_checkpoint)\n    return mixtral_model\n</code></pre>"},{"location":"algorithms/moe_based_upscaling/#fusion_bench.method.mixture_of_experts.mixtral_upcycling.upscale_to_mixtral_for_causal_lm","title":"<code>upscale_to_mixtral_for_causal_lm(input_model, output_model)</code>","text":"<p>A helper function.</p> <p>Upscales a LlamaForCausalLM or MistralForCausalLM to a MixtralForCausalLM.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li>           \u2013            <p>None</p> </li> </ul> Source code in <code>fusion_bench/method/mixture_of_experts/mixtral_upcycling.py</code> <pre><code>def upscale_to_mixtral_for_causal_lm(\n    input_model: LlamaForCausalLM | MistralForCausalLM, output_model: MixtralForCausalLM\n):\n    \"\"\"\n    A helper function.\n\n    Upscales a LlamaForCausalLM or MistralForCausalLM to a MixtralForCausalLM.\n\n    Args:\n        input_model (LlamaForCausalLM | MistralForCausalLM): The input model to be upscaled.\n        output_model (MixtralForCausalLM): The output model where the upscaled weights will be loaded.\n\n    Returns:\n        None\n    \"\"\"\n    output_model.lm_head.load_state_dict(input_model.lm_head.state_dict())\n    upscale_to_mixtral_model(input_model.model, output_model.model)\n</code></pre>"},{"location":"algorithms/moe_based_upscaling/#fusion_bench.method.mixture_of_experts.mixtral_upcycling.upscale_to_mixtral_for_causal_lm(input_model)","title":"<code>input_model</code>","text":"(<code>LlamaForCausalLM | MistralForCausalLM</code>)           \u2013            <p>The input model to be upscaled.</p>"},{"location":"algorithms/moe_based_upscaling/#fusion_bench.method.mixture_of_experts.mixtral_upcycling.upscale_to_mixtral_for_causal_lm(output_model)","title":"<code>output_model</code>","text":"(<code>MixtralForCausalLM</code>)           \u2013            <p>The output model where the upscaled weights will be loaded.</p>"},{"location":"algorithms/moe_based_upscaling/#fusion_bench.method.mixture_of_experts.mixtral_upcycling.upscale_to_mixtral_model","title":"<code>upscale_to_mixtral_model(input_model, output_model)</code>","text":"<p>A helper function.</p> <p>Upscales a LlamaModel or MistralModel to a MixtralModel.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li>           \u2013            <p>None</p> </li> </ul> Source code in <code>fusion_bench/method/mixture_of_experts/mixtral_upcycling.py</code> <pre><code>def upscale_to_mixtral_model(\n    input_model: LlamaModel | MistralModel, output_model: MixtralModel\n):\n    \"\"\"\n    A helper function.\n\n    Upscales a LlamaModel or MistralModel to a MixtralModel.\n\n    Args:\n        input_model (LlamaModel | MistralModel): The input model to be upscaled.\n        output_model (MixtralModel): The output model where the upscaled weights will be loaded.\n\n    Returns:\n        None\n    \"\"\"\n    # copy the weights from the pretrained model\n    output_model.embed_tokens.load_state_dict(input_model.embed_tokens.state_dict())\n    output_model.norm.load_state_dict(input_model.norm.state_dict())\n    for input_layer, output_layer in tqdm(\n        zip(input_model.layers, output_model.layers),\n        desc=\"Upscaling layers\",\n        total=len(input_model.layers),\n    ):\n        _upscale_decoder_layer(input_layer, output_layer)\n</code></pre>"},{"location":"algorithms/moe_based_upscaling/#fusion_bench.method.mixture_of_experts.mixtral_upcycling.upscale_to_mixtral_model(input_model)","title":"<code>input_model</code>","text":"(<code>LlamaModel | MistralModel</code>)           \u2013            <p>The input model to be upscaled.</p>"},{"location":"algorithms/moe_based_upscaling/#fusion_bench.method.mixture_of_experts.mixtral_upcycling.upscale_to_mixtral_model(output_model)","title":"<code>output_model</code>","text":"(<code>MixtralModel</code>)           \u2013            <p>The output model where the upscaled weights will be loaded.</p>"},{"location":"algorithms/pwe_moe/","title":"PWEMoE: Pareto-Driven Weight-Ensembling Mixture of Experts","text":"Overview of PWE MoE     (a) An illustration of Pareto front learning in MOOP. Where \\(P_1\\) and \\(P_2\\) are performance metrics for two tasks, colored lines represent different Pareto optimal solutions, and the solid black line represents the Pareto front.     (b) An overview of the model up-scaling process.     We upcycle the MLP modules to MoE modules and merge the remaining parts using task arithmetic.     (c) The MoE module, comprising a routing network and a parameter decoder network.     The routing network accepts a user preference vector and generates routing weights for weight-ensembling. <p>Abstract</p> <p>Solving multi-objective optimization problems for large deep neural networks is a challenging task due to the complexity of the loss landscape and the expensive computational cost of training and evaluating models. Efficient Pareto front approximation of large models enables multi-objective optimization for various tasks such as multi-task learning and trade-off analysis. Existing algorithms for learning Pareto set, including (1) evolutionary, hypernetworks, and hypervolume-maximization methods, are computationally expensive and have restricted scalability to large models; (2) Scalarization algorithms, where a separate model is trained for each objective ray, which is inefficient for learning the entire Pareto set and fails to capture the objective trade-offs effectively. Inspired by the recent success of model merging, we propose a practical and scalable approach to Pareto set learning problem via mixture of experts (MoE) based model fusion. By ensembling the weights of specialized single-task models, the MoE module can effectively capture the trade-offs between multiple objectives and closely approximate the entire Pareto set of large neural networks. Once the routers are learned and a preference vector is set, the MoE module can be unloaded, thus no additional computational cost is introduced during inference. We conduct extensive experiments on vision and language tasks using large-scale models such as CLIP-ViT and GPT-2. The experimental results demonstrate that our method efficiently approximates the entire Pareto front of large models. Using only hundreds of trainable parameters of the MoE routers, our method even has lower memory usage compared to linear scalarization and algorithms that learn a single Pareto optimal solution, and are scalable to both the number of objectives and the size of the model. Our method significantly reduces the computational burden of learning the Pareto set, for example, in the two-task case, it can be achieved in just a few minutes. Code is available at: GitHub .</p>"},{"location":"algorithms/pwe_moe/#examples","title":"Examples","text":"<p>Not tested yet</p> <p>The examples provided below have not been tested yet.</p> <p>For a thoroughly tested and verified implementation of the algorithm, please refer to the original repository: tanganke/pareto_set_learning .  Additionally, the experimental results and further insights into the algorithm can be found in the original research paper: arXiv:2406.09770 .</p> <p>PWEMoE-LS on eight image classification tasks using CLIP-ViT-B/32 models, and the results are logged to <code>outputs/logs/ViT-B-32/PWEMoE-LS-8tasks</code>.</p> <pre><code>fusion_bench \\\n    method=pwe_moe_ls_for_clip \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \\\n    fabric.loggers.root_dir=outputs/logs/ViT-B-32 \\\n    fabric.loggers.name=PWEMoE-LS-8tasks\n</code></pre>"},{"location":"algorithms/pwe_moe/#references","title":"References","text":""},{"location":"algorithms/pwe_moe/#fusion_bench.method.pwe_moe.clip_pwe_moe","title":"<code>clip_pwe_moe</code>","text":""},{"location":"algorithms/pwe_moe/#fusion_bench.method.pwe_moe.clip_pwe_moe.PWEMoEAlgorithmForCLIP","title":"<code>PWEMoEAlgorithmForCLIP</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>SimpleProfilerMixin</code>, <code>CLIPClassificationMixin</code></p> Source code in <code>fusion_bench/method/pwe_moe/clip_pwe_moe.py</code> <pre><code>class PWEMoEAlgorithmForCLIP(\n    BaseAlgorithm,\n    SimpleProfilerMixin,\n    CLIPClassificationMixin,\n):\n    modelpool: CLIPVisionModelPool = None\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"upscale_mlp\": \"upscale_mlp\",\n        \"upscale_attn\": \"upscale_attn\",\n        \"init_lambda\": \"init_lambda\",\n        \"router_hidden_layers\": \"router_hidden_layers\",\n        \"lr\": \"lr\",\n        \"num_steps\": \"num_steps\",\n        \"save_interval\": \"save_interval\",\n        \"alpha\": \"alpha\",\n        \"checkpoint_path\": \"checkpoint_path\",\n        \"eval_grid\": \"eval_grid\",\n        \"eval_grid_n\": \"eval_grid_n\",\n        \"eval_grid_m\": \"eval_grid_m\",\n        \"_dataloader_kwargs\": \"dataloader_kwargs\",\n    }\n\n    def __init__(\n        self,\n        *,\n        upscale_mlp: bool,\n        upscale_attn: bool,\n        init_lambda: float,\n        router_hidden_layers: int,\n        lr: float,\n        num_steps: int,\n        save_interval: int,\n        alpha: float,\n        checkpoint_path: str,\n        eval_grid: bool,\n        eval_grid_n: int,\n        eval_grid_m: int,\n        dataloader_kwargs: DictConfig,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.upscale_mlp = upscale_mlp\n        self.upscale_attn = upscale_attn\n        self.init_lambda = init_lambda\n        self.router_hidden_layers = router_hidden_layers\n        self.lr = lr\n        self.num_steps = num_steps\n        self.save_interval = save_interval\n        self.alpha = alpha\n        self.checkpoint_path = checkpoint_path\n        self.eval_grid = eval_grid\n        self.eval_grid_n = eval_grid_n\n        self.eval_grid_m = eval_grid_m\n        self._dataloader_kwargs = dataloader_kwargs\n\n    @override\n    def run(self, modelpool: CLIPVisionModelPool):\n        self.modelpool = modelpool\n\n        model = self.setup_model()\n        if self.checkpoint_path is not None:\n            model.load_state_dict(torch.load(self.checkpoint_path, map_location=\"cpu\"))\n        else:\n            train_loaders = self.setup_train_loaders()\n            model = self.train(model, train_loaders)\n\n        if self.eval_grid:\n            return map(\n                lambda m, r: {\n                    \"model\": ParetoWeightEnsemblingModule.set_preferenece_vector(\n                        m,\n                        torch.as_tensor(\n                            r, device=self.fabric.device, dtype=torch.float32\n                        ),\n                    ),\n                    \"preference_vector\": r,\n                },\n                itertools.cycle([model]),\n                generate_simplex_grid(self.eval_grid_n, self.eval_grid_m),\n            )\n        return model\n\n    def load_clip_models(self):\n        \"\"\"\n        Loads the pretrained CLIP model and the fine-tuned models for each dataset specified in the configuration.\n        \"\"\"\n        # load pretrained and fine-tuned model\n        with timeit_context():\n            log.info(\"load models\")\n            pretrained_model: CLIPVisionModel = self.modelpool.load_model(\n                \"_pretrained_\"\n            )\n            finetuned_models = {\n                model_name: self.modelpool.load_model(model_name)\n                for model_name in self.modelpool.model_names\n            }\n\n        log.info(\"pretrained model statistics:\")\n        print_parameters(pretrained_model)\n        return pretrained_model, finetuned_models\n\n    def setup_model(self):\n        pretrained_model, finetuned_models = self.load_clip_models()\n        self.setup_zero_shot_classification_head()\n\n        with timeit_context(\"Building PWEMoE model\"):\n            model = deepcopy(pretrained_model)\n\n            # merge the remaining layers using task arithmetic\n            if self.init_lambda != 0:\n                task_arithmetic_merge(\n                    model,\n                    finetuned_models.values(),\n                    scaling_factor=self.init_lambda,\n                    inplace=True,\n                )\n            # fix all parameters\n            model.requires_grad_(False)\n\n            num_layers = len(model.vision_model.encoder.layers)\n\n            def get_layer(m, i):\n                return cast(CLIPEncoderLayer, m.vision_model.encoder.layers[i])\n\n            for layer_idx in tqdm(range(num_layers)):\n                if self.upscale_mlp:\n                    # upscale the mlp layer\n                    get_layer(model, layer_idx).mlp = ParetoWeightEnsemblingModule(\n                        base_model=get_layer(pretrained_model, layer_idx).mlp,\n                        expert_models=[\n                            get_layer(m, layer_idx).mlp\n                            for m in finetuned_models.values()\n                        ],\n                        init_lambda=self.init_lambda,\n                        fix_base_model_and_experts=True,\n                        router_hidden_layers=self.router_hidden_layers,\n                    )\n\n                if self.upscale_attn:\n                    # upscale the Attention layer\n                    get_layer(model, layer_idx).self_attn = (\n                        ParetoWeightEnsemblingModule(\n                            base_model=get_layer(pretrained_model, layer_idx).self_attn,\n                            expert_models=[\n                                get_layer(m, layer_idx).self_attn\n                                for m in finetuned_models.values()\n                            ],\n                            init_lambda=self.init_lambda,\n                            fix_base_model_and_experts=True,\n                            router_hidden_layers=self.router_hidden_layers,\n                        )\n                    )\n\n            print(\"model statistics after upscaling:\")\n            print_parameters(model)\n            return model\n\n    def setup_train_loaders(self):\n        \"\"\"\n        Loads the datasets specified in the configuration.\n        \"\"\"\n        train_datasets = {\n            dataset_name: self.modelpool.load_train_dataset(\n                dataset_name, self.clip_processor\n            )\n            for dataset_name in self.modelpool.model_names\n        }\n        train_loaders = {\n            dataset_name: DataLoader(dataset, shuffle=True, **self._dataloader_kwargs)\n            for dataset_name, dataset in train_datasets.items()\n        }\n        train_loaders = {\n            dataset_name: self.fabric.setup_dataloaders(loader)\n            for dataset_name, loader in train_loaders.items()\n        }\n        return train_loaders\n\n    def train(self, model: nn.Module, train_loaders: Dict[str, DataLoader]):\n        config = self.config\n\n        # save the configuration\n        self.log_hyperparams(config, filename=\"method_config.yaml\")\n\n        # setup the model\n        num_objectives = len(self.modelpool.model_names)\n        model = model\n\n        # setup data loaders\n        train_loaders = {\n            name: InfiniteDataLoader(loader) for name, loader in train_loaders.items()\n        }\n\n        # set up the optimizer and learning rate scheduler\n        optimizer = torch.optim.Adam(\n            filter(lambda p: p.requires_grad, model.parameters()),\n            lr=config.lr,\n        )\n        model, optimizer = self.fabric.setup(model, optimizer)\n        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer=optimizer, T_max=config.num_steps, eta_min=config.lr * 0.1\n        )\n\n        model.train()\n        device = self.fabric.device\n        for step_idx in tqdm(\n            range(1, 1 + config.num_steps), \"training\", dynamic_ncols=True\n        ):\n            # sample a preference ray\n            ray = torch.from_numpy(\n                np.random.dirichlet((config.alpha,) * num_objectives, 1)\n                .astype(np.float32)\n                .flatten()\n            ).to(device)\n            ParetoWeightEnsemblingModule.set_preferenece_vector(model, ray)\n\n            losses = []\n            for dataset_idx, dataset_name in enumerate(train_loaders):\n                batch = next(train_loaders[dataset_name])\n                images, labels = batch\n\n                logits = self.compute_logits(model, images, dataset_name)\n                _loss = F.cross_entropy(logits, labels)\n                losses.append(_loss)\n\n            loss = self.compute_loss(model, ray, losses)\n\n            optimizer.zero_grad()\n            self.fabric.backward(loss)\n            optimizer.step()\n\n            lr_scheduler.step()\n\n            self.fabric.log(\"train/loss\", loss.item(), step=step_idx)\n\n            if step_idx % config.save_interval == 0:\n                (Path(self.log_dir) / \"checkpoints\").mkdir(exist_ok=True, parents=True)\n                save_path = (\n                    Path(self.log_dir) / \"checkpoints\" / f\"model_step={step_idx}.pt\"\n                )\n                torch.save(model.state_dict(), save_path)\n\n        return model\n\n    @abstractmethod\n    def compute_loss(\n        self, model: nn.Module, ray: Tensor, losses: List[Tensor]\n    ) -&gt; Tensor:\n        \"\"\"\n        Computes the overall losses using the given preference ray.\n\n        Args:\n            model (nn.Module): The model being trained.\n            ray (Tensor): A tensor representing the preference ray, which contains the weights for each objective.\n            losses (List[Tensor]): A list of loss values for each objective.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"algorithms/pwe_moe/#fusion_bench.method.pwe_moe.clip_pwe_moe.PWEMoEAlgorithmForCLIP.compute_loss","title":"<code>compute_loss(model, ray, losses)</code>  <code>abstractmethod</code>","text":"<p>Computes the overall losses using the given preference ray.</p> <p>Parameters:</p> <ul> <li> <code>model</code> \u00b6              (<code>Module</code>)           \u2013            <p>The model being trained.</p> </li> <li> <code>ray</code> \u00b6              (<code>Tensor</code>)           \u2013            <p>A tensor representing the preference ray, which contains the weights for each objective.</p> </li> <li> <code>losses</code> \u00b6              (<code>List[Tensor]</code>)           \u2013            <p>A list of loss values for each objective.</p> </li> </ul> Source code in <code>fusion_bench/method/pwe_moe/clip_pwe_moe.py</code> <pre><code>@abstractmethod\ndef compute_loss(\n    self, model: nn.Module, ray: Tensor, losses: List[Tensor]\n) -&gt; Tensor:\n    \"\"\"\n    Computes the overall losses using the given preference ray.\n\n    Args:\n        model (nn.Module): The model being trained.\n        ray (Tensor): A tensor representing the preference ray, which contains the weights for each objective.\n        losses (List[Tensor]): A list of loss values for each objective.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"algorithms/pwe_moe/#fusion_bench.method.pwe_moe.clip_pwe_moe.PWEMoEAlgorithmForCLIP.load_clip_models","title":"<code>load_clip_models()</code>","text":"<p>Loads the pretrained CLIP model and the fine-tuned models for each dataset specified in the configuration.</p> Source code in <code>fusion_bench/method/pwe_moe/clip_pwe_moe.py</code> <pre><code>def load_clip_models(self):\n    \"\"\"\n    Loads the pretrained CLIP model and the fine-tuned models for each dataset specified in the configuration.\n    \"\"\"\n    # load pretrained and fine-tuned model\n    with timeit_context():\n        log.info(\"load models\")\n        pretrained_model: CLIPVisionModel = self.modelpool.load_model(\n            \"_pretrained_\"\n        )\n        finetuned_models = {\n            model_name: self.modelpool.load_model(model_name)\n            for model_name in self.modelpool.model_names\n        }\n\n    log.info(\"pretrained model statistics:\")\n    print_parameters(pretrained_model)\n    return pretrained_model, finetuned_models\n</code></pre>"},{"location":"algorithms/pwe_moe/#fusion_bench.method.pwe_moe.clip_pwe_moe.PWEMoEAlgorithmForCLIP.setup_train_loaders","title":"<code>setup_train_loaders()</code>","text":"<p>Loads the datasets specified in the configuration.</p> Source code in <code>fusion_bench/method/pwe_moe/clip_pwe_moe.py</code> <pre><code>def setup_train_loaders(self):\n    \"\"\"\n    Loads the datasets specified in the configuration.\n    \"\"\"\n    train_datasets = {\n        dataset_name: self.modelpool.load_train_dataset(\n            dataset_name, self.clip_processor\n        )\n        for dataset_name in self.modelpool.model_names\n    }\n    train_loaders = {\n        dataset_name: DataLoader(dataset, shuffle=True, **self._dataloader_kwargs)\n        for dataset_name, dataset in train_datasets.items()\n    }\n    train_loaders = {\n        dataset_name: self.fabric.setup_dataloaders(loader)\n        for dataset_name, loader in train_loaders.items()\n    }\n    return train_loaders\n</code></pre>"},{"location":"algorithms/pwe_moe/#fusion_bench.method.pwe_moe.clip_pwe_moe.PWEMoELinearScalarizationForCLIP","title":"<code>PWEMoELinearScalarizationForCLIP</code>","text":"<p>               Bases: <code>PWEMoEAlgorithmForCLIP</code></p> Source code in <code>fusion_bench/method/pwe_moe/clip_pwe_moe.py</code> <pre><code>class PWEMoELinearScalarizationForCLIP(PWEMoEAlgorithmForCLIP):\n    def compute_loss(self, model, ray, losses):\n        loss = 0\n        for r, l in zip(ray, losses):\n            loss += r * l\n        return loss\n</code></pre>"},{"location":"algorithms/pwe_moe/#fusion_bench.method.pwe_moe.clip_pwe_moe.PWEMoExactParetoOptimalForCLIP","title":"<code>PWEMoExactParetoOptimalForCLIP</code>","text":"<p>               Bases: <code>PWEMoEAlgorithmForCLIP</code></p> Source code in <code>fusion_bench/method/pwe_moe/clip_pwe_moe.py</code> <pre><code>class PWEMoExactParetoOptimalForCLIP(PWEMoEAlgorithmForCLIP):\n    def compute_loss(self, model: nn.Module, ray: Tensor, losses: Tuple[Tensor]):\n        from phn.solvers import EPOSolver\n\n        if self.epo_solver is None:\n            num_objectives = len(self.finetuned_models)\n            self.epo_solver = EPOSolver(n_tasks=num_objectives, n_params=None)\n        epo_solver = self.epo_solver\n\n        losses = torch.stack(losses)\n        loss = epo_solver.get_weighted_loss(\n            losses,\n            ray,\n            tuple(filter(lambda p: p.requires_grad, model.parameters())),\n        )\n        return loss\n</code></pre>"},{"location":"algorithms/regmean/","title":"RegMean","text":""},{"location":"algorithms/regmean/#code-integration","title":"Code Integration","text":"<p>Merge CLIP-ViT-B/32 models on eight image classification tasks</p> <pre><code>fusion_bench method=clip_regmean \\\n  modelpool=clip-vit-base-patch32_TA8 \\\n  taskpool=clip-vit-classification_TA8\n</code></pre> <p>Merge CLIP-ViT-L/14 models on eight image classification tasks</p> <pre><code>fusion_bench \\\n  method=clip_regmean \\\n    method.batch_size=8 method.num_workers=4 \\\n  modelpool=clip-vit-large-patch14_TA8 \\\n  taskpool=clip-vit-classification_TA8 \\\n    taskpool.clip_model=openai/clip-vit-large-patch14\n</code></pre> <p>Merge GPT-2 models for text classification tasks:</p> <pre><code>fusion_bench \\\n  method=gpt2_regmean \\\n  modelpool=gpt-2_glue \\\n  taskpool=gpt-2_glue\n</code></pre>"},{"location":"algorithms/regmean/#references","title":"References","text":"<ol> <li> <p>Xisen Jin, et al. \"Dataless Knowledge Fusion by Merging Weights of Language Models.\" http://arxiv.org/abs/2212.09849\u00a0\u21a9</p> </li> </ol>"},{"location":"algorithms/simple_averaging/","title":"Simple Averaging","text":"<p>Simple averaging is known in the literature as isotropic merging, ModelSoups, aims to yield a more robust and generalizable model. Simple Averaging is a technique frequently employed when there are multiple models that have been fine-tuned or independently trained from scratch.  Specifically, if we possess \\(n\\) models that share a common architecture but different weights denoted as \\(\\theta_i\\), the weights of the merged model, represented as \\(\\theta\\), are computed as follows:</p> \\[ \\theta = \\frac{1}{n} \\sum_{i=1}^{n} \\theta_i \\] <p>This equation simply states that each weight of the final model is the average of the corresponding weights in the individual models. For example, if we have three models and the weight of the first neuron in the first layer is 0.1, 0.2, and 0.3 in each model respectively, the weight of that neuron in the final model will be (0.1 + 0.2 + 0.3) / 3 = 0.2.</p> <p>Simple averaging is a straightforward and scalable method for model fusion. It does not require any additional training or fine-tuning, making it a good choice when computational resources are limited, where maintaining an ensemble of models is not feasible.</p> <p>This method often assumes that all models are equally good.  If some models are significantly better than others, it might be beneficial to assign more weight to the better models when averaging.  This can be done by using weighted averaging, where each model's contribution to the final model is weighted by its performance on a validation set or some other metric. See Weighed Averaging for more details. Otherwise, the poor model may have a negative impact on the merged model.</p>"},{"location":"algorithms/simple_averaging/#examples","title":"Examples","text":"<p>In this example, we will demonstrate how to use the <code>SimpleAverageAlgorithm</code> class from the <code>fusion_bench.method</code> module.  This algorithm is used to merge multiple models by averaging their parameters.</p> <pre><code>from fusion_bench.method.simple_average import SimpleAverageAlgorithm\n\n# Instantiate the SimpleAverageAlgorithm\n# This algorithm will be used to merge multiple models by averaging their parameters.\nalgorithm = SimpleAverageAlgorithm()\n\n# Assume we have a list of PyTorch models (nn.Module instances) that we want to merge.\n# The models should all have the same architecture.\nmodels = [...]\n\n# Run the algorithm on the models.\n# This will return a new model that is the result of averaging the parameters of the input models.\nmerged_model = algorithm.run(models)\n</code></pre> <p>The <code>run</code> method of the <code>SimpleAverageAlgorithm</code> class takes a list of models as input and returns a new model.  The new model's parameters are the average of the parameters of the input models.  This is useful in scenarios where you have trained multiple models and want to combine them into a single model that hopefully performs better than any individual model.</p>"},{"location":"algorithms/simple_averaging/#code-integration","title":"Code Integration","text":"<p>Configuration template for the Simple Averaging algorithm:</p> config/method/simple_average.yaml<pre><code>name: simple_average\n</code></pre> <p>use the following command to run the Simple Averaging algorithm:</p> <pre><code>fusion_bench method=simple_average ...\n</code></pre>"},{"location":"algorithms/simple_averaging/#references","title":"References","text":""},{"location":"algorithms/simple_averaging/#fusion_bench.method.simple_average.SimpleAverageAlgorithm","title":"<code>SimpleAverageAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>SimpleProfilerMixin</code></p> Source code in <code>fusion_bench/method/simple_average.py</code> <pre><code>class SimpleAverageAlgorithm(\n    BaseAlgorithm,\n    SimpleProfilerMixin,\n):\n    @torch.no_grad()\n    def run(self, modelpool: Union[BaseModelPool, Dict[str, nn.Module]]):\n        \"\"\"\n        Fuse the models in the given model pool using simple averaging.\n\n        This method iterates over the names of the models in the model pool, loads each model, and appends it to a list.\n        It then returns the simple average of the models in the list.\n\n        Args:\n            modelpool: The pool of models to fuse.\n\n        Returns:\n            The fused model obtained by simple averaging.\n        \"\"\"\n        if isinstance(modelpool, dict):\n            modelpool = BaseModelPool(modelpool)\n\n        log.info(\n            f\"Fusing models using simple average on {len(modelpool.model_names)} models.\"\n            f\"models: {modelpool.model_names}\"\n        )\n        sd: Optional[StateDictType] = None\n        forward_model = None\n        merged_model_names = []\n\n        for model_name in modelpool.model_names:\n            with self.profile(\"load model\"):\n                model = modelpool.load_model(model_name)\n                merged_model_names.append(model_name)\n                print(f\"load model of type: {type(model).__name__}\")\n            with self.profile(\"merge weights\"):\n                if sd is None:\n                    # Initialize the state dictionary with the first model's state dictionary\n                    sd = model.state_dict(keep_vars=True)\n                    forward_model = model\n                else:\n                    # Add the current model's state dictionary to the accumulated state dictionary\n                    sd = state_dict_add(sd, model.state_dict(keep_vars=True))\n        with self.profile(\"merge weights\"):\n            # Divide the accumulated state dictionary by the number of models to get the average\n            sd = state_dict_div(sd, len(modelpool.model_names))\n\n        forward_model.load_state_dict(sd)\n        # print profile report and log the merged models\n        self.print_profile_summary()\n        log.info(f\"merged {len(merged_model_names)} models:\")\n        for model_name in merged_model_names:\n            log.info(f\"  - {model_name}\")\n        return forward_model\n</code></pre>"},{"location":"algorithms/simple_averaging/#fusion_bench.method.simple_average.SimpleAverageAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Fuse the models in the given model pool using simple averaging.</p> <p>This method iterates over the names of the models in the model pool, loads each model, and appends it to a list. It then returns the simple average of the models in the list.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li>           \u2013            <p>The fused model obtained by simple averaging.</p> </li> </ul> Source code in <code>fusion_bench/method/simple_average.py</code> <pre><code>@torch.no_grad()\ndef run(self, modelpool: Union[BaseModelPool, Dict[str, nn.Module]]):\n    \"\"\"\n    Fuse the models in the given model pool using simple averaging.\n\n    This method iterates over the names of the models in the model pool, loads each model, and appends it to a list.\n    It then returns the simple average of the models in the list.\n\n    Args:\n        modelpool: The pool of models to fuse.\n\n    Returns:\n        The fused model obtained by simple averaging.\n    \"\"\"\n    if isinstance(modelpool, dict):\n        modelpool = BaseModelPool(modelpool)\n\n    log.info(\n        f\"Fusing models using simple average on {len(modelpool.model_names)} models.\"\n        f\"models: {modelpool.model_names}\"\n    )\n    sd: Optional[StateDictType] = None\n    forward_model = None\n    merged_model_names = []\n\n    for model_name in modelpool.model_names:\n        with self.profile(\"load model\"):\n            model = modelpool.load_model(model_name)\n            merged_model_names.append(model_name)\n            print(f\"load model of type: {type(model).__name__}\")\n        with self.profile(\"merge weights\"):\n            if sd is None:\n                # Initialize the state dictionary with the first model's state dictionary\n                sd = model.state_dict(keep_vars=True)\n                forward_model = model\n            else:\n                # Add the current model's state dictionary to the accumulated state dictionary\n                sd = state_dict_add(sd, model.state_dict(keep_vars=True))\n    with self.profile(\"merge weights\"):\n        # Divide the accumulated state dictionary by the number of models to get the average\n        sd = state_dict_div(sd, len(modelpool.model_names))\n\n    forward_model.load_state_dict(sd)\n    # print profile report and log the merged models\n    self.print_profile_summary()\n    log.info(f\"merged {len(merged_model_names)} models:\")\n    for model_name in merged_model_names:\n        log.info(f\"  - {model_name}\")\n    return forward_model\n</code></pre>"},{"location":"algorithms/simple_averaging/#fusion_bench.method.simple_average.SimpleAverageAlgorithm.run(modelpool)","title":"<code>modelpool</code>","text":"(<code>Union[BaseModelPool, Dict[str, Module]]</code>)           \u2013            <p>The pool of models to fuse.</p>"},{"location":"algorithms/simple_ensemble/","title":"Simple Ensemble","text":"<p>Ensemble methods are simple and effective ways to improve the performance of machine learning models.  They combine the outputs of multiple models to create a stronger model. </p>"},{"location":"algorithms/simple_ensemble/#examples","title":"Examples","text":"<pre><code>from fusion_bench.method import EnsembleAlgorithm\n\n# Instantiate the EnsembleAlgorithm\nalgorithm = EnsembleAlgorithm()\n\n# Assume we have a list of PyTorch models (nn.Module instances) that we want to ensemble.\nmodels = [...]\n\n# Run the algorithm on the models.\nmerged_model = algorithm.run(models)\n</code></pre>"},{"location":"algorithms/simple_ensemble/#code-integration","title":"Code Integration","text":"<p>Configuration template for the ensemble algorithm:</p> config/method/simple_ensemble.yaml<pre><code>name: simple_ensemble\n</code></pre> <p>create a simple ensemble of CLIP-ViT models for image classification tasks.</p> <pre><code>fusion_bench \\\n  method=ensemble/simple_ensemble \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \n</code></pre>"},{"location":"algorithms/simple_ensemble/#references","title":"References","text":""},{"location":"algorithms/simple_ensemble/#fusion_bench.method.SimpleEnsembleAlgorithm","title":"<code>SimpleEnsembleAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code></p> Source code in <code>fusion_bench/method/ensemble.py</code> <pre><code>class SimpleEnsembleAlgorithm(BaseAlgorithm):\n    @torch.no_grad()\n    def run(self, modelpool: BaseModelPool | List[nn.Module]):\n        \"\"\"\n        Run the simple ensemble algorithm on the given model pool.\n\n        Args:\n            modelpool (BaseModelPool | List[nn.Module]): The pool of models to ensemble.\n\n        Returns:\n            EnsembleModule: The ensembled model.\n        \"\"\"\n        log.info(f\"Running ensemble algorithm with {len(modelpool)} models\")\n\n        models = [modelpool.load_model(m) for m in modelpool.model_names]\n        ensemble = EnsembleModule(models=models)\n        return ensemble\n</code></pre>"},{"location":"algorithms/simple_ensemble/#fusion_bench.method.SimpleEnsembleAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Run the simple ensemble algorithm on the given model pool.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>EnsembleModule</code>          \u2013            <p>The ensembled model.</p> </li> </ul> Source code in <code>fusion_bench/method/ensemble.py</code> <pre><code>@torch.no_grad()\ndef run(self, modelpool: BaseModelPool | List[nn.Module]):\n    \"\"\"\n    Run the simple ensemble algorithm on the given model pool.\n\n    Args:\n        modelpool (BaseModelPool | List[nn.Module]): The pool of models to ensemble.\n\n    Returns:\n        EnsembleModule: The ensembled model.\n    \"\"\"\n    log.info(f\"Running ensemble algorithm with {len(modelpool)} models\")\n\n    models = [modelpool.load_model(m) for m in modelpool.model_names]\n    ensemble = EnsembleModule(models=models)\n    return ensemble\n</code></pre>"},{"location":"algorithms/simple_ensemble/#fusion_bench.method.SimpleEnsembleAlgorithm.run(modelpool)","title":"<code>modelpool</code>","text":"(<code>BaseModelPool | List[Module]</code>)           \u2013            <p>The pool of models to ensemble.</p>"},{"location":"algorithms/smile_upscaling/","title":"SMILE Upscaling","text":"The architecture of the Sparse MIxture of Low-rank Experts (SMILE) module.<sup>1</sup>"},{"location":"algorithms/smile_upscaling/#taxonomy-for-smile-upscaling","title":"Taxonomy for SMILE Upscaling","text":"<p>Here we present the taxonomy for the SMILE upscaling method following \"A Survey on Model MoErging\" by Yadav et al. (2024) <sup>2</sup>.</p> Expert Training Standard Expert Data Private Routing Dataset None Input Granularity Step Depth Granularity Module Expert Selection Sparse Expert Aggregation Output Generalization In-Distribution User Dataset Zero-Shot"},{"location":"algorithms/smile_upscaling/#configurations","title":"Configurations","text":"<p>The SMILE upscaling method offers several configuration options, which are located in the <code>config/method/</code> directory.</p> <ol> <li>General <code>nn.Module</code> Upscaling:      This configuration is designed for upscaling any neural network module (<code>nn.Module</code>).</li> <li>Mistral Model Upscaling:      This specific configuration is for Mistral models.</li> </ol> <p>Each configuration file contains detailed parameters and options that can be adjusted to meet the specific needs of your model and application.</p> config/method/smile_upscaling.yaml<pre><code>name: smile_upscaling\n\n# merge device on cuda can accelerate the SVD computation\ndevice: cpu\n# device to compute svd\nupscaling_accelerator: cuda\nfull_matrices: true # set to false if you are sure k &lt; rank\n\ngate_k: 1\nk: 128\ntop_k: 1\n\nrouting_use_diff: true\n# average the remaining part, if this is set the False, the remaining part will kept as base model (the pretrained model)\naverage_experts: false\n\n# path to save/load the model\nmodel_path: null\n</code></pre> config/method/smile_mistral_upscaling.yaml<pre><code>name: smile_mistral_upscaling\n\ndevice: cpu\naccelerator: cuda\n\n# path to save/load the model\nmodel_path: null\nmodel_dtype: float16\n\nnum_experts_per_tok: 1\nrank_of_router: 8\nrank_of_expert: 512\n</code></pre>"},{"location":"algorithms/smile_upscaling/#examples","title":"Examples","text":""},{"location":"algorithms/smile_upscaling/#clip-vit-b32-on-eight-tasks","title":"CLIP-ViT-B/32 on eight tasks","text":"<p>Evaluate single fine-tuned models and save the results to <code>outputs/ViT-B-32/single-task/</code> and <code>outputs/ViT-L-14/single-task/</code> for CLIP-ViT-B/32 and CLIP-ViT-L/14 models, respectively.</p> <pre><code># evaluate singlue fine-tuned models\nfor task in sun397 stanford-cars resisc45 eurosat svhn gtsrb mnist dtd\ndo\n    fusion_bench method=dummy \\\n        modelpool=clip-vit-base-patch32_individual \\\n            modelpool.models.0.path=tanganke/clip-vit-base-patch32_${task} \\\n        taskpool=clip-vit-classification_TA8 \\\n        report_save_path=\"outputs/ViT-B-32/single-task/clip-vit-base-patch32_${task}.json\"\ndone\n\n# if you have multiple GPUs, you can run the following code to evaluate the CLIP-ViT-L/14 models in parallel\n# evaluate singlue fine-tuned models clip-vit-large\ntasks=(sun397 stanford-cars resisc45 eurosat svhn gtsrb mnist dtd)\nCUDA_DEVICES=(0 1 2 3 4 5 6 7)  # List of CUDA devices to use\n\nfor i in \"${!CUDA_DEVICES[@]}\"; do\n    task=${tasks[$i]}\n    CUDA_VISIBLE_DEVICES=${CUDA_DEVICES[$i]} fusion_bench method=dummy \\\n        modelpool=clip-vit-large-patch14_individual \\\n            modelpool.models.0.path=tanganke/clip-vit-large-patch14_${task} \\\n        taskpool=clip-vit-classification_TA8 \\\n            taskpool.clip_model=openai/clip-vit-large-patch14 \\\n        report_save_path=\"outputs/ViT-L-14/single-task/clip-vit-large-patch14_${task}.json\" &amp;\ndone\n</code></pre> <p>Upscale eight CLIP-ViT-B/32 models with SMILE, each CLIP-ViT-B/32 model is trained on a downstream task.</p> <pre><code>gate_k=16\nk=32\nfusion_bench \\\n    method=smile_upscaling \\\n        method.device=cuda \\\n        method.gate_k=$gate_k method.k=$k \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \\\n    report_save_path=\"outputs/ViT-B-32/eight_tasks/gate_k\\=${gate_k}_k\\=${k}.json\"\n</code></pre> <p>Hyperparameter search for SMILE upscaling. Pre-run results can be found in <code>examples/smile_upscaling/clip-vit-base-patch32.ipynb</code>.</p> <pre><code>for gate_k in 1 2 4 8 16 32 64 128 256 512 768; do\n    for k in 4 8 16 32 64 128 -1; do\n        fusion_bench \\\n            method=smile_upscaling \\\n                method.device=cuda \\\n                method.gate_k=$gate_k method.k=$k \\\n            modelpool=clip-vit-base-patch32_TA8 \\\n            taskpool=clip-vit-classification_TA8 \\\n            report_save_path=\"outputs/ViT-B-32/eight_tasks/gate_k\\=${gate_k}_k\\=${k}.json\"\n    done\ndone\n</code></pre> <p>Ablations on number of experts per token (Top-K). Pre-run results can be found in <code>examples/smile_upscaling/clip-vit-base-patch32-ablations-topk.ipynb</code>.</p> <pre><code>gate_k=16\nk=32\nfor top_k in 1 2 4\ndo\nfusion_bench \\\n    method=smile_upscaling \\\n        method.device=cuda \\\n        method.gate_k=$gate_k method.k=$k \\\n    modelpool=clip-vit-base-patch32_TA8 \\\n    taskpool=clip-vit-classification_TA8 \\\n    report_save_path=\"outputs/ViT-B-32/ablation/gate_k\\=${gate_k}_k\\=${k}.json\"\ndone\n</code></pre>"},{"location":"algorithms/smile_upscaling/#clip-vit-l14-on-eight-tasks","title":"CLIP-ViT-L/14 on eight tasks","text":"<p>hyperparameter search for SMILE upscaling. Pre-run results can be found in <code>examples/smile_upscaling/clip-vit-large-patch14.ipynb</code>.</p> <pre><code>for gate_k in 1 2 4 8 16 32 64 128; do\n    for k in 4 8 16 32 64 128 -1; do\n        fusion_bench \\\n            method=smile_upscaling \\\n                method.gate_k=$gate_k method.k=$k \\\n            modelpool=clip-vit-large-patch14_TA8 \\\n            taskpool=clip-vit-classification_TA8 \\\n                taskpool.clip_model=openai/clip-vit-large-patch14 \\\n            report_save_path=\"outputs/ViT-B-32/eight_tasks/gate_k\\=${gate_k}_k\\=${k}.json\"\n    done\ndone\n</code></pre>"},{"location":"algorithms/smile_upscaling/#flan-t5-models-on-eight-tasks-from-glue-benchmark","title":"Flan-T5 models on eight tasks from GLUE benchmark","text":"<p>Hyperparameter search for full fine-tuned and lora fine-tuned Flan-T5 models. Pre-run results can be found in <code>examples/smile_upscaling/flan-t5-base.ipynb</code> and <code>examples/smile_upscaling/flan-t5-base-lora16.ipynb</code>.</p> <pre><code># hyperparameter search for full fine-tuned flan-t5-base\nfor gate_k in 4 8 16 32; do\n    for k in 16 32 64 128; do\n        fusion_bench \\\n            method=smile_upscaling \\\n                method.device=cpu \\\n                method.gate_k=$gate_k method.k=$k \\\n            modelpool=flan-t5-base_glue \\\n            taskpool=flan-t5_glue_text_generation \\\n            report_save_path=\"outputs/flan-t5-base/glue_text_generation/gate_k\\=${gate_k}_k\\=${k}.json\"\n    done\ndone\n\n# hyperparameter search for lora fine-tuned flan-t5-base\nfor gate_k in 2 4 8; do\n    for k in 4 8 16; do\n        fusion_bench \\\n            method=smile_upscaling \\\n                method.device=cuda \\\n                method.gate_k=$gate_k method.k=$k \\\n            modelpool=flan-t5-base_glue_lora16 \\\n            taskpool=flan-t5_glue_text_generation \\\n            report_save_path=\"outputs/flan-t5-base_lora16/glue_text_generation/gate_k\\=${gate_k}_k\\=${k}.json\"\n    done\ndone\n</code></pre>"},{"location":"algorithms/smile_upscaling/#upscale-mistral-7b-models","title":"Upscale Mistral-7B models","text":"<p>Here we upscale several Mistral-7B models using SMILE. The models are trained on different tasks and are used as experts in the SMILE upscaling.</p> <p>We first provide an example of the upscaled model, where we upscale the linear layers of the original Mistral model into a SMILE linear layer.</p> <pre><code>import torch\nfrom accelerate import init_empty_weights\nfrom transformers import AutoConfig\n\nfrom fusion_bench.models.modeling_smile_mistral import (\n    SmileMistralConfig,\n    SmileMistralForCausalLM,\n)\n\n\nconfig = AutoConfig.from_pretrained(\n    \"mistralai/Mistral-7B-v0.1\"\n)\nconfig = SmileMistralConfig(\n    num_experts_per_tok=1,\n    rank_of_router=8,\n    rank_of_expert=8,\n    num_local_experts=3,\n    **config.to_dict()\n)\nwith init_empty_weights():\n    model = SmileMistralForCausalLM(config)\nmodel.to(dtype=torch.float16).to_empty(device=\"cuda\")\n</code></pre> <p>The model architecture is as follows:</p> <pre><code>SmileMistralForCausalLM(\n  (model): SmileMistralModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x SmileMistralDecoderLayer(\n        (self_attn): SmileMistralAttention(\n          (q_proj): SingularMoELinear(in_features=4096, out_features=4096, num_local_experts=3, num_experts_per_tok=1, rank_of_router=8, rank_of_expert=8)\n          (k_proj): SingularMoELinear(in_features=4096, out_features=1024, num_local_experts=3, num_experts_per_tok=1, rank_of_router=8, rank_of_expert=8)\n          (v_proj): SingularMoELinear(in_features=4096, out_features=1024, num_local_experts=3, num_experts_per_tok=1, rank_of_router=8, rank_of_expert=8)\n          (o_proj): SingularMoELinear(in_features=4096, out_features=4096, num_local_experts=3, num_experts_per_tok=1, rank_of_router=8, rank_of_expert=8)\n          (rotary_emb): MistralRotaryEmbedding()\n        )\n        (mlp): SmileMistralMLP(\n          (gate_proj): SingularMoELinear(in_features=4096, out_features=14336, num_local_experts=3, num_experts_per_tok=1, rank_of_router=8, rank_of_expert=8)\n          (up_proj): SingularMoELinear(in_features=4096, out_features=14336, num_local_experts=3, num_experts_per_tok=1, rank_of_router=8, rank_of_expert=8)\n          (down_proj): SingularMoELinear(in_features=14336, out_features=4096, num_local_experts=3, num_experts_per_tok=1, rank_of_router=8, rank_of_expert=8)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): MistralRMSNorm()\n        (post_attention_layernorm): MistralRMSNorm()\n      )\n    )\n    (norm): MistralRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)\n</code></pre> <p>Knowing the model architecture, we can upscale the Mistral-7B models using the following steps:</p> <ol> <li> <p>Prepare the following 4 configuration files in <code>configs/modelpool</code>:</p> config/modelpool/smile_mistral_exp_v1.yaml<pre><code>type: AutoModelForCausalLMPool\nmodels:\n- name: _pretrained_\n    path: mistralai/Mistral-7B-v0.1\n- name: expert_1\n    path: meta-math/MetaMath-Mistral-7B\n\ndtype: float16\n</code></pre> config/modelpool/smile_mistral_exp_v2.yaml<pre><code>type: AutoModelForCausalLMPool\nmodels:\n- name: _pretrained_\n    path: mistralai/Mistral-7B-v0.1\n- name: expert_1\n    path: cognitivecomputations/dolphin-2.1-mistral-7b\n\ndtype: float16\n</code></pre> config/modelpool/smile_mistral_exp_v3.yaml<pre><code>type: AutoModelForCausalLMPool\nmodels:\n- name: _pretrained_\n    path: mistralai/Mistral-7B-v0.1\n- name: expert_1\n    path: uukuguy/speechless-code-mistral-7b-v1.0\n\ndtype: float16\n</code></pre> config/modelpool/smile_mistral_exp_v4.yaml<pre><code>type: AutoModelForCausalLMPool\nmodels:\n- name: _pretrained_\n    path: mistralai/Mistral-7B-v0.1\n- name: expert_1\n    path: meta-math/MetaMath-Mistral-7B\n- name: expert_2\n    path: cognitivecomputations/dolphin-2.1-mistral-7b\n- name: expert_3\n    path: uukuguy/speechless-code-mistral-7b-v1.0\n\ndtype: float16\n</code></pre> </li> <li> <p>Upscale Mistral-7B models. The upscaled models are saved in <code>outputs/mistral/gate_k-${gate_k}_k-${k}/version_${version}</code>.</p> <pre><code>function model_fusion() {\n    output_dir=outputs/mistral/gate_k-${gate_k}_k-${k}/version_${version}\n    fusion_bench \\\n        method=smile_mistral_upscaling \\\n            method.rank_of_router=$gate_k method.rank_of_expert=$k \\\n            method.model_path=${output_dir} \\\n        modelpool=smile_mistral_exp_v${version} \\\n            modelpool.dtype=float32 \\\n        taskpool=dummy \\\n        report_save_path=\"${output_dir}/model_info.json\"\n}\n\ngate_k=8\nfor k in 8 16 32 64 128 256 384 512; do\n    for version in 1 2 3 4; do\n        model_fusion\n    done\ndone\n</code></pre> </li> <li> <p>Use lm-evaluation-harness to evaluate the models. We use the default configurations for each task.</p> <pre><code># For some GPUs, the following environment variables need to be set\n# export NCCL_P2P_DISABLE=\"1\"\n# export NCCL_IB_DISABLE=\"1\"\n\nfunction model_eval() {\n    output_dir=outputs/mistral/gate_k-${gate_k}_k-${k}/version_${version}\n\n    # Check if ${output_dir}/${task}.json exists as a directory and return if it does\n    if [ -d \"${output_dir}/${task}.json\" ]; then\n        echo \"Directory ${output_dir}/${task}.json already exists. Skipping evaluation.\"\n        return\n    fi\n\n    lm_eval --model hf \\\n        --model_args pretrained=${output_dir},dtype=\"float16\",parallelize=True \\\n        --tasks ${task} \\\n        --output_path ${output_dir}/${task}.json \\\n        --batch_size 6\n}\n</code></pre> <p>The above function can be used to evaluate the models on specified task. Pre-run results can be found in <code>examples/smile_upscaling/mistral_gsm8k.ipynb</code>.</p> <pre><code># Evaluate all the models on GSM8K task\ngate_k=8\ntask=gsm8k\nfor k in 8 16 32 64 128 256 384 512; do\n    for version in 1 2 3 4; do\n        model_eval\n    done\ndone\n\n# Evaluate all M0;123 models on truthfulqa gsm8k arc_challenge mmlu\nk=8\nversion=4\nfor task in truthfulqa gsm8k arc_challenge mmlu; do\n    model_eval\ndone\n</code></pre> <p>The reported metrics are:</p> <ul> <li>mmlu (general): acc</li> <li>truthfulqa (truthful): mc2</li> <li>gsm8k (math): flexible exact match</li> <li>arc_challenge (reasoning): acc_norm</li> </ul> </li> </ol>"},{"location":"algorithms/smile_upscaling/#scope","title":"Scope","text":""},{"location":"algorithms/smile_upscaling/#projection-merge-experiments","title":"Projection Merge Experiments","text":"<p>Pre-run results can be found in <code>examples/smile_upscaling/clip-vit-base-patch32_single-task_projection-merging.ipynb</code>.</p> <pre><code># project into different subspaces\nfor task in sun397 stanford-cars resisc45 eurosat svhn gtsrb mnist dtd\ndo\n    # Space I\n    CUDA_VISIBLE_DEVICES=0 fusion_bench \\\n        method=singular_projection_merging \\\n            method.device=cuda method.rank=low method.k=-1 method.full_matrices=false \\\n        modelpool=clip-vit-base-patch32_single_finetuned \\\n            modelpool.models.1.name=${task} \\\n            modelpool.models.1.path=tanganke/clip-vit-base-patch32_${task} \\\n        taskpool=clip-vit-classification_TA8 \\\n        report_save_path=\"outputs/ViT-B-32/single-task/projection_merging_zone1_${task}.json\" &amp;\n\n    # Space II\n    CUDA_VISIBLE_DEVICES=1 fusion_bench \\\n        method=singular_projection_merging \\\n            method.device=cuda method.rank=high method.k=-1 method.full_matrices=false \\\n        modelpool=clip-vit-base-patch32_single_finetuned \\\n            modelpool.models.1.name=${task} \\\n            modelpool.models.1.path=tanganke/clip-vit-base-patch32_${task} \\\n        taskpool=clip-vit-classification_TA8 \\\n        report_save_path=\"outputs/ViT-B-32/single-task/projection_merging_zone2_${task}.json\" &amp;\n\n    # Space III\n    CUDA_VISIBLE_DEVICES=2 fusion_bench \\\n        method=singular_projection_merging \\\n            method.device=cuda method.rank=high method.k=-1 method.full_matrices=true \\\n        modelpool=clip-vit-base-patch32_single_finetuned \\\n            modelpool.models.1.name=${task} \\\n            modelpool.models.1.path=tanganke/clip-vit-base-patch32_${task} \\\n        taskpool=clip-vit-classification_TA8 \\\n        report_save_path=\"outputs/ViT-B-32/single-task/projection_merging_zone23_${task}.json\" &amp;\n    wait\ndone\n</code></pre>"},{"location":"algorithms/smile_upscaling/#references","title":"References","text":""},{"location":"algorithms/smile_upscaling/#algorithms","title":"Algorithms","text":"<ol> <li> <p>A. Tang et. al. SMILE: Zero-Shot Sparse Mixture of Low-Rank Experts Construction From Pre-Trained Foundation Models. Aug, 2024. https://arxiv.org/abs/2408.10174 \u21a9</p> </li> <li> <p>Yadav, Prateek, et al. \"A Survey on Model MoErging: Recycling and Routing Among Specialized Experts for Collaborative Learning.\" arXiv preprint arXiv:2408.07057 (2024).\u00a0\u21a9</p> </li> </ol>"},{"location":"algorithms/smile_upscaling/#fusion_bench.method.SmileUpscalingAlgorithm","title":"<code>SmileUpscalingAlgorithm</code>","text":"<p>               Bases: <code>SimpleProfilerMixin</code>, <code>BaseAlgorithm</code></p> Source code in <code>fusion_bench/method/smile_upscaling/smile_upscaling.py</code> <pre><code>class SmileUpscalingAlgorithm(\n    SimpleProfilerMixin,\n    BaseAlgorithm,\n):\n    _linear_layer_cls = (nn.Linear,)\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"device\": \"device\",\n        \"upscaling_accelerator\": \"upscaling_accelerator\",\n        \"full_matrices\": \"full_matrices\",\n        \"gate_k\": \"gate_k\",\n        \"k\": \"k\",\n        \"top_k\": \"top_k\",\n        \"routing_use_diff\": \"routing_use_diff\",\n        \"average_experts\": \"average_experts\",\n        \"model_path\": \"model_path\",\n    }\n\n    def __init__(\n        self,\n        *,\n        device: str = \"cuda\",\n        upscaling_accelerator: str = None,\n        full_matrices: bool = True,\n        gate_k: int = 256,\n        k: int = 256,\n        top_k: int = 1,\n        routing_use_diff: bool = True,\n        average_experts: bool = False,\n        model_path: str = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the SmileUpscalingAlgorithm.\n\n        Args:\n            device (str): The device to perform the computation on.\n            upscaling_accelerator (str): The device to perform the SVD computation on.\n            full_matrices (bool): Whether to compute the full-sized U and V matrices.\n            gate_k (int): The number of singular values to keep for the gate.\n            k (int): The number of singular values to keep for the experts.\n            top_k (int): The number of top experts to select.\n            routing_use_diff (bool): Whether to use weight differences for routing.\n            average_experts (bool): Whether to average the experts.\n            model_path (str): The path to save/load the model.\n            **kwargs: Additional arguments.\n        \"\"\"\n        super().__init__()\n        self.device = device\n        self.upscaling_accelerator = upscaling_accelerator\n        self.full_matrices = full_matrices\n        self.gate_k = gate_k\n        self.k = k\n        self.top_k = top_k\n        self.routing_use_diff = routing_use_diff\n        self.average_experts = average_experts\n        self.model_path = model_path\n        for key, value in kwargs.items():\n            log.warning(f\"Unrecognized argument: {key}\")\n            setattr(self, key, value)\n\n        # print `self.config` as yaml\n        print(f\"=== Config for `{type(self).__name__}` ===\")\n        print(OmegaConf.to_yaml(self.config))\n        print(f\"=== Config for `{type(self).__name__}` ===\")\n\n    @torch.no_grad()\n    def run(self, modelpool: BaseModelPool):\n        \"\"\"\n        Executes the upscaling process.\n\n        Args:\n            modelpool (ModelPool): The pool of models to be used for upscaling.\n\n        Returns:\n            nn.Module: The upscaled model.\n        \"\"\"\n        if not isinstance(modelpool, BaseModelPool):\n            modelpool = BaseModelPool(modelpool)\n\n        if self.config.model_path is not None and os.path.exists(\n            self.config.model_path\n        ):\n            log.info(f\"Loading model from {self.config.model_path}\")\n            model = torch.load(self.config.model_path)\n            print_parameters(model)\n            return model\n\n        with self.profile(\"load pretrained model\"):\n            pretrained_model = modelpool.load_model(\"_pretrained_\")\n        with self.profile(\"load fine-tuned model\"):\n            finetuned_models = [\n                m for m in tqdm(modelpool.models(), total=len(modelpool.model_names))\n            ]\n\n        if self.config.device == \"cuda\" and torch.cuda.is_available():\n            pretrained_model = pretrained_model.cuda()\n            finetuned_models = [m.cuda() for m in finetuned_models]\n\n        with self.profile(\"merge model\"):\n            model = self.merge(pretrained_model, finetuned_models)\n\n        self.print_profile_summary()\n        if self.config.model_path is not None:\n            os.makedirs(os.path.dirname(self.config.model_path), exist_ok=True)\n            log.info(f\"Saving model to {self.config.model_path}\")\n            torch.save(model, self.config.model_path)\n        print_parameters(model)\n        return model\n\n    def merge(\n        self,\n        pretrained_model: nn.Module,\n        finetuned_models: List[nn.Module],\n        in_place: bool = True,\n    ):\n        \"\"\"\n        Merges the pretrained model with the fine-tuned models to create an upscaled model.\n\n        Args:\n            pretrained_model (nn.Module): The pretrained model.\n            finetuned_models (List[nn.Module]): A list of fine-tuned models.\n            in_place (bool): If True, modifies the pretrained model in place. Otherwise, creates a copy.\n\n        Returns:\n            nn.Module: The merged model.\n        \"\"\"\n        if in_place:\n            model = pretrained_model\n        else:\n            model = deepcopy(pretrained_model)\n\n        self._upscale_submodules(model, finetuned_models)\n        return model\n\n    def _upscale_linear_layer(\n        self,\n        pretrained_model,\n        finetuned_models,\n        name: str,\n    ):\n        \"\"\"\n        Upscale a linear layer by merging it with the corresponding layers from the fine-tuned models.\n\n        Args:\n            pretrained_model (nn.Module): The pretrained model.\n            finetuned_models (List[nn.Module]): A list of fine-tuned models.\n            name (str): The name of the linear layer to upscale.\n        \"\"\"\n        config = self.config\n\n        name_list = name.split(\".\")\n        module = get_attr(pretrained_model, name_list)\n        experts = [get_attr(m, name_list) for m in finetuned_models]\n        try:\n            moe_linear = SmileMoELinear(\n                module,\n                experts,\n                gate_k=config.gate_k,\n                k=config.k,\n                top_k=config.top_k,\n                routing_use_diff=self.routing_use_diff,\n                full_matrices=self.full_matrices,\n                upscaling_accelerator=self.upscaling_accelerator,\n            )\n        except ExpertNotTrainedError:\n            print(f\"skip {name} because the experts are not trained.\")\n            return\n        set_attr(pretrained_model, name_list, moe_linear)\n        # remove the original module from fine-tuned models to save memory\n        for m in finetuned_models:\n            set_attr(m, name_list, None)\n\n    def _average_experts(self, pretarined_model, finetuned_models, name: str):\n        \"\"\"\n        Average the experts for a given layer.\n\n        Args:\n            pretarined_model (nn.Module): The pretrained model.\n            finetuned_models (List[nn.Module]): A list of fine-tuned models.\n            name (str): The name of the layer to average.\n        \"\"\"\n        name_list = name.split(\".\")\n        experts = [get_attr(m, name_list) for m in finetuned_models]\n        averaged_module = simple_average(experts)\n        set_attr(pretarined_model, name_list, averaged_module)\n\n    def _upscale_submodules(\n        self,\n        pretrained_model: nn.Module,\n        finetuned_models: List[nn.Module],\n        tqdm_desc: str = \"Upscaling Linear Modules\",\n    ):\n        \"\"\"\n        Upscales the submodules of the pretrained model by merging them with the corresponding submodules from the fine-tuned models.\n\n        Args:\n            pretrained_model (nn.Module): The pretrained model.\n            finetuned_models (List[nn.Module]): A list of fine-tuned models.\n            tqdm_desc (str): Description for the tqdm progress bar.\n        \"\"\"\n        config = self.config\n        for name, module in tqdm(\n            tuple(pretrained_model.named_modules()),\n            tqdm_desc,\n            leave=False,\n            dynamic_ncols=True,\n        ):\n            if isinstance(module, self._linear_layer_cls):\n                self._upscale_linear_layer(\n                    pretrained_model=pretrained_model,\n                    finetuned_models=finetuned_models,\n                    name=name,\n                )\n            elif config.average_experts and len(tuple(module.named_modules())) == 1:\n                # if the module is a leaf module, we perform a parameter average\n                self._average_experts(pretrained_model, finetuned_models, name)\n</code></pre>"},{"location":"algorithms/smile_upscaling/#fusion_bench.method.SmileUpscalingAlgorithm.__init__","title":"<code>__init__(*, device='cuda', upscaling_accelerator=None, full_matrices=True, gate_k=256, k=256, top_k=1, routing_use_diff=True, average_experts=False, model_path=None, **kwargs)</code>","text":"<p>Initialize the SmileUpscalingAlgorithm.</p> <p>Parameters:</p> Source code in <code>fusion_bench/method/smile_upscaling/smile_upscaling.py</code> <pre><code>def __init__(\n    self,\n    *,\n    device: str = \"cuda\",\n    upscaling_accelerator: str = None,\n    full_matrices: bool = True,\n    gate_k: int = 256,\n    k: int = 256,\n    top_k: int = 1,\n    routing_use_diff: bool = True,\n    average_experts: bool = False,\n    model_path: str = None,\n    **kwargs,\n):\n    \"\"\"\n    Initialize the SmileUpscalingAlgorithm.\n\n    Args:\n        device (str): The device to perform the computation on.\n        upscaling_accelerator (str): The device to perform the SVD computation on.\n        full_matrices (bool): Whether to compute the full-sized U and V matrices.\n        gate_k (int): The number of singular values to keep for the gate.\n        k (int): The number of singular values to keep for the experts.\n        top_k (int): The number of top experts to select.\n        routing_use_diff (bool): Whether to use weight differences for routing.\n        average_experts (bool): Whether to average the experts.\n        model_path (str): The path to save/load the model.\n        **kwargs: Additional arguments.\n    \"\"\"\n    super().__init__()\n    self.device = device\n    self.upscaling_accelerator = upscaling_accelerator\n    self.full_matrices = full_matrices\n    self.gate_k = gate_k\n    self.k = k\n    self.top_k = top_k\n    self.routing_use_diff = routing_use_diff\n    self.average_experts = average_experts\n    self.model_path = model_path\n    for key, value in kwargs.items():\n        log.warning(f\"Unrecognized argument: {key}\")\n        setattr(self, key, value)\n\n    # print `self.config` as yaml\n    print(f\"=== Config for `{type(self).__name__}` ===\")\n    print(OmegaConf.to_yaml(self.config))\n    print(f\"=== Config for `{type(self).__name__}` ===\")\n</code></pre>"},{"location":"algorithms/smile_upscaling/#fusion_bench.method.SmileUpscalingAlgorithm.__init__(device)","title":"<code>device</code>","text":"(<code>str</code>, default:                   <code>'cuda'</code> )           \u2013            <p>The device to perform the computation on.</p>"},{"location":"algorithms/smile_upscaling/#fusion_bench.method.SmileUpscalingAlgorithm.__init__(upscaling_accelerator)","title":"<code>upscaling_accelerator</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The device to perform the SVD computation on.</p>"},{"location":"algorithms/smile_upscaling/#fusion_bench.method.SmileUpscalingAlgorithm.__init__(full_matrices)","title":"<code>full_matrices</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to compute the full-sized U and V matrices.</p>"},{"location":"algorithms/smile_upscaling/#fusion_bench.method.SmileUpscalingAlgorithm.__init__(gate_k)","title":"<code>gate_k</code>","text":"(<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The number of singular values to keep for the gate.</p>"},{"location":"algorithms/smile_upscaling/#fusion_bench.method.SmileUpscalingAlgorithm.__init__(k)","title":"<code>k</code>","text":"(<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The number of singular values to keep for the experts.</p>"},{"location":"algorithms/smile_upscaling/#fusion_bench.method.SmileUpscalingAlgorithm.__init__(top_k)","title":"<code>top_k</code>","text":"(<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of top experts to select.</p>"},{"location":"algorithms/smile_upscaling/#fusion_bench.method.SmileUpscalingAlgorithm.__init__(routing_use_diff)","title":"<code>routing_use_diff</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use weight differences for routing.</p>"},{"location":"algorithms/smile_upscaling/#fusion_bench.method.SmileUpscalingAlgorithm.__init__(average_experts)","title":"<code>average_experts</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to average the experts.</p>"},{"location":"algorithms/smile_upscaling/#fusion_bench.method.SmileUpscalingAlgorithm.__init__(model_path)","title":"<code>model_path</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The path to save/load the model.</p>"},{"location":"algorithms/smile_upscaling/#fusion_bench.method.SmileUpscalingAlgorithm.__init__(**kwargs)","title":"<code>**kwargs</code>","text":"\u2013            <p>Additional arguments.</p>"},{"location":"algorithms/smile_upscaling/#fusion_bench.method.SmileUpscalingAlgorithm.merge","title":"<code>merge(pretrained_model, finetuned_models, in_place=True)</code>","text":"<p>Merges the pretrained model with the fine-tuned models to create an upscaled model.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li>           \u2013            <p>nn.Module: The merged model.</p> </li> </ul> Source code in <code>fusion_bench/method/smile_upscaling/smile_upscaling.py</code> <pre><code>def merge(\n    self,\n    pretrained_model: nn.Module,\n    finetuned_models: List[nn.Module],\n    in_place: bool = True,\n):\n    \"\"\"\n    Merges the pretrained model with the fine-tuned models to create an upscaled model.\n\n    Args:\n        pretrained_model (nn.Module): The pretrained model.\n        finetuned_models (List[nn.Module]): A list of fine-tuned models.\n        in_place (bool): If True, modifies the pretrained model in place. Otherwise, creates a copy.\n\n    Returns:\n        nn.Module: The merged model.\n    \"\"\"\n    if in_place:\n        model = pretrained_model\n    else:\n        model = deepcopy(pretrained_model)\n\n    self._upscale_submodules(model, finetuned_models)\n    return model\n</code></pre>"},{"location":"algorithms/smile_upscaling/#fusion_bench.method.SmileUpscalingAlgorithm.merge(pretrained_model)","title":"<code>pretrained_model</code>","text":"(<code>Module</code>)           \u2013            <p>The pretrained model.</p>"},{"location":"algorithms/smile_upscaling/#fusion_bench.method.SmileUpscalingAlgorithm.merge(finetuned_models)","title":"<code>finetuned_models</code>","text":"(<code>List[Module]</code>)           \u2013            <p>A list of fine-tuned models.</p>"},{"location":"algorithms/smile_upscaling/#fusion_bench.method.SmileUpscalingAlgorithm.merge(in_place)","title":"<code>in_place</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, modifies the pretrained model in place. Otherwise, creates a copy.</p>"},{"location":"algorithms/smile_upscaling/#fusion_bench.method.SmileUpscalingAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Executes the upscaling process.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li>           \u2013            <p>nn.Module: The upscaled model.</p> </li> </ul> Source code in <code>fusion_bench/method/smile_upscaling/smile_upscaling.py</code> <pre><code>@torch.no_grad()\ndef run(self, modelpool: BaseModelPool):\n    \"\"\"\n    Executes the upscaling process.\n\n    Args:\n        modelpool (ModelPool): The pool of models to be used for upscaling.\n\n    Returns:\n        nn.Module: The upscaled model.\n    \"\"\"\n    if not isinstance(modelpool, BaseModelPool):\n        modelpool = BaseModelPool(modelpool)\n\n    if self.config.model_path is not None and os.path.exists(\n        self.config.model_path\n    ):\n        log.info(f\"Loading model from {self.config.model_path}\")\n        model = torch.load(self.config.model_path)\n        print_parameters(model)\n        return model\n\n    with self.profile(\"load pretrained model\"):\n        pretrained_model = modelpool.load_model(\"_pretrained_\")\n    with self.profile(\"load fine-tuned model\"):\n        finetuned_models = [\n            m for m in tqdm(modelpool.models(), total=len(modelpool.model_names))\n        ]\n\n    if self.config.device == \"cuda\" and torch.cuda.is_available():\n        pretrained_model = pretrained_model.cuda()\n        finetuned_models = [m.cuda() for m in finetuned_models]\n\n    with self.profile(\"merge model\"):\n        model = self.merge(pretrained_model, finetuned_models)\n\n    self.print_profile_summary()\n    if self.config.model_path is not None:\n        os.makedirs(os.path.dirname(self.config.model_path), exist_ok=True)\n        log.info(f\"Saving model to {self.config.model_path}\")\n        torch.save(model, self.config.model_path)\n    print_parameters(model)\n    return model\n</code></pre>"},{"location":"algorithms/smile_upscaling/#fusion_bench.method.SmileUpscalingAlgorithm.run(modelpool)","title":"<code>modelpool</code>","text":"(<code>ModelPool</code>)           \u2013            <p>The pool of models to be used for upscaling.</p>"},{"location":"algorithms/task_arithmetic/","title":"Task Arithmetic","text":"<p>In the rapidly advancing field of machine learning, multi-task learning has emerged as a powerful paradigm, allowing models to leverage information from multiple tasks to improve performance and generalization. One intriguing method in this domain is Task Arithmetic, which involves the combination of task-specific vectors derived from model parameters. </p> Task Arithmetic. This figure is credited to <sup>2</sup> <p>Task Vector. A task vector is used to encapsulate the adjustments needed by a model to specialize in a specific task.  It is derived from the differences between a pre-trained model's parameters and those fine-tuned for a particular task.  Formally, if \\(\\theta_i\\) represents the model parameters fine-tuned for the i-th task and \\(\\theta_0\\) denotes the parameters of the pre-trained model, the task vector for the i-th task is defined as:</p> \\[\\tau_i = \\theta_i - \\theta_0\\] <p>This representation is crucial for methods like Task Arithmetic, where multiple task vectors are aggregated and scaled to form a comprehensive multi-task model.</p> <p>Task Arithmetic<sup>1</sup> begins by computing a task vector \\(\\tau_i\\) for each individual task, using the set of model parameters \\(\\theta_0 \\cup \\{\\theta_i\\}_i\\) where \\(\\theta_0\\) is the pre-trained model and \\(\\theta_i\\) are the fine-tuned parameters for i-th task. These task vectors are then aggregated to form a multi-task vector. Subsequently, the multi-task vector is combined with the pre-trained model parameters to obtain the final multi-task model. This process involves scaling the combined vector element-wise by a scaling coefficient (denoted as \\(\\lambda\\)), before adding it to the initial pre-trained model parameters.  The resulting formulation for obtaining a multi-task model is expressed as </p> \\[ \\theta = \\theta_0 + \\lambda \\sum_{i} \\tau_i. \\] <p>The choice of the scaling coefficient \\(\\lambda\\) plays a crucial role in the final model performance. Typically, \\(\\lambda\\) is chosen based on validation set performance. </p>"},{"location":"algorithms/task_arithmetic/#examples","title":"Examples","text":"<p>To use the Task Arithmetic algorithm, you can use the <code>TaskArithmeticAlgorithm</code> class from the <code>fusion_bench.method</code> module.</p> <pre><code>from fusion_bench.method.task_arithmetic import TaskArithmeticAlgorithm\nfrom omegaconf import DictConfig\n\n# Instantiate the TaskArithmeticAlgorithm\nmethod_config = {'name': 'task_arithmetic', 'scaling_factor': 0.5}\nalgorithm = TaskArithmeticAlgorithm(DictConfig(method_config))\n\n# Assume we have a dict of PyTorch models (nn.Module instances) that we want to merge.\n# The models should all have the same architecture.\n# the dict must contain the pre-trained model with the key '_pretrained_', and arbitrary number of fine-tuned models.\nmodels = {'_pretrained_': nn.Linear(10,10), 'model_1': nn.Linear(10,10), 'model_2': nn.Linear(10,10)}\n\n# Run the algorithm on the models.\n# This will return a new model that is the result of task arithmetic on the input models.\nmerged_model = algorithm.run(models)\n</code></pre>"},{"location":"algorithms/task_arithmetic/#code-integration","title":"Code Integration","text":"<p>Configuration template for the Task Arithmetic algorithm:</p> config/method/task_arithmetic.yaml<pre><code>name: task_arithmetic\nscaling_factor: 0.5 # Scaling factor for task vectors\n</code></pre> <p>Use the following command to run the Task Arithmetic algorithm:</p> <pre><code>fusion_bench method=task_arithmetic ...\n</code></pre> <p>For example, to run the Task Arithmetic algorithm on two models with scaling factor 0.5:</p> <pre><code>fusion_bench method=task_arithmetic \\\n    method.scaling_factor=0.5 \\\n  modelpool=clip-vit-base-patch32_svhn_and_mnist \\\n  taskpool=clip-vit-base-patch32_svhn_and_mnist\n</code></pre> <p>where the configuration for the model pool is:</p> config/modelpool/clip-vit-base-patch32_svhn_and_mnist.yaml<pre><code>type: huggingface_clip_vision\n# the modelpool must contain the pre-trained model with the name '_pretrained_', \n# and arbitrary number of fine-tuned models.\nmodels:\n  - name: _pretrained_\n    path: openai/clip-vit-base-patch32\n  - name: svhn\n    path: tanganke/clip-vit-base-patch32_svhn\n  - name: mnist\n    path: tanganke/clip-vit-base-patch32_mnist\n</code></pre> <p>and the configuration for the task pool:</p> config/taskpool/clip-vit-base-patch32_svhn_and_mnist.yaml<pre><code>type: clip_vit_classification\n\ndataset_type: huggingface_image_classification\ntasks:\n  - name: svhn\n    dataset:\n      type: instantiate\n      name: svhn\n      object: \n        _target_: datasets.load_dataset\n        _args_:\n          - svhn\n          - cropped_digits\n        split: test\n  - name: mnist\n    dataset:\n      name: mnist\n      split: test\n\n...\n</code></pre>"},{"location":"algorithms/task_arithmetic/#references","title":"References","text":"<ol> <li> <p>(ICLR 2023) Editing Models with Task Arithmetic. http://arxiv.org/abs/2212.04089\u00a0\u21a9</p> </li> <li> <p>(ICLR 2024) AdaMerging: Adaptive Model Merging for Multi-Task Learning. http://arxiv.org/abs/2310.02575\u00a0\u21a9</p> </li> <li> <p>(NIPS 2023 Oral) Guillermo Ortiz-Jimenez, Alessandro Favero, and Pascal Frossard, \u201cTask Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models,\u201d doi: 10.48550/arXiv.2305.12827.\u00a0\u21a9</p> </li> </ol>"},{"location":"algorithms/task_arithmetic/#fusion_bench.method.task_arithmetic.TaskArithmeticAlgorithm","title":"<code>TaskArithmeticAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>SimpleProfilerMixin</code></p> <p>Task Arithmetic Algorithm for model fusion.</p> <p>This class implements the Task Arithmetic method for fusing models. It inherits from BaseModelFusionAlgorithm and SimpleProfilerMixin to provide the necessary functionality for model fusion and profiling.</p> <p>Attributes:</p> <ul> <li> <code>scaling_factor</code>               (<code>int</code>)           \u2013            <p>The factor by which the task vectors will be scaled before merging.</p> </li> </ul> Source code in <code>fusion_bench/method/task_arithmetic/task_arithmetic.py</code> <pre><code>class TaskArithmeticAlgorithm(\n    BaseAlgorithm,\n    SimpleProfilerMixin,\n):\n    \"\"\"\n    Task Arithmetic Algorithm for model fusion.\n\n    This class implements the Task Arithmetic method for fusing models. It inherits from\n    BaseModelFusionAlgorithm and SimpleProfilerMixin to provide the necessary functionality\n    for model fusion and profiling.\n\n    Attributes:\n        scaling_factor (int): The factor by which the task vectors will be scaled before merging.\n    \"\"\"\n\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"scaling_factor\": \"scaling_factor\"\n    }\n\n    def __init__(self, scaling_factor: int):\n        \"\"\"\n        Initializes the TaskArithmeticAlgorithm with the given scaling factor.\n\n        Args:\n            scaling_factor (int): The factor by which the task vectors will be scaled before merging.\n        \"\"\"\n        self.scaling_factor = scaling_factor\n        super().__init__()\n\n    @torch.no_grad()\n    def run(self, modelpool: Union[BaseModelPool, Dict[str, nn.Module]]):\n        \"\"\"\n        Runs the Task Arithmetic Algorithm to fuse models in the given model pool.\n\n        Args:\n            modelpool (Union[BaseModelPool, Dict[str, nn.Module]]): The pool of models to fuse.\n\n        Returns:\n            nn.Module: The pre-trained model with the merged task vectors.\n        \"\"\"\n        if not isinstance(modelpool, BaseModelPool):\n            modelpool = BaseModelPool(modelpool)\n\n        log.info(\"Fusing models using task arithmetic.\")\n        task_vector = None\n        with self.profile(\"load model\"):\n            pretrained_model = modelpool.load_model(\"_pretrained_\")\n\n        # Calculate the total task vector\n        for model_name in modelpool.model_names:\n            with self.profile(\"load model\"):\n                model = modelpool.load_model(model_name)\n            with self.profile(\"merge weights\"):\n                if task_vector is None:\n                    task_vector = state_dict_sub(\n                        model.state_dict(keep_vars=True),\n                        pretrained_model.state_dict(keep_vars=True),\n                    )\n                else:\n                    task_vector = state_dict_add(\n                        task_vector,\n                        state_dict_sub(\n                            model.state_dict(keep_vars=True),\n                            pretrained_model.state_dict(keep_vars=True),\n                        ),\n                    )\n        with self.profile(\"merge weights\"):\n            # scale the task vector\n            task_vector = state_dict_mul(task_vector, self.config.scaling_factor)\n            # add the task vector to the pretrained model\n            state_dict = state_dict_add(\n                pretrained_model.state_dict(keep_vars=True), task_vector\n            )\n\n        self.print_profile_summary()\n        pretrained_model.load_state_dict(state_dict)\n        return pretrained_model\n</code></pre>"},{"location":"algorithms/task_arithmetic/#fusion_bench.method.task_arithmetic.TaskArithmeticAlgorithm._config_mapping","title":"<code>_config_mapping = BaseAlgorithm._config_mapping | {'scaling_factor': 'scaling_factor'}</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"algorithms/task_arithmetic/#fusion_bench.method.task_arithmetic.TaskArithmeticAlgorithm.scaling_factor","title":"<code>scaling_factor = scaling_factor</code>  <code>instance-attribute</code>","text":""},{"location":"algorithms/task_arithmetic/#fusion_bench.method.task_arithmetic.TaskArithmeticAlgorithm.__init__","title":"<code>__init__(scaling_factor)</code>","text":"<p>Initializes the TaskArithmeticAlgorithm with the given scaling factor.</p> <p>Parameters:</p> Source code in <code>fusion_bench/method/task_arithmetic/task_arithmetic.py</code> <pre><code>def __init__(self, scaling_factor: int):\n    \"\"\"\n    Initializes the TaskArithmeticAlgorithm with the given scaling factor.\n\n    Args:\n        scaling_factor (int): The factor by which the task vectors will be scaled before merging.\n    \"\"\"\n    self.scaling_factor = scaling_factor\n    super().__init__()\n</code></pre>"},{"location":"algorithms/task_arithmetic/#fusion_bench.method.task_arithmetic.TaskArithmeticAlgorithm.__init__(scaling_factor)","title":"<code>scaling_factor</code>","text":"(<code>int</code>)           \u2013            <p>The factor by which the task vectors will be scaled before merging.</p>"},{"location":"algorithms/task_arithmetic/#fusion_bench.method.task_arithmetic.TaskArithmeticAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Runs the Task Arithmetic Algorithm to fuse models in the given model pool.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li>           \u2013            <p>nn.Module: The pre-trained model with the merged task vectors.</p> </li> </ul> Source code in <code>fusion_bench/method/task_arithmetic/task_arithmetic.py</code> <pre><code>@torch.no_grad()\ndef run(self, modelpool: Union[BaseModelPool, Dict[str, nn.Module]]):\n    \"\"\"\n    Runs the Task Arithmetic Algorithm to fuse models in the given model pool.\n\n    Args:\n        modelpool (Union[BaseModelPool, Dict[str, nn.Module]]): The pool of models to fuse.\n\n    Returns:\n        nn.Module: The pre-trained model with the merged task vectors.\n    \"\"\"\n    if not isinstance(modelpool, BaseModelPool):\n        modelpool = BaseModelPool(modelpool)\n\n    log.info(\"Fusing models using task arithmetic.\")\n    task_vector = None\n    with self.profile(\"load model\"):\n        pretrained_model = modelpool.load_model(\"_pretrained_\")\n\n    # Calculate the total task vector\n    for model_name in modelpool.model_names:\n        with self.profile(\"load model\"):\n            model = modelpool.load_model(model_name)\n        with self.profile(\"merge weights\"):\n            if task_vector is None:\n                task_vector = state_dict_sub(\n                    model.state_dict(keep_vars=True),\n                    pretrained_model.state_dict(keep_vars=True),\n                )\n            else:\n                task_vector = state_dict_add(\n                    task_vector,\n                    state_dict_sub(\n                        model.state_dict(keep_vars=True),\n                        pretrained_model.state_dict(keep_vars=True),\n                    ),\n                )\n    with self.profile(\"merge weights\"):\n        # scale the task vector\n        task_vector = state_dict_mul(task_vector, self.config.scaling_factor)\n        # add the task vector to the pretrained model\n        state_dict = state_dict_add(\n            pretrained_model.state_dict(keep_vars=True), task_vector\n        )\n\n    self.print_profile_summary()\n    pretrained_model.load_state_dict(state_dict)\n    return pretrained_model\n</code></pre>"},{"location":"algorithms/task_arithmetic/#fusion_bench.method.task_arithmetic.TaskArithmeticAlgorithm.run(modelpool)","title":"<code>modelpool</code>","text":"(<code>Union[BaseModelPool, Dict[str, Module]]</code>)           \u2013            <p>The pool of models to fuse.</p>"},{"location":"algorithms/ties_merging/","title":"Ties Merging","text":"Ties-Merging. Credit to <sup>1</sup> <p>Ties-Merging<sup>1</sup> represents a novel and structured approach to consolidating multiple task-specific models into a single, efficient multi-task model. This method employs a sequence of deliberate steps to systematically merge task vectors, ensuring that the final model effectively integrates the strengths of each individual task-specific model and resolves potential conflicts between them.</p> <p>The Ties-Merging algorithm operates through three primary steps:</p> <ol> <li>Trim: This initial step involves refining the task-specific models by trimming unnecessary parameters, focusing the model on essential elements for each task.</li> <li>Elect Sign of Parameters: In this step, the algorithm selects the appropriate signs for the parameters, ensuring that the integrated model parameters are optimally oriented for multi-task learning.</li> <li>Disjoint Merge: Finally, the method performs a disjoint merge to combine the task-specific parameters into a single cohesive task vector, denoted as \\(\\tau\\).</li> </ol> <p>Given the final merged task vector \\(\\tau\\), the ultimate model is determined similarly to the method used in task arithmetic. The formulation is expressed as:</p> \\[ \\theta = \\theta_0 + \\lambda \\tau \\] <p>where \\(\\lambda\\) is a hyperparameter chosen based on the validation set to ensure the best-performing model.</p> <p>By following these structured steps, Ties-Merging effectively integrates multiple task-specific models into a unified multi-task model, balancing the contributions of each task to enhance overall performance. The process ensures that the final model retains the benefits of the pre-trained model while optimally incorporating the diverse knowledge contained within the individual task-specific models.</p>"},{"location":"algorithms/ties_merging/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"Task Arithmetic and Ties-Merging. Here we illustrate the average performance of models merged using Task Arithmetic and Ties-Merging methods, with varying scaling coefficients.  The subfigures represent different models: CLIP-ViT-B/32, CLIP-ViT-L/14, Flan-T5-base (LoRA fine-tuned), and Flan-T5-large (LoRA fine-tuned).  <p>In the above figure, we show the average performance of Task Arithmetic and Ties-Merging merged models as the scaling coefficient varies. Subfigure (a), (b), (c), and (d) show the results of CLIP-ViT-B/32, CLIP-ViT-L/14, Flan-T5-base (LoRA fine-tuned), and Flan-T5-large (LoRA fine-tuned), respectively. It is evident that the merged multi-task model hits a peak in average performance across various tasks when the scaling coefficient is set around 0.3. This value was empirically selected as the scaling coefficient in our experiments. As we increase the scaling coefficient beyond this point, the average performance of the model begins to decline, eventually even falling below the level of the pre-trained model\u2019s original performance. This suggests that too high of a scaling coefficient can have a negative impact on the knowledge that the pre-trained model initially possessed, emphasizing the importance of calibrating the scaling coefficient parameter \\(\\lambda\\) to avoid diminishing the model\u2019s existing strengths.</p>"},{"location":"algorithms/ties_merging/#code-integration","title":"Code Integration","text":"<p>Configuration template for the Ties-Merging algorithm:</p> config/method/ties_merging.yaml<pre><code>name: ties_merging\n# Scaling factor $\\lambda$\nscaling_factor: 0.5\nthreshold: 0.5\n# List of keys to remove from the state dict, default is empty\nremove_keys: []\n# Function to merge the models, default is sum. Options are 'sum', 'mean', and 'max'\nmerge_func: sum \n</code></pre> <p>Use the following command to run the Ties-Merging algorithm:</p> <pre><code>fusion_bench method=ties_merging ...\n</code></pre>"},{"location":"algorithms/ties_merging/#reference","title":"Reference","text":"<ol> <li> <p>(NIPS 2023) Resolving Interference When Merging Models. http://arxiv.org/abs/2306.01708\u00a0\u21a9</p> </li> </ol>"},{"location":"algorithms/ties_merging/#fusion_bench.method.ties_merging.TiesMergingAlgorithm","title":"<code>TiesMergingAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code></p> <p>TiesMergingAlgorithm is a class for fusing multiple models using the TIES merging technique.</p> <p>Attributes:</p> <ul> <li> <code>scaling_factor</code>               (<code>float</code>)           \u2013            <p>The scaling factor to apply to the merged task vector.</p> </li> <li> <code>threshold</code>               (<code>float</code>)           \u2013            <p>The threshold for resetting values in the task vector.</p> </li> <li> <code>remove_keys</code>               (<code>List[str]</code>)           \u2013            <p>List of keys to remove from the state dictionary.</p> </li> <li> <code>merge_func</code>               (<code>Literal['sum', 'mean', 'max']</code>)           \u2013            <p>The merge function to use for disjoint merging.</p> </li> </ul> Source code in <code>fusion_bench/method/ties_merging/ties_merging.py</code> <pre><code>class TiesMergingAlgorithm(BaseAlgorithm):\n    \"\"\"\n    TiesMergingAlgorithm is a class for fusing multiple models using the TIES merging technique.\n\n    Attributes:\n        scaling_factor (float): The scaling factor to apply to the merged task vector.\n        threshold (float): The threshold for resetting values in the task vector.\n        remove_keys (List[str]): List of keys to remove from the state dictionary.\n        merge_func (Literal[\"sum\", \"mean\", \"max\"]): The merge function to use for disjoint merging.\n    \"\"\"\n\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"scaling_factor\": \"scaling_factor\",\n        \"threshold\": \"threshold\",\n        \"remove_keys\": \"remove_keys\",\n        \"merge_func\": \"merge_func\",\n    }\n\n    def __init__(\n        self,\n        scaling_factor: float,\n        threshold: float,\n        remove_keys: List[str],\n        merge_func: Literal[\"sum\", \"mean\", \"max\"],\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the TiesMergingAlgorithm with the given parameters.\n\n        Args:\n            scaling_factor (float): The scaling factor to apply to the merged task vector.\n            threshold (float): The threshold for resetting values in the task vector.\n            remove_keys (List[str]): List of keys to remove from the state dictionary.\n            merge_func (Literal[\"sum\", \"mean\", \"max\"]): The merge function to use for disjoint merging.\n            **kwargs: Additional keyword arguments for the base class.\n        \"\"\"\n        self.scaling_factor = scaling_factor\n        self.threshold = threshold\n        self.remove_keys = remove_keys\n        self.merge_func = merge_func\n        super().__init__(**kwargs)\n\n    @torch.no_grad()\n    def run(self, modelpool: BaseModelPool | Dict[str, nn.Module], **kwargs):\n        \"\"\"\n        Run the TIES merging algorithm to fuse models in the model pool.\n\n        Args:\n            modelpool (BaseModelPool | Dict[str, nn.Module]): The model pool containing the models to fuse.\n\n        Returns:\n            nn.Module: The fused model.\n        \"\"\"\n        log.info(\"Fusing models using ties merging.\")\n        modelpool = to_modelpool(modelpool)\n        remove_keys = self.config.get(\"remove_keys\", [])\n        merge_func = self.config.get(\"merge_func\", \"sum\")\n        scaling_factor = self.scaling_factor\n        threshold = self.threshold\n\n        # Load the pretrained model\n        pretrained_model = modelpool.load_model(\"_pretrained_\")\n\n        # Load the state dicts of the models\n        ft_checks: List[StateDictType] = [\n            modelpool.load_model(model_name).state_dict(keep_vars=True)\n            for model_name in modelpool.model_names\n        ]\n        ptm_check: StateDictType = pretrained_model.state_dict(keep_vars=True)\n\n        # Compute the task vectors\n        flat_ft: Tensor = torch.vstack(\n            [state_dict_to_vector(check, remove_keys) for check in ft_checks]\n        )\n        flat_ptm: Tensor = state_dict_to_vector(ptm_check, remove_keys)\n        tv_flat_checks = flat_ft - flat_ptm\n\n        # Perform TIES Merging\n        merged_tv = ties_merging(\n            tv_flat_checks,\n            reset_thresh=threshold,\n            merge_func=merge_func,\n        )\n        merged_check = flat_ptm + scaling_factor * merged_tv\n        merged_state_dict = vector_to_state_dict(\n            merged_check, ptm_check, remove_keys=remove_keys\n        )\n\n        # Load the merged state dict into the pretrained model\n        pretrained_model.load_state_dict(merged_state_dict)\n        return pretrained_model\n</code></pre>"},{"location":"algorithms/ties_merging/#fusion_bench.method.ties_merging.TiesMergingAlgorithm._config_mapping","title":"<code>_config_mapping = BaseAlgorithm._config_mapping | {'scaling_factor': 'scaling_factor', 'threshold': 'threshold', 'remove_keys': 'remove_keys', 'merge_func': 'merge_func'}</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"algorithms/ties_merging/#fusion_bench.method.ties_merging.TiesMergingAlgorithm.merge_func","title":"<code>merge_func = merge_func</code>  <code>instance-attribute</code>","text":""},{"location":"algorithms/ties_merging/#fusion_bench.method.ties_merging.TiesMergingAlgorithm.remove_keys","title":"<code>remove_keys = remove_keys</code>  <code>instance-attribute</code>","text":""},{"location":"algorithms/ties_merging/#fusion_bench.method.ties_merging.TiesMergingAlgorithm.scaling_factor","title":"<code>scaling_factor = scaling_factor</code>  <code>instance-attribute</code>","text":""},{"location":"algorithms/ties_merging/#fusion_bench.method.ties_merging.TiesMergingAlgorithm.threshold","title":"<code>threshold = threshold</code>  <code>instance-attribute</code>","text":""},{"location":"algorithms/ties_merging/#fusion_bench.method.ties_merging.TiesMergingAlgorithm.__init__","title":"<code>__init__(scaling_factor, threshold, remove_keys, merge_func, **kwargs)</code>","text":"<p>Initialize the TiesMergingAlgorithm with the given parameters.</p> <p>Parameters:</p> Source code in <code>fusion_bench/method/ties_merging/ties_merging.py</code> <pre><code>def __init__(\n    self,\n    scaling_factor: float,\n    threshold: float,\n    remove_keys: List[str],\n    merge_func: Literal[\"sum\", \"mean\", \"max\"],\n    **kwargs,\n):\n    \"\"\"\n    Initialize the TiesMergingAlgorithm with the given parameters.\n\n    Args:\n        scaling_factor (float): The scaling factor to apply to the merged task vector.\n        threshold (float): The threshold for resetting values in the task vector.\n        remove_keys (List[str]): List of keys to remove from the state dictionary.\n        merge_func (Literal[\"sum\", \"mean\", \"max\"]): The merge function to use for disjoint merging.\n        **kwargs: Additional keyword arguments for the base class.\n    \"\"\"\n    self.scaling_factor = scaling_factor\n    self.threshold = threshold\n    self.remove_keys = remove_keys\n    self.merge_func = merge_func\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"algorithms/ties_merging/#fusion_bench.method.ties_merging.TiesMergingAlgorithm.__init__(scaling_factor)","title":"<code>scaling_factor</code>","text":"(<code>float</code>)           \u2013            <p>The scaling factor to apply to the merged task vector.</p>"},{"location":"algorithms/ties_merging/#fusion_bench.method.ties_merging.TiesMergingAlgorithm.__init__(threshold)","title":"<code>threshold</code>","text":"(<code>float</code>)           \u2013            <p>The threshold for resetting values in the task vector.</p>"},{"location":"algorithms/ties_merging/#fusion_bench.method.ties_merging.TiesMergingAlgorithm.__init__(remove_keys)","title":"<code>remove_keys</code>","text":"(<code>List[str]</code>)           \u2013            <p>List of keys to remove from the state dictionary.</p>"},{"location":"algorithms/ties_merging/#fusion_bench.method.ties_merging.TiesMergingAlgorithm.__init__(merge_func)","title":"<code>merge_func</code>","text":"(<code>Literal['sum', 'mean', 'max']</code>)           \u2013            <p>The merge function to use for disjoint merging.</p>"},{"location":"algorithms/ties_merging/#fusion_bench.method.ties_merging.TiesMergingAlgorithm.__init__(**kwargs)","title":"<code>**kwargs</code>","text":"\u2013            <p>Additional keyword arguments for the base class.</p>"},{"location":"algorithms/ties_merging/#fusion_bench.method.ties_merging.TiesMergingAlgorithm.run","title":"<code>run(modelpool, **kwargs)</code>","text":"<p>Run the TIES merging algorithm to fuse models in the model pool.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li>           \u2013            <p>nn.Module: The fused model.</p> </li> </ul> Source code in <code>fusion_bench/method/ties_merging/ties_merging.py</code> <pre><code>@torch.no_grad()\ndef run(self, modelpool: BaseModelPool | Dict[str, nn.Module], **kwargs):\n    \"\"\"\n    Run the TIES merging algorithm to fuse models in the model pool.\n\n    Args:\n        modelpool (BaseModelPool | Dict[str, nn.Module]): The model pool containing the models to fuse.\n\n    Returns:\n        nn.Module: The fused model.\n    \"\"\"\n    log.info(\"Fusing models using ties merging.\")\n    modelpool = to_modelpool(modelpool)\n    remove_keys = self.config.get(\"remove_keys\", [])\n    merge_func = self.config.get(\"merge_func\", \"sum\")\n    scaling_factor = self.scaling_factor\n    threshold = self.threshold\n\n    # Load the pretrained model\n    pretrained_model = modelpool.load_model(\"_pretrained_\")\n\n    # Load the state dicts of the models\n    ft_checks: List[StateDictType] = [\n        modelpool.load_model(model_name).state_dict(keep_vars=True)\n        for model_name in modelpool.model_names\n    ]\n    ptm_check: StateDictType = pretrained_model.state_dict(keep_vars=True)\n\n    # Compute the task vectors\n    flat_ft: Tensor = torch.vstack(\n        [state_dict_to_vector(check, remove_keys) for check in ft_checks]\n    )\n    flat_ptm: Tensor = state_dict_to_vector(ptm_check, remove_keys)\n    tv_flat_checks = flat_ft - flat_ptm\n\n    # Perform TIES Merging\n    merged_tv = ties_merging(\n        tv_flat_checks,\n        reset_thresh=threshold,\n        merge_func=merge_func,\n    )\n    merged_check = flat_ptm + scaling_factor * merged_tv\n    merged_state_dict = vector_to_state_dict(\n        merged_check, ptm_check, remove_keys=remove_keys\n    )\n\n    # Load the merged state dict into the pretrained model\n    pretrained_model.load_state_dict(merged_state_dict)\n    return pretrained_model\n</code></pre>"},{"location":"algorithms/ties_merging/#fusion_bench.method.ties_merging.TiesMergingAlgorithm.run(modelpool)","title":"<code>modelpool</code>","text":"(<code>BaseModelPool | Dict[str, Module]</code>)           \u2013            <p>The model pool containing the models to fuse.</p>"},{"location":"algorithms/weight_ensembling_moe/","title":"Weight-Ensembling Mixture of Experts (Data-Adaptive Model Merging)","text":"(a) Framework overview. This figure shows the overall framework of our proposed method to merge the pre-trained model and fine-tuned task-specific models. We merge weights in the Transformer Layers except for the MLPs. For the MLPs, we upcycle them into weight-assembling MoE modules. (b) Wieght-Ensembling Mixture of Experts (MoE) Module. Here we outline the detailed structure of the Weight-Ensembling MoE module, composed of the router, pre-trained MLP weights, and a collection of task vectors. Collaboration between shared weights and task vectors is employed to create input-conditioned weights dynamically. In this way, we separate shared information and task-specific knowledge, which are then combined based on input in time.      <p>This method is designed to handle a wide range of tasks by segregating shared information and task-specific knowledge.  It dynamically combines these elements based on the input samples.</p> <p>The Weight-Ensembling MoE module consists of three main components: the router, the pre-trained MLP weights, and a collection of task vectors.  The router, which is an MLP, processes the input data and generates routing weights. These weights determine how the knowledge from different tasks is combined. The pre-trained MLP weights are crucial as they have been trained to recognize a wide range of data patterns.  The task vectors represent the differences between the MLPs that have been fine-tuned for specific tasks and the pre-trained ones, capturing the unique adjustments made to optimize them for specific tasks. The routing weights are averaged across the input tokens, and these weights are used to select task vectors from a dictionary matrix. These task vectors are then added to the pre-trained MLP weights to create input-conditioned weights.</p> <p>Algorithm Requirements:</p> Method Access to labeled tasks data Access to validation data (labeled) Test time adaptation Fisher Merging Yes (Estimate Fisher information matrix) No No RegMean Yes (compute Gram Matrix) No No Task Arithmetic No Yes (select sacling factor) No Ties-Merging No Yes (select sacling factor) No AdaMerging No No Yes Ours No No Yes"},{"location":"algorithms/weight_ensembling_moe/#wemoe-v2-e-wemoe","title":"WEMoE V2: E-WEMoE","text":"<p>L. Shen, A. Tang, E. Yang et al. Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task Model Merging. Oct, 2024.<sup>3</sup></p> <p> </p>    (a) Overview of the Efficient Weight-Ensembling Mixture of Experts (E-WEMoE) Framework. It merges all non-MLP modules through task arithmetic and upgrades the MLP modules into an efficient E-WEMoE module. (b) E-WEMoE Module. The module includes a router shared across all Transformer blocks, the pre-trained MLP module, and a set of sparse task-specific vectors w.r.t. MLP modules.         Comparison of (a) trainable parameters and (b) total parameters between WEMoE and E-WEMoE-90%.         Comparison of the relationship between parameter count and performance across various model merging methods."},{"location":"algorithms/weight_ensembling_moe/#parameters-comparison","title":"Parameters Comparison","text":"<p>Tip for reducing the parameter count</p> <p>Here we present the parameter count for the method outlined in the original paper<sup>1</sup>.  An effective strategy to minimize the number of parameters involves employing Singular Value Decomposition (SVD) to compress the task vectors.  This approach significantly cuts down on the number of parameters while only marginally impacting performance.  For additional information, please refer to the Twin-Merging paper<sup>2</sup>.  Which not only reduces the number of parameters but also conducts extensive experiments to demonstrate the effectiveness of data-adaptive merging on language domain.</p> <p>Here is the number of parameters compared to a single pre-trained model (OpenCLIP CLIP-ViT-B/32):</p> Method Trainable Parameters Total Parameters Paremeters Reduced by Merging Single Pre-trained 113.45M (100%) 113.45M - WEMoE (2-layer, 1 task) 7.10M (4.00%) 177.21M - WEMoE (2-layer, 2 tasks) 7.11M (3.04%) 233.89M 2*113.45-233.89=-6.99M WEMoE (2-layer, 3 tasks) 7.11M (2.45%) 290.57M 3*113.45-290.57=49.78M WEMoE (2-layer, 4 tasks) 7.12M (2.02%) 347.25M 4*113.45-347.25=106.55M WEMoE (2-layer, 5 tasks) 7.13M (1.77%) 403.93M 5*113.45-403.93=163.32M WEMoE (2-layer, 6 tasks) 7.14M (1.55%) 460.61M 6*113.45-460.61=220.09M WEMoE (2-layer, 7 tasks) 7.15M (1.38%) 517.28M 7*113.45-517.28=276.87M WEMoE (2-layer, 8 tasks) 7.16M (1.25%) 573.96M 8*113.45-573.96=333.64M <p>The number of parameter count of HuggingFace CLIP vision models (of type <code>transformers.models.clip.modeling_clip.CLIPVisionModel</code>) are different from the OpenCLIP models downloaded from the task arithmetic repo, because the OpenCLIP models (of type <code>src.modeling.ImageEncoder</code>) include the embedding layer for text tokens, while the HuggingFace CLIP vision models do not. Therefore, the relative parameter count of the upscaled model using Transformer CLIP vision models will be larger than the OpenCLIP models.</p> OpenCLIP models of type <code>src.modeling.ImageEncoder</code>Transfomers CLIP vision model of type <code>transformers.models.clip.modeling_clip.CLIPVisionModel</code> <pre><code>ImageEncoder( # (1)\n  (model): CLIP(\n    (visual): VisualTransformer( # (2)\n      (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (transformer): Transformer(\n        (resblocks): ModuleList(\n          (0-11): 12 x ResidualAttentionBlock(\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_attn): Identity()\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (ln): Identity()\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n          )\n        )\n      )\n      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (token_embedding): Embedding(49408, 512) # (3)\n    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  )\n)\n</code></pre> <ol> <li>trainable params: 113.45M || all params: 113.45M || trainable%: 100.0000</li> <li>trainable params: 87.85M || all params: 87.85M || trainable%: 100.0000</li> <li>trainable params: 25.30M || all params: 25.30M || trainable%: 100.0000</li> </ol> <pre><code>CLIPVisionModel( # (1)\n  (vision_model): CLIPVisionTransformer(\n    (embeddings): CLIPVisionEmbeddings(\n      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n      (position_embedding): Embedding(50, 768)\n    )\n    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (encoder): CLIPEncoder(\n      (layers): ModuleList(\n        (0-11): 12 x CLIPEncoderLayer(\n          (self_attn): CLIPAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): CLIPMLP(\n            (activation_fn): QuickGELUActivation()\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n)\n</code></pre> <ol> <li>trainable params: 87.85M || all params: 87.85M || trainable%: 100.0000</li> </ol>"},{"location":"algorithms/weight_ensembling_moe/#loss-landscape-visualization","title":"Loss Landscape Visualization","text":"Visualization of the joint loss \\(\\mathcal{L}_1 + \\mathcal{L}_2\\) and five task pairs for CLIP-ViT-B/32 in the loss landscape.     We perform interpolations between pre-trained weights and two fine-tuned weights in the weight space on a 2D plane using the formula \\(\\theta=\\theta_0 + \\lambda_1 \\tau_1 + \\lambda_2 \\tau_2\\), where \\(\\theta_0\\) represents pre-trained weights, \\(\\tau_i=\\theta_i -\\theta_0\\)  are two task vectors with \\(\\lambda_i\\) in the range [-1, 1]."},{"location":"algorithms/weight_ensembling_moe/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>In the below figure, we show the performance of the merged models with varying numbers of steps. Figure (b) shows the performance of the merged WEMoE models with varying number of steps. In Figure (a), we merge CLIP-ViT-B/32 models with different learning rate configurations. We observe that the performance of the merged model shows an upward trend with an increase in the number of training steps, and it converges rapidly, reaching a high accuracy level in just 200 steps. Furthermore, the influence of different learning rates is not significant, suggesting that our method is insensitive to the learning rate parameter. This is a desirable property as it reduces the need for hyperparameter tuning.</p>  The performance of the merged models with a varying number of steps. (a) CLIP-ViT-B/32 model with different learning rates. (b) Comparison of CLIP-ViT-B/32 and CLIP-ViT-L/14."},{"location":"algorithms/weight_ensembling_moe/#ablations-of-router-depth","title":"Ablations of Router Depth","text":"<p>Table: Parameter comparison of WEMoE (1-layer) and WEMoE (2-layer) on CLIP-ViT-B/32 models (OpenCLIP).</p> Method Number of Trainable Parameters AdaMerging (layer-wise) 1.3K WEMoE (1-layer) 73.8K (0.01%) WEMoE (2-layer) 7.16M (1.25%) <p>Table: Ablation study of the router depth on the performance of the up-scaled CLIP-ViT-B/32 models (OpenCLIP).</p> Method SUN397 CARS RESISC45 EuroSAT SVHN GRSRB MNIST DTD Avg. AdaMerging (layer-wise) 66.6 68.3 82.4 92.5 86.5 93.7 97.7 61.1 80.9 WEMoE (1-layer) 73.2 76.7 93.8 98.6 95.7 98.6 99.5 74.5 88.3 WEMoE (2-layer) 74.1 77.4 93.7 99.1 96.2 98.9 99.6 76.4 89.4 <p>To explore the influence of router depth on the performance of the scaled-up model, we perform an ablation study where the router depth is varied. In WEMoE modules, the router is implemented as a multi-layer perceptron (MLP).</p> <ul> <li>WEMoE (0-layer) functions as a bias-only model, representing a special case of an MLP with no hidden layers. It generates a constant routing weight for all inputs, captured by the formula as \\(r(h) = b_0\\), indicating that it does not adjust based on the input.   When we only up-scale the MLP modules of the vision Transformers to MoE modules, WEMoE (0-layer) can be considered as a partial implementation of AdaMerging. Add when we up-scale the vision Transformers layer-wisely, WEMoE (0-layer) can be considered equivalent to AdaMerging.   For WEMoE (0-layer), the MoE modules can be unloaded, thus no additional parameters and inference cost are introduced.</li> <li>For WEMoE (1-layer), each router is a one-layer MLP that takes the input sample \\(h\\) and outputs the routing weight \\(r(h)\\), which is adaptive to the input. The routing weight is calculated as \\(r(h) = W_1 h + b_1\\).</li> <li>For WEMoE (2-layer), each router is a two-layer MLP and the routing weight is calculated as \\(r(h) = W_2 ReLU(W_1 h + b_1) + b_2\\).</li> </ul> <p>In the above two Tables, we present additional findings to support our argument. We compare the number of trainable parameters and performance between WEMoE (1-layer) and WEMoE (2-layer). The data reveal that WEMoE (1-layer) possesses 73.8K trainable parameters, which constitute only 0.01% of the total parameters in the merged model. Notably, the performance of WEMoE (1-layer) is significantly better than AdaMerging and nearly matches that of WEMoE (2-layer) across all tasks. This evidence underscores our claim that the MoE design is crucial for performance enhancement.</p>"},{"location":"algorithms/weight_ensembling_moe/#code-integration","title":"Code Integration","text":"<p>multi-task model fusion experiment on eight image classification tasks.</p> <pre><code># merge eight CLIP-ViT-B/32 models using WE MoE\nfusion_bench \\\n  method=weight_ensembling_moe \\\n    method.name=clip_weight_ensembling_moe \\\n    method.use_grad_accumulate=false \\\n    method.save_checkpoint=outputs/clip-vit-base-patch32_TA8_weight_ensembling_moe_checkpoint.ckpt \\\n  modelpool=clip-vit-base-patch32_TA8 \\\n  taskpool=clip-vit-classification_TA8\n</code></pre> <p>merge eight CLIP-ViT-L/14 models:</p> <pre><code># merge eight CLIP-ViT-L/14 models using WE MoE, fine-tune the routers\nfusion_bench print_config=false \\\n  method=weight_ensembling_moe \\\n    method.name=clip_weight_ensembling_moe \\\n    method.use_grad_accumulate=true \\\n    method.save_checkpoint=outputs/clip-vit-large-patch14_TA8_weight_ensembling_moe_checkpoint.ckpt \\\n    method.batch_size=4 method.devices=4 \\\n  modelpool=clip-vit-large-patch14_TA8 \\\n  taskpool=dummy &amp;&amp;\n\n# load the checkpoint and evaluate the model\nfusion_bench \\\n  method=weight_ensembling_moe \\\n    method.name=clip_weight_ensembling_moe \\\n    method.checkpoint=outputs/clip-vit-large-patch14_TA8_weight_ensembling_moe_checkpoint.ckpt \\\n  modelpool=clip-vit-large-patch14_TA8 \\\n  taskpool=clip-vit-classification_TA8 \\\n    taskpool.clip_model=openai/clip-vit-large-patch14\n</code></pre>"},{"location":"algorithms/weight_ensembling_moe/#reference","title":"Reference","text":"<ol> <li> <p>Anke Tang et.al. ICML 2024. Merging Multi-Task Models via Weight-Ensembling Mixture of Experts. http://arxiv.org/abs/2402.00433 ICML 2024.\u00a0\u21a9</p> </li> <li> <p>Z. Lu, C. Fan, W. Wei, X. Qu, D. Chen, and Y. Cheng, \u201cTwin-Merging: Dynamic Integration of Modular Expertise in Model Merging,\u201d doi: 10.48550/arXiv.2406.15479. NeurIPS 2024.\u00a0\u21a9</p> </li> <li> <p>L. Shen, A. Tang, E. Yang et al. Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task Model Merging. Oct, 2024.\u00a0\u21a9</p> </li> </ol>"},{"location":"algorithms/weight_ensembling_moe/#fusion_bench.method.we_moe.we_moe","title":"<code>we_moe</code>","text":""},{"location":"algorithms/weight_ensembling_moe/#fusion_bench.method.we_moe.we_moe.WeightEnsemblingMoEAlgorithm","title":"<code>WeightEnsemblingMoEAlgorithm</code>","text":"<p>               Bases: <code>ModelFusionAlgorithm</code></p> <p>Algorithm for fusing models using Weight Ensembling Mixture of Experts (MoE).</p> <p>This class provides methods for constructing the MoE model, performing test-time adaptation, and running the fusion process.</p> <p>Attributes:</p> <ul> <li> <code>_fabric</code>               (<code>Fabric</code>)           \u2013            <p>The fabric for distributed training.</p> </li> <li> <code>modelpool</code>               (<code>ModelPool</code>)           \u2013            <p>The pool of models to be fused.</p> </li> <li> <code>profiler</code>               (<code>SimpleProfiler</code>)           \u2013            <p>The profiler for measuring performance.</p> </li> </ul> Source code in <code>fusion_bench/method/we_moe/we_moe.py</code> <pre><code>class WeightEnsemblingMoEAlgorithm(ModelFusionAlgorithm):\n    \"\"\"\n    Algorithm for fusing models using Weight Ensembling Mixture of Experts (MoE).\n\n    This class provides methods for constructing the MoE model, performing test-time adaptation,\n    and running the fusion process.\n\n    Attributes:\n        _fabric (L.Fabric): The fabric for distributed training.\n        modelpool (ModelPool): The pool of models to be fused.\n        profiler (SimpleProfiler): The profiler for measuring performance.\n    \"\"\"\n\n    _fabric: L.Fabric = None\n    modelpool: ModelPool = None\n\n    def __init__(self, algorithm_config: DictConfig):\n        \"\"\"\n        Initialize the WeightEnsemblingMoEAlgorithm with the given configuration.\n\n        Args:\n            algorithm_config (DictConfig): The configuration for the algorithm.\n        \"\"\"\n        super().__init__(algorithm_config)\n\n        if self._fabric is None and torch.cuda.is_available():\n            self._fabric = L.Fabric(\n                devices=self.config.get(\"devices\", 1),\n            )\n            self._fabric.launch()\n        else:\n            assert \"No CUDA device available.\"\n        self.profiler = SimpleProfiler(\n            self.config.get(\"cache_dir\", \"outputs\"), \"we_moe_profiler.txt\"\n        )\n\n    @abstractmethod\n    def load_checkpoint(self, model, checkpoint):\n        \"\"\"\n        Load the checkpoint file.\n\n        Args:\n            model: The model to load the checkpoint into.\n            checkpoint: The checkpoint file to load.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def save_checkpoint(self, model, checkpoint):\n        \"\"\"\n        Save the checkpoint file.\n\n        Args:\n            model: The model to save the checkpoint from.\n            checkpoint: The checkpoint file to save.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def construct_moe_model(self) -&gt; WeightEnsemblingMoE:\n        \"\"\"\n        Construct the Mixture of Experts model using the models in the model pool.\n\n        Returns:\n            WeightEnsemblingMoE: The constructed MoE model.\n        \"\"\"\n        pass\n\n    def on_test_time_adaptation_start(self):\n        \"\"\"\n        Hook method called at the start of test-time adaptation.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_shuffled_test_loader_iter(self, task: str) -&gt; DataLoader:\n        \"\"\"\n        Get an iterator for the shuffled test data loader for a specific task.\n\n        Args:\n            task (str): The task for which to get the test data loader.\n\n        Returns:\n            DataLoader: The shuffled test data loader iterator.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def compute_logits(self, module, batch, task) -&gt; Tensor:\n        \"\"\"\n        Compute the logits for a given batch and task.\n\n        Args:\n            module: The model module to use for computing logits.\n            batch: The batch of data.\n            task: The task for which to compute logits.\n\n        Returns:\n            Tensor: The computed logits.\n        \"\"\"\n        pass\n\n    def test_time_adaptation(self, module: WeightEnsemblingMoE):\n        \"\"\"\n        Perform test-time adaptation for the given module.\n\n        Args:\n            module (WeightEnsemblingMoE): The MoE module to adapt.\n\n        Returns:\n            WeightEnsemblingMoE: The adapted MoE module.\n        \"\"\"\n        self.on_test_time_adaptation_start()\n\n        # configure optimizer\n        if self.config.optimizer == \"adam\":\n            optimizer = torch.optim.Adam(\n                [p for p in module.parameters() if p.requires_grad], lr=self.config.lr\n            )\n        else:\n            raise ValueError(f\"Unsupported optimizer: {self.config.optimizer}\")\n\n        if self._fabric is not None:\n            module, optimizer = self._fabric.setup(module, optimizer)\n\n        module.train()\n\n        if self.config.get(\"fast_dev_run\", False):\n            log.info(\"Running fast_dev_run, only one step\")\n            pbar = tqdm(\n                range(1),\n                \"Test-time adaptation\",\n                dynamic_ncols=True,\n            )\n        else:\n            pbar = tqdm(\n                range(self.config.max_steps),\n                \"Test-time adaptation\",\n                dynamic_ncols=True,\n            )\n        for step_idx in pbar:\n            if self.config.use_grad_accumulate:\n                for task in self.modelpool.model_names:\n                    with self.profiler.profile(\"data time\"):\n                        batch = next(self.get_shuffled_test_loader_iter(task))\n                    with self.profiler.profile(\"forward pass\"):\n                        logits = self.compute_logits(module, batch, task)\n                        assert (\n                            logits.dim() == 2\n                        ), f\"Expected logits to be 2D, got {logits.dim()}\"\n                        loss = entropy_loss(logits)\n                    # .backward() accumulates when .zero_grad() wasn't called\n                    # this can save memory\n                    with self.profiler.profile(\"backward pass\"):\n                        self._fabric.backward(loss, retain_graph=True)\n            else:\n                loss = 0\n                for task in self.modelpool.model_names:\n                    with self.profiler.profile(\"data time\"):\n                        batch = next(self.get_shuffled_test_loader_iter(task))\n                    with self.profiler.profile(\"forward pass\"):\n                        logits = self.compute_logits(module, batch, task)\n                        assert (\n                            logits.dim() == 2\n                        ), f\"Expected logits to be 2D, got {logits.dim()}\"\n                        loss = loss + entropy_loss(logits)\n                with self.profiler.profile(\"backward pass\"):\n                    self._fabric.backward(loss, retain_graph=True)\n\n            with self.profiler.profile(\"optimizer step\"):\n                optimizer.step()\n                optimizer.zero_grad()\n\n        return module\n\n    def run(self, modelpool: ModelPool):\n        \"\"\"\n        Run the WeightEnsemblingMoEAlgorithm to fuse models using Weight Ensembling Mixture of Experts.\n\n        Args:\n            modelpool (ModelPool): The pool of models to be fused.\n\n        Returns:\n            WeightEnsemblingMoE: The fused MoE model.\n        \"\"\"\n        log.info(\"Fusing models using WeightEnsembling Mixture of Experts modules.\")\n        self.modelpool = modelpool\n\n        with timeit_context(\"upscaling models to a weight-ensembling MoE model\"):\n            moe_model = self.construct_moe_model()\n            print_parameters(moe_model)\n\n        if self.config.get(\"checkpoint\", False):\n            log.info(\n                f\"load checkpoint from {self.config.checkpoint}, test-time adaptation will be skipped.\"\n            )\n            self.load_checkpoint(moe_model, self.config.checkpoint)\n        else:\n            with self.profiler.profile(\"test-time adaptation\"):\n                moe_model = self.test_time_adaptation(moe_model)\n            if self.config.get(\"save_checkpoint\", False):\n                log.info(f\"save checkpoint to {self.config.save_checkpoint}\")\n                self.save_checkpoint(moe_model, self.config.save_checkpoint)\n\n            if lightning.fabric.wrappers.is_wrapped(moe_model):\n                moe_model = lightning.fabric.wrappers._unwrap_objects(moe_model)\n\n        # enable sample-wise adaptation\n        moe_model.batch_reduce = False\n        print(self.profiler.summary())\n        return moe_model\n</code></pre>"},{"location":"algorithms/weight_ensembling_moe/#fusion_bench.method.we_moe.we_moe.WeightEnsemblingMoEAlgorithm.__init__","title":"<code>__init__(algorithm_config)</code>","text":"<p>Initialize the WeightEnsemblingMoEAlgorithm with the given configuration.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_config</code> \u00b6              (<code>DictConfig</code>)           \u2013            <p>The configuration for the algorithm.</p> </li> </ul> Source code in <code>fusion_bench/method/we_moe/we_moe.py</code> <pre><code>def __init__(self, algorithm_config: DictConfig):\n    \"\"\"\n    Initialize the WeightEnsemblingMoEAlgorithm with the given configuration.\n\n    Args:\n        algorithm_config (DictConfig): The configuration for the algorithm.\n    \"\"\"\n    super().__init__(algorithm_config)\n\n    if self._fabric is None and torch.cuda.is_available():\n        self._fabric = L.Fabric(\n            devices=self.config.get(\"devices\", 1),\n        )\n        self._fabric.launch()\n    else:\n        assert \"No CUDA device available.\"\n    self.profiler = SimpleProfiler(\n        self.config.get(\"cache_dir\", \"outputs\"), \"we_moe_profiler.txt\"\n    )\n</code></pre>"},{"location":"algorithms/weight_ensembling_moe/#fusion_bench.method.we_moe.we_moe.WeightEnsemblingMoEAlgorithm.compute_logits","title":"<code>compute_logits(module, batch, task)</code>  <code>abstractmethod</code>","text":"<p>Compute the logits for a given batch and task.</p> <p>Parameters:</p> <ul> <li> <code>module</code> \u00b6          \u2013            <p>The model module to use for computing logits.</p> </li> <li> <code>batch</code> \u00b6          \u2013            <p>The batch of data.</p> </li> <li> <code>task</code> \u00b6          \u2013            <p>The task for which to compute logits.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>Tensor</code> )          \u2013            <p>The computed logits.</p> </li> </ul> Source code in <code>fusion_bench/method/we_moe/we_moe.py</code> <pre><code>@abstractmethod\ndef compute_logits(self, module, batch, task) -&gt; Tensor:\n    \"\"\"\n    Compute the logits for a given batch and task.\n\n    Args:\n        module: The model module to use for computing logits.\n        batch: The batch of data.\n        task: The task for which to compute logits.\n\n    Returns:\n        Tensor: The computed logits.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"algorithms/weight_ensembling_moe/#fusion_bench.method.we_moe.we_moe.WeightEnsemblingMoEAlgorithm.construct_moe_model","title":"<code>construct_moe_model()</code>  <code>abstractmethod</code>","text":"<p>Construct the Mixture of Experts model using the models in the model pool.</p> <p>Returns:</p> <ul> <li> <code>WeightEnsemblingMoE</code> (              <code>WeightEnsemblingMoE</code> )          \u2013            <p>The constructed MoE model.</p> </li> </ul> Source code in <code>fusion_bench/method/we_moe/we_moe.py</code> <pre><code>@abstractmethod\ndef construct_moe_model(self) -&gt; WeightEnsemblingMoE:\n    \"\"\"\n    Construct the Mixture of Experts model using the models in the model pool.\n\n    Returns:\n        WeightEnsemblingMoE: The constructed MoE model.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"algorithms/weight_ensembling_moe/#fusion_bench.method.we_moe.we_moe.WeightEnsemblingMoEAlgorithm.get_shuffled_test_loader_iter","title":"<code>get_shuffled_test_loader_iter(task)</code>  <code>abstractmethod</code>","text":"<p>Get an iterator for the shuffled test data loader for a specific task.</p> <p>Parameters:</p> <ul> <li> <code>task</code> \u00b6              (<code>str</code>)           \u2013            <p>The task for which to get the test data loader.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code> (              <code>DataLoader</code> )          \u2013            <p>The shuffled test data loader iterator.</p> </li> </ul> Source code in <code>fusion_bench/method/we_moe/we_moe.py</code> <pre><code>@abstractmethod\ndef get_shuffled_test_loader_iter(self, task: str) -&gt; DataLoader:\n    \"\"\"\n    Get an iterator for the shuffled test data loader for a specific task.\n\n    Args:\n        task (str): The task for which to get the test data loader.\n\n    Returns:\n        DataLoader: The shuffled test data loader iterator.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"algorithms/weight_ensembling_moe/#fusion_bench.method.we_moe.we_moe.WeightEnsemblingMoEAlgorithm.load_checkpoint","title":"<code>load_checkpoint(model, checkpoint)</code>  <code>abstractmethod</code>","text":"<p>Load the checkpoint file.</p> <p>Parameters:</p> <ul> <li> <code>model</code> \u00b6          \u2013            <p>The model to load the checkpoint into.</p> </li> <li> <code>checkpoint</code> \u00b6          \u2013            <p>The checkpoint file to load.</p> </li> </ul> Source code in <code>fusion_bench/method/we_moe/we_moe.py</code> <pre><code>@abstractmethod\ndef load_checkpoint(self, model, checkpoint):\n    \"\"\"\n    Load the checkpoint file.\n\n    Args:\n        model: The model to load the checkpoint into.\n        checkpoint: The checkpoint file to load.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"algorithms/weight_ensembling_moe/#fusion_bench.method.we_moe.we_moe.WeightEnsemblingMoEAlgorithm.on_test_time_adaptation_start","title":"<code>on_test_time_adaptation_start()</code>","text":"<p>Hook method called at the start of test-time adaptation.</p> Source code in <code>fusion_bench/method/we_moe/we_moe.py</code> <pre><code>def on_test_time_adaptation_start(self):\n    \"\"\"\n    Hook method called at the start of test-time adaptation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"algorithms/weight_ensembling_moe/#fusion_bench.method.we_moe.we_moe.WeightEnsemblingMoEAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Run the WeightEnsemblingMoEAlgorithm to fuse models using Weight Ensembling Mixture of Experts.</p> <p>Parameters:</p> <ul> <li> <code>modelpool</code> \u00b6              (<code>ModelPool</code>)           \u2013            <p>The pool of models to be fused.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>WeightEnsemblingMoE</code>          \u2013            <p>The fused MoE model.</p> </li> </ul> Source code in <code>fusion_bench/method/we_moe/we_moe.py</code> <pre><code>def run(self, modelpool: ModelPool):\n    \"\"\"\n    Run the WeightEnsemblingMoEAlgorithm to fuse models using Weight Ensembling Mixture of Experts.\n\n    Args:\n        modelpool (ModelPool): The pool of models to be fused.\n\n    Returns:\n        WeightEnsemblingMoE: The fused MoE model.\n    \"\"\"\n    log.info(\"Fusing models using WeightEnsembling Mixture of Experts modules.\")\n    self.modelpool = modelpool\n\n    with timeit_context(\"upscaling models to a weight-ensembling MoE model\"):\n        moe_model = self.construct_moe_model()\n        print_parameters(moe_model)\n\n    if self.config.get(\"checkpoint\", False):\n        log.info(\n            f\"load checkpoint from {self.config.checkpoint}, test-time adaptation will be skipped.\"\n        )\n        self.load_checkpoint(moe_model, self.config.checkpoint)\n    else:\n        with self.profiler.profile(\"test-time adaptation\"):\n            moe_model = self.test_time_adaptation(moe_model)\n        if self.config.get(\"save_checkpoint\", False):\n            log.info(f\"save checkpoint to {self.config.save_checkpoint}\")\n            self.save_checkpoint(moe_model, self.config.save_checkpoint)\n\n        if lightning.fabric.wrappers.is_wrapped(moe_model):\n            moe_model = lightning.fabric.wrappers._unwrap_objects(moe_model)\n\n    # enable sample-wise adaptation\n    moe_model.batch_reduce = False\n    print(self.profiler.summary())\n    return moe_model\n</code></pre>"},{"location":"algorithms/weight_ensembling_moe/#fusion_bench.method.we_moe.we_moe.WeightEnsemblingMoEAlgorithm.save_checkpoint","title":"<code>save_checkpoint(model, checkpoint)</code>  <code>abstractmethod</code>","text":"<p>Save the checkpoint file.</p> <p>Parameters:</p> <ul> <li> <code>model</code> \u00b6          \u2013            <p>The model to save the checkpoint from.</p> </li> <li> <code>checkpoint</code> \u00b6          \u2013            <p>The checkpoint file to save.</p> </li> </ul> Source code in <code>fusion_bench/method/we_moe/we_moe.py</code> <pre><code>@abstractmethod\ndef save_checkpoint(self, model, checkpoint):\n    \"\"\"\n    Save the checkpoint file.\n\n    Args:\n        model: The model to save the checkpoint from.\n        checkpoint: The checkpoint file to save.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"algorithms/weight_ensembling_moe/#fusion_bench.method.we_moe.we_moe.WeightEnsemblingMoEAlgorithm.test_time_adaptation","title":"<code>test_time_adaptation(module)</code>","text":"<p>Perform test-time adaptation for the given module.</p> <p>Parameters:</p> <ul> <li> <code>module</code> \u00b6              (<code>WeightEnsemblingMoE</code>)           \u2013            <p>The MoE module to adapt.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>WeightEnsemblingMoE</code>          \u2013            <p>The adapted MoE module.</p> </li> </ul> Source code in <code>fusion_bench/method/we_moe/we_moe.py</code> <pre><code>def test_time_adaptation(self, module: WeightEnsemblingMoE):\n    \"\"\"\n    Perform test-time adaptation for the given module.\n\n    Args:\n        module (WeightEnsemblingMoE): The MoE module to adapt.\n\n    Returns:\n        WeightEnsemblingMoE: The adapted MoE module.\n    \"\"\"\n    self.on_test_time_adaptation_start()\n\n    # configure optimizer\n    if self.config.optimizer == \"adam\":\n        optimizer = torch.optim.Adam(\n            [p for p in module.parameters() if p.requires_grad], lr=self.config.lr\n        )\n    else:\n        raise ValueError(f\"Unsupported optimizer: {self.config.optimizer}\")\n\n    if self._fabric is not None:\n        module, optimizer = self._fabric.setup(module, optimizer)\n\n    module.train()\n\n    if self.config.get(\"fast_dev_run\", False):\n        log.info(\"Running fast_dev_run, only one step\")\n        pbar = tqdm(\n            range(1),\n            \"Test-time adaptation\",\n            dynamic_ncols=True,\n        )\n    else:\n        pbar = tqdm(\n            range(self.config.max_steps),\n            \"Test-time adaptation\",\n            dynamic_ncols=True,\n        )\n    for step_idx in pbar:\n        if self.config.use_grad_accumulate:\n            for task in self.modelpool.model_names:\n                with self.profiler.profile(\"data time\"):\n                    batch = next(self.get_shuffled_test_loader_iter(task))\n                with self.profiler.profile(\"forward pass\"):\n                    logits = self.compute_logits(module, batch, task)\n                    assert (\n                        logits.dim() == 2\n                    ), f\"Expected logits to be 2D, got {logits.dim()}\"\n                    loss = entropy_loss(logits)\n                # .backward() accumulates when .zero_grad() wasn't called\n                # this can save memory\n                with self.profiler.profile(\"backward pass\"):\n                    self._fabric.backward(loss, retain_graph=True)\n        else:\n            loss = 0\n            for task in self.modelpool.model_names:\n                with self.profiler.profile(\"data time\"):\n                    batch = next(self.get_shuffled_test_loader_iter(task))\n                with self.profiler.profile(\"forward pass\"):\n                    logits = self.compute_logits(module, batch, task)\n                    assert (\n                        logits.dim() == 2\n                    ), f\"Expected logits to be 2D, got {logits.dim()}\"\n                    loss = loss + entropy_loss(logits)\n            with self.profiler.profile(\"backward pass\"):\n                self._fabric.backward(loss, retain_graph=True)\n\n        with self.profiler.profile(\"optimizer step\"):\n            optimizer.step()\n            optimizer.zero_grad()\n\n    return module\n</code></pre>"},{"location":"algorithms/weight_ensembling_moe/#fusion_bench.method.we_moe.we_moe.entropy_loss","title":"<code>entropy_loss(logits)</code>","text":"<p>Compute the entropy loss of a set of logits.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>Tensor</code> )          \u2013            <p>The entropy loss of the logits.</p> </li> </ul> Source code in <code>fusion_bench/method/we_moe/we_moe.py</code> <pre><code>def entropy_loss(logits: Tensor) -&gt; Tensor:\n    \"\"\"\n    Compute the entropy loss of a set of logits.\n\n    Args:\n        logits (Tensor): The logits to compute the entropy loss of.\n\n    Returns:\n        Tensor: The entropy loss of the logits.\n    \"\"\"\n    probs = torch.softmax(logits, dim=-1)\n    return -torch.sum(probs * torch.log(probs + 1e-8), dim=-1).mean()\n</code></pre>"},{"location":"algorithms/weight_ensembling_moe/#fusion_bench.method.we_moe.we_moe.entropy_loss(logits)","title":"<code>logits</code>","text":"(<code>Tensor</code>)           \u2013            <p>The logits to compute the entropy loss of.</p>"},{"location":"algorithms/weight_ensembling_moe/#fusion_bench.method.we_moe.clip_we_moe","title":"<code>clip_we_moe</code>","text":""},{"location":"algorithms/weight_ensembling_moe/#fusion_bench.method.we_moe.clip_we_moe.CLIPWeightEnsemblingMoEAlgorithm","title":"<code>CLIPWeightEnsemblingMoEAlgorithm</code>","text":"<p>               Bases: <code>WeightEnsemblingMoEAlgorithm</code>, <code>CLIPClassificationMixin</code></p> <p>CLIPWeightEnsemblingMoEAlgorithm is a class that implements the WeightEnsemblingMoEAlgorithm for CLIP models. It extends the WeightEnsemblingMoEAlgorithm and CLIPClassificationMixin classes.</p> <p>Attributes:</p> <ul> <li> <code>modelpool</code>               (<code>CLIPVisionModelPool</code>)           \u2013            <p>The model pool containing the CLIP models.</p> </li> </ul> Source code in <code>fusion_bench/method/we_moe/clip_we_moe.py</code> <pre><code>class CLIPWeightEnsemblingMoEAlgorithm(\n    WeightEnsemblingMoEAlgorithm,\n    CLIPClassificationMixin,\n):\n    \"\"\"\n    CLIPWeightEnsemblingMoEAlgorithm is a class that implements the WeightEnsemblingMoEAlgorithm\n    for CLIP models. It extends the WeightEnsemblingMoEAlgorithm and CLIPClassificationMixin classes.\n\n    Attributes:\n        modelpool (CLIPVisionModelPool): The model pool containing the CLIP models.\n    \"\"\"\n\n    modelpool: CLIPVisionModelPool = None\n\n    def load_checkpoint(self, model, checkpoint):\n        \"\"\"\n        Load the checkpoint file.\n\n        Args:\n            model: The model to load the checkpoint into.\n            checkpoint: The path to the checkpoint file.\n        \"\"\"\n        state = {\"model\": model}\n        self._fabric.load(checkpoint, state)\n\n    def save_checkpoint(self, model, checkpoint):\n        \"\"\"\n        Save the checkpoint file.\n\n        Args:\n            model: The model to save the checkpoint from.\n            checkpoint: The path to the checkpoint file.\n        \"\"\"\n        self._fabric.save(checkpoint, {\"model\": model})\n\n    def construct_moe_model(self) -&gt; WeightEnsemblingMoE:\n        \"\"\"\n        Construct the Mixture of Experts (MoE) model using the models in the model pool.\n\n        Returns:\n            WeightEnsemblingMoE: The constructed MoE model.\n        \"\"\"\n        base_model = self.modelpool.load_model(\"_pretrained_\")\n        expert_models = [\n            self.modelpool.load_model(m) for m in self.modelpool.model_names\n        ]\n\n        # Merge the models using task arithmetic\n        moe_model = task_arithmetic_merge(\n            # This function modifies the model in place, so we need to pass a deepcopy\n            deepcopy(base_model),\n            expert_models,\n            scaling_factor=self.config.init_lambda,\n        ).requires_grad_(False)\n\n        # Up-scale MLP modules\n        base_encoder: CLIPEncoder = base_model.vision_model.encoder\n        moe_encoder: CLIPEncoder = moe_model.vision_model.encoder\n        expert_encoders = [m.vision_model.encoder for m in expert_models]\n\n        num_layers = len(base_encoder.layers)\n        for layer_idx in range(num_layers):\n            base_mlp = base_encoder.layers[layer_idx].mlp\n            expert_mlps = [e.layers[layer_idx].mlp for e in expert_encoders]\n\n            moe_encoder.layers[layer_idx].mlp = WeightEnsemblingMoE(\n                hidden_size=base_encoder.config.hidden_size,\n                base_model=base_mlp,\n                expert_models=expert_mlps,\n                init_lambda=self.config.init_lambda,\n                batch_first=True,  # For open_clip models this is False\n                router_hidden_layers=self.config.router_hidden_layers,\n                batch_reduce=self.config.batch_reduce,\n            )\n\n        return moe_model\n\n    @functools.cache\n    def get_shuffled_test_loader_iter(self, tta_dataset: str):\n        \"\"\"\n        Get an iterator for the shuffled test data loader.\n\n        Args:\n            tta_dataset (str): The name of the test-time adaptation dataset.\n\n        Returns:\n            Iterator: An iterator for the shuffled test data loader.\n        \"\"\"\n        dataset = self.modelpool.load_test_dataset(tta_dataset)\n        dataset = CLIPDataset(dataset, processor=self.clip_processor)\n        log.info(\"get_shuffled_test_loader_iter\")\n        loader = DataLoader(\n            dataset,\n            batch_size=self.config.batch_size,\n            shuffle=True,\n            num_workers=self.config.num_workers,\n            pin_memory=True,\n        )\n        loader = self.fabric.setup_dataloaders(loader)\n        return iter(InfiniteDataLoader(loader))\n\n    def on_test_time_adaptation_start(self):\n        \"\"\"\n        Load the CLIP processor and construct the zero-shot classification head for each task.\n        \"\"\"\n        self.setup_zero_shot_classification_head()\n\n    def compute_logits(self, module, batch, task) -&gt; Tensor:\n        \"\"\"\n        Compute the logits for the given batch and task.\n\n        Args:\n            module: The model module.\n            batch: The input batch.\n            task: The task name.\n\n        Returns:\n            Tensor: The computed logits.\n        \"\"\"\n        images, _ = batch\n        text_embeds = self.zeroshot_weights[task]\n\n        image_embeds = module(images)[1]\n        image_embeds = self.visual_projection(image_embeds)\n\n        # Normalize embeddings\n        image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n\n        # Cosine similarity\n        logits_per_text = (\n            torch.matmul(text_embeds, image_embeds.t()) * self.logit_scale_exp\n        )\n        logits_per_image = logits_per_text.t()\n\n        return logits_per_image\n</code></pre>"},{"location":"algorithms/weight_ensembling_moe/#fusion_bench.method.we_moe.clip_we_moe.CLIPWeightEnsemblingMoEAlgorithm.compute_logits","title":"<code>compute_logits(module, batch, task)</code>","text":"<p>Compute the logits for the given batch and task.</p> <p>Parameters:</p> <ul> <li> <code>module</code> \u00b6          \u2013            <p>The model module.</p> </li> <li> <code>batch</code> \u00b6          \u2013            <p>The input batch.</p> </li> <li> <code>task</code> \u00b6          \u2013            <p>The task name.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>Tensor</code> )          \u2013            <p>The computed logits.</p> </li> </ul> Source code in <code>fusion_bench/method/we_moe/clip_we_moe.py</code> <pre><code>def compute_logits(self, module, batch, task) -&gt; Tensor:\n    \"\"\"\n    Compute the logits for the given batch and task.\n\n    Args:\n        module: The model module.\n        batch: The input batch.\n        task: The task name.\n\n    Returns:\n        Tensor: The computed logits.\n    \"\"\"\n    images, _ = batch\n    text_embeds = self.zeroshot_weights[task]\n\n    image_embeds = module(images)[1]\n    image_embeds = self.visual_projection(image_embeds)\n\n    # Normalize embeddings\n    image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n\n    # Cosine similarity\n    logits_per_text = (\n        torch.matmul(text_embeds, image_embeds.t()) * self.logit_scale_exp\n    )\n    logits_per_image = logits_per_text.t()\n\n    return logits_per_image\n</code></pre>"},{"location":"algorithms/weight_ensembling_moe/#fusion_bench.method.we_moe.clip_we_moe.CLIPWeightEnsemblingMoEAlgorithm.construct_moe_model","title":"<code>construct_moe_model()</code>","text":"<p>Construct the Mixture of Experts (MoE) model using the models in the model pool.</p> <p>Returns:</p> <ul> <li> <code>WeightEnsemblingMoE</code> (              <code>WeightEnsemblingMoE</code> )          \u2013            <p>The constructed MoE model.</p> </li> </ul> Source code in <code>fusion_bench/method/we_moe/clip_we_moe.py</code> <pre><code>def construct_moe_model(self) -&gt; WeightEnsemblingMoE:\n    \"\"\"\n    Construct the Mixture of Experts (MoE) model using the models in the model pool.\n\n    Returns:\n        WeightEnsemblingMoE: The constructed MoE model.\n    \"\"\"\n    base_model = self.modelpool.load_model(\"_pretrained_\")\n    expert_models = [\n        self.modelpool.load_model(m) for m in self.modelpool.model_names\n    ]\n\n    # Merge the models using task arithmetic\n    moe_model = task_arithmetic_merge(\n        # This function modifies the model in place, so we need to pass a deepcopy\n        deepcopy(base_model),\n        expert_models,\n        scaling_factor=self.config.init_lambda,\n    ).requires_grad_(False)\n\n    # Up-scale MLP modules\n    base_encoder: CLIPEncoder = base_model.vision_model.encoder\n    moe_encoder: CLIPEncoder = moe_model.vision_model.encoder\n    expert_encoders = [m.vision_model.encoder for m in expert_models]\n\n    num_layers = len(base_encoder.layers)\n    for layer_idx in range(num_layers):\n        base_mlp = base_encoder.layers[layer_idx].mlp\n        expert_mlps = [e.layers[layer_idx].mlp for e in expert_encoders]\n\n        moe_encoder.layers[layer_idx].mlp = WeightEnsemblingMoE(\n            hidden_size=base_encoder.config.hidden_size,\n            base_model=base_mlp,\n            expert_models=expert_mlps,\n            init_lambda=self.config.init_lambda,\n            batch_first=True,  # For open_clip models this is False\n            router_hidden_layers=self.config.router_hidden_layers,\n            batch_reduce=self.config.batch_reduce,\n        )\n\n    return moe_model\n</code></pre>"},{"location":"algorithms/weight_ensembling_moe/#fusion_bench.method.we_moe.clip_we_moe.CLIPWeightEnsemblingMoEAlgorithm.get_shuffled_test_loader_iter","title":"<code>get_shuffled_test_loader_iter(tta_dataset)</code>  <code>cached</code>","text":"<p>Get an iterator for the shuffled test data loader.</p> <p>Parameters:</p> <ul> <li> <code>tta_dataset</code> \u00b6              (<code>str</code>)           \u2013            <p>The name of the test-time adaptation dataset.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Iterator</code>          \u2013            <p>An iterator for the shuffled test data loader.</p> </li> </ul> Source code in <code>fusion_bench/method/we_moe/clip_we_moe.py</code> <pre><code>@functools.cache\ndef get_shuffled_test_loader_iter(self, tta_dataset: str):\n    \"\"\"\n    Get an iterator for the shuffled test data loader.\n\n    Args:\n        tta_dataset (str): The name of the test-time adaptation dataset.\n\n    Returns:\n        Iterator: An iterator for the shuffled test data loader.\n    \"\"\"\n    dataset = self.modelpool.load_test_dataset(tta_dataset)\n    dataset = CLIPDataset(dataset, processor=self.clip_processor)\n    log.info(\"get_shuffled_test_loader_iter\")\n    loader = DataLoader(\n        dataset,\n        batch_size=self.config.batch_size,\n        shuffle=True,\n        num_workers=self.config.num_workers,\n        pin_memory=True,\n    )\n    loader = self.fabric.setup_dataloaders(loader)\n    return iter(InfiniteDataLoader(loader))\n</code></pre>"},{"location":"algorithms/weight_ensembling_moe/#fusion_bench.method.we_moe.clip_we_moe.CLIPWeightEnsemblingMoEAlgorithm.load_checkpoint","title":"<code>load_checkpoint(model, checkpoint)</code>","text":"<p>Load the checkpoint file.</p> <p>Parameters:</p> <ul> <li> <code>model</code> \u00b6          \u2013            <p>The model to load the checkpoint into.</p> </li> <li> <code>checkpoint</code> \u00b6          \u2013            <p>The path to the checkpoint file.</p> </li> </ul> Source code in <code>fusion_bench/method/we_moe/clip_we_moe.py</code> <pre><code>def load_checkpoint(self, model, checkpoint):\n    \"\"\"\n    Load the checkpoint file.\n\n    Args:\n        model: The model to load the checkpoint into.\n        checkpoint: The path to the checkpoint file.\n    \"\"\"\n    state = {\"model\": model}\n    self._fabric.load(checkpoint, state)\n</code></pre>"},{"location":"algorithms/weight_ensembling_moe/#fusion_bench.method.we_moe.clip_we_moe.CLIPWeightEnsemblingMoEAlgorithm.on_test_time_adaptation_start","title":"<code>on_test_time_adaptation_start()</code>","text":"<p>Load the CLIP processor and construct the zero-shot classification head for each task.</p> Source code in <code>fusion_bench/method/we_moe/clip_we_moe.py</code> <pre><code>def on_test_time_adaptation_start(self):\n    \"\"\"\n    Load the CLIP processor and construct the zero-shot classification head for each task.\n    \"\"\"\n    self.setup_zero_shot_classification_head()\n</code></pre>"},{"location":"algorithms/weight_ensembling_moe/#fusion_bench.method.we_moe.clip_we_moe.CLIPWeightEnsemblingMoEAlgorithm.save_checkpoint","title":"<code>save_checkpoint(model, checkpoint)</code>","text":"<p>Save the checkpoint file.</p> <p>Parameters:</p> <ul> <li> <code>model</code> \u00b6          \u2013            <p>The model to save the checkpoint from.</p> </li> <li> <code>checkpoint</code> \u00b6          \u2013            <p>The path to the checkpoint file.</p> </li> </ul> Source code in <code>fusion_bench/method/we_moe/clip_we_moe.py</code> <pre><code>def save_checkpoint(self, model, checkpoint):\n    \"\"\"\n    Save the checkpoint file.\n\n    Args:\n        model: The model to save the checkpoint from.\n        checkpoint: The path to the checkpoint file.\n    \"\"\"\n    self._fabric.save(checkpoint, {\"model\": model})\n</code></pre>"},{"location":"algorithms/weighted_averaging/","title":"Weighted Averaging","text":"<p>Weighted averaging, also known as weight-ensembling. In the context of full fine-tuned models, the weights are averaged according to their respective performance weights. Concretely, this means that if we have \\(n\\) models with their respective weights \\(\\theta_i\\) and model-wise weights \\(w_i\\), the weights of the final model \\(\\theta\\) are computed as:</p> \\[ \\theta = \\sum_{i=1}^{n} w_i \\theta_i \\]"},{"location":"algorithms/weighted_averaging/#examples","title":"Examples","text":""},{"location":"algorithms/weighted_averaging/#general-usage","title":"General Usage","text":"<p>Configuration template for the Weighted Averaging algorithm:</p> config/method/weighted_average.yaml<pre><code>name: weighted_average\nnormalize: true # if true, the weights will be normalized before merging\nweights: # List of weights for each model\n  - 0.5\n  - 0.5\n</code></pre> <p>Use the following command to run the Weighted Averaging algorithm:</p> <pre><code>fusion_bench method=weighted_average ...\n</code></pre>"},{"location":"algorithms/weighted_averaging/#merge-clip-vit-models","title":"Merge CLIP-ViT Models","text":"<p>The following command merges eight clip-ViT models using a weighted average approach. Because <code>method.normalize</code> is set to true, the weights are normalized to sum to 1, thus equivalent to simple average.</p> <pre><code>fusion_bench \\\n    method=weighted_average \\\n    method.normalize=true \\\n    method.weights=[0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3] \\\n    modelpool=clip-vit-base-patch32_TA8_model_only \\\n    taskpool=clip-vit-classification_TA8\n</code></pre>"},{"location":"algorithms/weighted_averaging/#merge-llamamistral-models","title":"Merge Llama/Mistral Models","text":"<p>Here is an example of how to use the Weighted Averaging algorithm to merge two LLama models. In particular, LLaMa models of the type <code>transformers.LlamaForCausalLM</code> are merged using the Weighted Averaging algorithm.</p> <pre><code>fusion_bench \\\n    method=weighted_average_for_llama \\\n    method.merged_model_save_path=outputs/test_merged_llama_model \\\n    modelpool=llama_for_causallm \\\n    taskpool=dummy\n</code></pre> <p>or using the following configuration file <code>config/llama_weighted_average.yaml</code></p> <pre><code>fusion_bench --config-name llama_weighted_average\n</code></pre> config/llama_weighted_average.yaml<pre><code>defaults:\n  - example_config\n  - override method: weighted_average_for_llama\n  - override modelpool: llama_for_causallm\n  - _self_\n\nmodelpool:\n  models:\n    # the pre-trained model (base model) is optional\n    # if not provided, the first model will be used as the base model\n    - name: _pretrained_\n      path: meta-llama/Meta-Llama-3-8B\n    - name: expert_1\n      path: meta-llama/Meta-Llama-3-8B\n    - name: expert_2\n      path: meta-llama/Meta-Llama-3-8B-Instruct\n\nmethod:\n  normalize: true # if true, the weights will be normalized before merging\n  weights: # List of weights for each model\n    - 0.5\n    - 0.5\n  # if true, only the backbone of the model will be merged and the head will be keeped as the pre-trained model (if the pre-trained model is provided, otherwise the head of the first model will be used)\n  # if false, the whole model will be merged\n  backbone_only: true\n\n  merged_model_save_path: null\n  save_tokenizer: true\n  push_to_hub: false\n</code></pre>"},{"location":"algorithms/weighted_averaging/#references","title":"References","text":""},{"location":"algorithms/weighted_averaging/#fusion_bench.method.weighted_average.weighted_average.WeightedAverageAlgorithm","title":"<code>WeightedAverageAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>SimpleProfilerMixin</code></p> Source code in <code>fusion_bench/method/weighted_average/weighted_average.py</code> <pre><code>class WeightedAverageAlgorithm(BaseAlgorithm, SimpleProfilerMixin):\n\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"normalize\": \"normalize\",\n        \"weights\": \"weights\",\n    }\n\n    def __init__(\n        self,\n        normalize: bool,\n        weights: List[float],\n        verbose: bool = True,\n        **kwargs,\n    ):\n        self.normalize = normalize\n        self.weights = weights\n        self.verbose = verbose\n        log.disabled = not self.verbose\n        super().__init__(**kwargs)\n\n    @override\n    @torch.no_grad()\n    def run(self, modelpool: BaseModelPool):\n        \"\"\"\n        Fuses the models in the model pool using a weighted average approach.\n\n        Parameters\n            modelpool (ModelPool): The pool of models to be fused.\n\n        Raises\n            ValueError: If the number of weights does not match the number of models in the model pool.\n\n        Returns\n            forward_model (torch.nn.Module): The resulting model after fusion.\n        \"\"\"\n        if not isinstance(modelpool, BaseModelPool):\n            modelpool = BaseModelPool(modelpool)\n\n        log.info(\"Fusing models using weighted average.\")\n        weights = np.asarray(self.weights)\n        if len(weights) != len(modelpool.model_names):\n            raise ValueError(\n                \"Number of weights must match the number of models.,\"\n                f\"but got {len(weights)} weights and {len(modelpool.model_names)} models.\"\n                f\"weights: {weights}, models: {modelpool.model_names}\"\n            )\n        if self.normalize:\n            weights = weights / np.sum(weights)\n        if self.verbose:\n            print(f\"weights: {weights}, normalized: {self.normalize}\")\n\n        sd: Optional[StateDictType] = None\n        forward_model = None\n\n        for model_name, weight in zip(modelpool.model_names, weights):\n            with self.profile(\"load_model\"):\n                model = modelpool.load_model(model_name)\n            with self.profile(\"merge weights\"):\n                if sd is None:\n                    sd = state_dict_mul(model.state_dict(keep_vars=True), weight)\n                    forward_model = model\n                else:\n                    sd = state_dict_add(\n                        sd, state_dict_mul(model.state_dict(keep_vars=True), weight)\n                    )\n\n        forward_model.load_state_dict(sd)\n        if self.verbose:\n            self.print_profile_summary()\n        return forward_model\n</code></pre>"},{"location":"algorithms/weighted_averaging/#fusion_bench.method.weighted_average.weighted_average.WeightedAverageAlgorithm._config_mapping","title":"<code>_config_mapping = BaseAlgorithm._config_mapping | {'normalize': 'normalize', 'weights': 'weights'}</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"algorithms/weighted_averaging/#fusion_bench.method.weighted_average.weighted_average.WeightedAverageAlgorithm.normalize","title":"<code>normalize = normalize</code>  <code>instance-attribute</code>","text":""},{"location":"algorithms/weighted_averaging/#fusion_bench.method.weighted_average.weighted_average.WeightedAverageAlgorithm.verbose","title":"<code>verbose = verbose</code>  <code>instance-attribute</code>","text":""},{"location":"algorithms/weighted_averaging/#fusion_bench.method.weighted_average.weighted_average.WeightedAverageAlgorithm.weights","title":"<code>weights = weights</code>  <code>instance-attribute</code>","text":""},{"location":"algorithms/weighted_averaging/#fusion_bench.method.weighted_average.weighted_average.WeightedAverageAlgorithm.__init__","title":"<code>__init__(normalize, weights, verbose=True, **kwargs)</code>","text":"Source code in <code>fusion_bench/method/weighted_average/weighted_average.py</code> <pre><code>def __init__(\n    self,\n    normalize: bool,\n    weights: List[float],\n    verbose: bool = True,\n    **kwargs,\n):\n    self.normalize = normalize\n    self.weights = weights\n    self.verbose = verbose\n    log.disabled = not self.verbose\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"algorithms/weighted_averaging/#fusion_bench.method.weighted_average.weighted_average.WeightedAverageAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Fuses the models in the model pool using a weighted average approach.</p> <p>Parameters     modelpool (ModelPool): The pool of models to be fused.</p> <p>Raises     ValueError: If the number of weights does not match the number of models in the model pool.</p> <p>Returns     forward_model (torch.nn.Module): The resulting model after fusion.</p> Source code in <code>fusion_bench/method/weighted_average/weighted_average.py</code> <pre><code>@override\n@torch.no_grad()\ndef run(self, modelpool: BaseModelPool):\n    \"\"\"\n    Fuses the models in the model pool using a weighted average approach.\n\n    Parameters\n        modelpool (ModelPool): The pool of models to be fused.\n\n    Raises\n        ValueError: If the number of weights does not match the number of models in the model pool.\n\n    Returns\n        forward_model (torch.nn.Module): The resulting model after fusion.\n    \"\"\"\n    if not isinstance(modelpool, BaseModelPool):\n        modelpool = BaseModelPool(modelpool)\n\n    log.info(\"Fusing models using weighted average.\")\n    weights = np.asarray(self.weights)\n    if len(weights) != len(modelpool.model_names):\n        raise ValueError(\n            \"Number of weights must match the number of models.,\"\n            f\"but got {len(weights)} weights and {len(modelpool.model_names)} models.\"\n            f\"weights: {weights}, models: {modelpool.model_names}\"\n        )\n    if self.normalize:\n        weights = weights / np.sum(weights)\n    if self.verbose:\n        print(f\"weights: {weights}, normalized: {self.normalize}\")\n\n    sd: Optional[StateDictType] = None\n    forward_model = None\n\n    for model_name, weight in zip(modelpool.model_names, weights):\n        with self.profile(\"load_model\"):\n            model = modelpool.load_model(model_name)\n        with self.profile(\"merge weights\"):\n            if sd is None:\n                sd = state_dict_mul(model.state_dict(keep_vars=True), weight)\n                forward_model = model\n            else:\n                sd = state_dict_add(\n                    sd, state_dict_mul(model.state_dict(keep_vars=True), weight)\n                )\n\n    forward_model.load_state_dict(sd)\n    if self.verbose:\n        self.print_profile_summary()\n    return forward_model\n</code></pre>"},{"location":"algorithms/weighted_averaging/#fusion_bench.method.weighted_average.llama.WeightedAverageForLLama","title":"<code>WeightedAverageForLLama</code>","text":"<p>               Bases: <code>BaseAlgorithm</code></p> <p>A class to perform weighted averaging of LlaMa/Mistral models.</p> Source code in <code>fusion_bench/method/weighted_average/llama.py</code> <pre><code>class WeightedAverageForLLama(BaseAlgorithm):\n    \"\"\"\n    A class to perform weighted averaging of LlaMa/Mistral models.\n    \"\"\"\n\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"normalize\": \"normalize\",\n        \"weights\": \"weights\",\n        \"backbone_only\": \"backbone_only\",\n        \"merged_model_save_path\": \"merged_model_save_path\",\n        \"save_tokenizer\": \"save_tokenizer\",\n        \"push_to_hub\": \"push_to_hub\",\n    }\n\n    def __init__(\n        self,\n        normalize: bool,\n        weights: List[float],\n        backbone_only: bool,\n        merged_model_save_path: str,\n        save_tokenizer: bool,\n        push_to_hub: bool,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the WeightedAverageForLLama class with the given parameters.\n\n        Args:\n            normalize (bool): Whether to normalize the weights.\n            weights (List[float]): The weights for averaging the models.\n            backbone_only (bool): Whether to use only the backbone of the models.\n            merged_model_save_path (str): The path to save the merged model.\n            save_tokenizer (bool): Whether to save the tokenizer.\n            push_to_hub (bool): Whether to push the model to the hub.\n        \"\"\"\n        self.normalize = normalize\n        self.weights = weights\n        self.backbone_only = backbone_only\n        self.merged_model_save_path = merged_model_save_path\n        self.save_tokenizer = save_tokenizer\n        self.push_to_hub = push_to_hub\n        super().__init__(**kwargs)\n\n    @override\n    @torch.no_grad()\n    def run(self, modelpool: CausalLMPool):\n        \"\"\"\n        Executes the weighted averaging of models in the provided model pool.\n\n        Args:\n            modelpool (LLamaForCausalLMPoolThe):  pool of models to be averaged.\n\n        Returns:\n            base_model: The base model after merging the state dictionaries of the models in the pool.\n\n        Raises:\n            ValueError: If the number of weights does not match the number of models in the pool.\n        \"\"\"\n        if modelpool.has_pretrained:\n            base_model = modelpool.load_model(\"_pretrained_\")\n        else:\n            base_model = modelpool.load_model(modelpool.model_names[0])\n\n        weights = self.weights\n        if len(weights) != len(modelpool.model_names):\n            raise ValueError(\n                \"Number of weights must match the number of models.,\"\n                f\"but got {len(weights)} weights and {len(modelpool.model_names)} models.\"\n                f\"weights: {weights}, models: {modelpool.model_names}\"\n            )\n        if self.normalize:\n            weights = np.asarray(weights)\n            weights = weights / np.sum(weights)\n\n        merged_state_dict: StateDictType = None\n        for model_name, weight in zip(modelpool.model_names, weights):\n            model = modelpool.load_model(model_name, backbone_only=self.backbone_only)\n            sd = state_dict_mul(model.state_dict(), weight)\n            if merged_state_dict is None:\n                merged_state_dict = sd\n            else:\n                merged_state_dict = state_dict_add(merged_state_dict, sd)\n\n        base_model.load_state_dict(\n            merged_state_dict, strict=False if self.backbone_only else True\n        )\n        if self.merged_model_save_path is not None:\n            with timeit_context(\n                f\"Saving the merged model to {self.merged_model_save_path}\"\n            ):\n                modelpool.save_model(\n                    base_model,\n                    path=self.merged_model_save_path,\n                    save_tokenizer=self.save_tokenizer,\n                    push_to_hub=self.push_to_hub,\n                )\n        return base_model\n</code></pre>"},{"location":"algorithms/weighted_averaging/#fusion_bench.method.weighted_average.llama.WeightedAverageForLLama.__init__","title":"<code>__init__(normalize, weights, backbone_only, merged_model_save_path, save_tokenizer, push_to_hub, **kwargs)</code>","text":"<p>Initialize the WeightedAverageForLLama class with the given parameters.</p> <p>Parameters:</p> Source code in <code>fusion_bench/method/weighted_average/llama.py</code> <pre><code>def __init__(\n    self,\n    normalize: bool,\n    weights: List[float],\n    backbone_only: bool,\n    merged_model_save_path: str,\n    save_tokenizer: bool,\n    push_to_hub: bool,\n    **kwargs,\n):\n    \"\"\"\n    Initialize the WeightedAverageForLLama class with the given parameters.\n\n    Args:\n        normalize (bool): Whether to normalize the weights.\n        weights (List[float]): The weights for averaging the models.\n        backbone_only (bool): Whether to use only the backbone of the models.\n        merged_model_save_path (str): The path to save the merged model.\n        save_tokenizer (bool): Whether to save the tokenizer.\n        push_to_hub (bool): Whether to push the model to the hub.\n    \"\"\"\n    self.normalize = normalize\n    self.weights = weights\n    self.backbone_only = backbone_only\n    self.merged_model_save_path = merged_model_save_path\n    self.save_tokenizer = save_tokenizer\n    self.push_to_hub = push_to_hub\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"algorithms/weighted_averaging/#fusion_bench.method.weighted_average.llama.WeightedAverageForLLama.__init__(normalize)","title":"<code>normalize</code>","text":"(<code>bool</code>)           \u2013            <p>Whether to normalize the weights.</p>"},{"location":"algorithms/weighted_averaging/#fusion_bench.method.weighted_average.llama.WeightedAverageForLLama.__init__(weights)","title":"<code>weights</code>","text":"(<code>List[float]</code>)           \u2013            <p>The weights for averaging the models.</p>"},{"location":"algorithms/weighted_averaging/#fusion_bench.method.weighted_average.llama.WeightedAverageForLLama.__init__(backbone_only)","title":"<code>backbone_only</code>","text":"(<code>bool</code>)           \u2013            <p>Whether to use only the backbone of the models.</p>"},{"location":"algorithms/weighted_averaging/#fusion_bench.method.weighted_average.llama.WeightedAverageForLLama.__init__(merged_model_save_path)","title":"<code>merged_model_save_path</code>","text":"(<code>str</code>)           \u2013            <p>The path to save the merged model.</p>"},{"location":"algorithms/weighted_averaging/#fusion_bench.method.weighted_average.llama.WeightedAverageForLLama.__init__(save_tokenizer)","title":"<code>save_tokenizer</code>","text":"(<code>bool</code>)           \u2013            <p>Whether to save the tokenizer.</p>"},{"location":"algorithms/weighted_averaging/#fusion_bench.method.weighted_average.llama.WeightedAverageForLLama.__init__(push_to_hub)","title":"<code>push_to_hub</code>","text":"(<code>bool</code>)           \u2013            <p>Whether to push the model to the hub.</p>"},{"location":"algorithms/weighted_averaging/#fusion_bench.method.weighted_average.llama.WeightedAverageForLLama.run","title":"<code>run(modelpool)</code>","text":"<p>Executes the weighted averaging of models in the provided model pool.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>base_model</code>          \u2013            <p>The base model after merging the state dictionaries of the models in the pool.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the number of weights does not match the number of models in the pool.</p> </li> </ul> Source code in <code>fusion_bench/method/weighted_average/llama.py</code> <pre><code>@override\n@torch.no_grad()\ndef run(self, modelpool: CausalLMPool):\n    \"\"\"\n    Executes the weighted averaging of models in the provided model pool.\n\n    Args:\n        modelpool (LLamaForCausalLMPoolThe):  pool of models to be averaged.\n\n    Returns:\n        base_model: The base model after merging the state dictionaries of the models in the pool.\n\n    Raises:\n        ValueError: If the number of weights does not match the number of models in the pool.\n    \"\"\"\n    if modelpool.has_pretrained:\n        base_model = modelpool.load_model(\"_pretrained_\")\n    else:\n        base_model = modelpool.load_model(modelpool.model_names[0])\n\n    weights = self.weights\n    if len(weights) != len(modelpool.model_names):\n        raise ValueError(\n            \"Number of weights must match the number of models.,\"\n            f\"but got {len(weights)} weights and {len(modelpool.model_names)} models.\"\n            f\"weights: {weights}, models: {modelpool.model_names}\"\n        )\n    if self.normalize:\n        weights = np.asarray(weights)\n        weights = weights / np.sum(weights)\n\n    merged_state_dict: StateDictType = None\n    for model_name, weight in zip(modelpool.model_names, weights):\n        model = modelpool.load_model(model_name, backbone_only=self.backbone_only)\n        sd = state_dict_mul(model.state_dict(), weight)\n        if merged_state_dict is None:\n            merged_state_dict = sd\n        else:\n            merged_state_dict = state_dict_add(merged_state_dict, sd)\n\n    base_model.load_state_dict(\n        merged_state_dict, strict=False if self.backbone_only else True\n    )\n    if self.merged_model_save_path is not None:\n        with timeit_context(\n            f\"Saving the merged model to {self.merged_model_save_path}\"\n        ):\n            modelpool.save_model(\n                base_model,\n                path=self.merged_model_save_path,\n                save_tokenizer=self.save_tokenizer,\n                push_to_hub=self.push_to_hub,\n            )\n    return base_model\n</code></pre>"},{"location":"algorithms/weighted_averaging/#fusion_bench.method.weighted_average.llama.WeightedAverageForLLama.run(modelpool)","title":"<code>modelpool</code>","text":"(<code>LLamaForCausalLMPoolThe</code>)           \u2013            <p>pool of models to be averaged.</p>"},{"location":"algorithms/weighted_ensemble/","title":"Weighted Ensemble","text":"<p>A weighted ensemble is a machine learning technique that combines the predictions of multiple models to produce a final prediction. The idea is to leverage the strengths of each individual model to improve overall performance and robustness.</p> <p>Formally, a weighted ensemble can be defined as follows:</p> <p>Given a set of \\(n\\) models, each model \\(f_i\\) produces a prediction \\(f_i(x)\\) for an input \\(x\\). Each model \\(i\\) also has an associated weight \\(w_i\\). The final prediction \\(F(x)\\) of the weighted ensemble is a weighted sum of the individual model predictions:</p> \\[ F(x) = w_1 f_1(x) + w_2 f_2(x) + ... + w_n f_n(x) \\] <p>The weights \\(w_i\\) are typically non-negative and sum to 1 (i.e., \\(\\sum_{i=1}^n w_i = 1\\)), which ensures that the final prediction is a convex combination of the individual model predictions. The weights can be determined in various ways. They could be set based on the performance of the models on a validation set, or they could be learned as part of the training process. In some cases, all models might be given equal weight. The goal of a weighted ensemble is to produce a final prediction that is more accurate or robust than any individual model. This is particularly useful when the individual models have complementary strengths and weaknesses.</p>"},{"location":"algorithms/weighted_ensemble/#examples","title":"Examples","text":"<p>The following Python code snippet demonstrates how to use the <code>WeightedEnsembleAlgorithm</code> class from the <code>fusion_bench.method</code> module to create a weighted ensemble of PyTorch models.</p> <pre><code>from omegaconf import DictConfig\nfrom fusion_bench.method import WeightedEnsembleAlgorithm\n\n#Instantiate the algorithm\nmethod_config = {'name': 'weighted_ensemble', 'weights': [0.3, 0.7]}\nalgorithm = WeightedEnsembleAlgorithm(DictConfig(method_config))\n\n# Assume we have a list of PyTorch models (nn.Module instances) that we want to ensemble.\nmodels = [...]\n\n# Run the algorithm on the models.\nmerged_model = algorithm.run(models)\n</code></pre> <p>Here's a step-by-step explanation:</p> <ol> <li> <p>Instantiate the <code>WeightedEnsembleAlgorithm</code>:</p> <ul> <li>A dictionary <code>method_config</code> is created with two keys: <code>'name'</code> and <code>'weights'</code>. The <code>'name'</code> key is set to <code>'weighted_ensemble'</code> indicating the type of ensemble method to use. The <code>'weights'</code> key is set to a list of weights <code>[0.3, 0.7]</code> indicating the weights assigned to each model in the ensemble.</li> <li>The <code>method_config</code> dictionary is converted to a <code>DictConfig</code> object, which is a configuration object used by the <code>omegaconf</code> library.</li> <li>The <code>WeightedEnsembleAlgorithm</code> is then instantiated with the <code>DictConfig</code> object as an argument.</li> </ul> </li> <li> <p>Assume a list of PyTorch models that you want to ensemble. This list is assigned to the variable <code>models</code>. The actual models are not shown in this code snippet.</p> </li> <li> <p>Run the algorithm on the models: The <code>run</code> method of the <code>WeightedEnsembleAlgorithm</code> instance is called with the <code>models</code> list as an argument. The result is a merged model that represents the weighted ensemble of the input models. This merged model is assigned to the variable <code>merged_model</code>.</p> </li> </ol> <p>Here we list the options for the weighted ensemble algorithm:</p> Option Default Description <code>weights</code> A list of floats representing the weights for each model in the ensemble. <code>normalize</code> <code>True</code> Whether to normalize the weights so that they sum to 1. Default is <code>True</code>. <p>if <code>normalize</code> is set to <code>True</code>, the weights will be normalized so that they sum to 1.  Mathematically, this means that the weights \\(w_i\\) will be divided by the sum of all weights, so that</p> \\[ F(x) = \\frac{w_1}{\\sum_{i=1}^n w_i} f_1(x) + \\frac{w_2}{\\sum_{i=1}^n w_i} f_2(x) + ... + \\frac{w_n}{\\sum_{i=1}^n w_i} f_n(x) \\]"},{"location":"algorithms/weighted_ensemble/#code-intergration","title":"Code Intergration","text":"<p>Configuration template for the weighted ensemble algorithm:</p> config/method.weighted_ensemble.yaml<pre><code>name: weighted_ensemble\n\n# this should be a list of floats, one for each model in the ensemble\n# If weights is null, the ensemble will use the default weights, which are equal weights for all models.\nweights: null\nnomalize: true\n</code></pre> <p>Construct a weighted ensemble using our CLI tool <code>fusion_bench</code>:</p> <pre><code>fusion_bench method=weighted_ensemble \\\n    method.weights=[0.3, 0.7] \\\n  modelpool=... \\\n  taskpool=...\n</code></pre>"},{"location":"algorithms/weighted_ensemble/#references","title":"References","text":""},{"location":"algorithms/weighted_ensemble/#fusion_bench.method.WeightedEnsembleAlgorithm","title":"<code>WeightedEnsembleAlgorithm</code>","text":"<p>               Bases: <code>BaseAlgorithm</code></p> Source code in <code>fusion_bench/method/ensemble.py</code> <pre><code>class WeightedEnsembleAlgorithm(BaseAlgorithm):\n\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"normalize\": \"normalize\",\n        \"weights\": \"weights\",\n    }\n\n    def __init__(self, normalize: bool, weights: List[float], **kwargs):\n        self.normalize = normalize\n        self.weights = weights\n        super().__init__(**kwargs)\n\n    @torch.no_grad()\n    def run(self, modelpool: BaseModelPool | List[nn.Module]):\n        \"\"\"\n        Run the weighted ensemble algorithm on the given model pool.\n\n        Args:\n            modelpool (BaseModelPool | List[nn.Module]): The pool of models to ensemble.\n\n        Returns:\n            WeightedEnsembleModule: The weighted ensembled model.\n        \"\"\"\n        if not isinstance(modelpool, BaseModelPool):\n            modelpool = BaseModelPool(models=modelpool)\n\n        log.info(f\"Running weighted ensemble algorithm with {len(modelpool)} models\")\n\n        models = [modelpool.load_model(m) for m in modelpool.model_names]\n        if self.weights is None:\n            weights = np.ones(len(models)) / len(models)\n        else:\n            weights = self.weights\n        ensemble = WeightedEnsembleModule(\n            models,\n            weights=weights,\n            normalize=self.config.get(\"normalize\", True),\n        )\n        return ensemble\n</code></pre>"},{"location":"algorithms/weighted_ensemble/#fusion_bench.method.WeightedEnsembleAlgorithm.run","title":"<code>run(modelpool)</code>","text":"<p>Run the weighted ensemble algorithm on the given model pool.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>WeightedEnsembleModule</code>          \u2013            <p>The weighted ensembled model.</p> </li> </ul> Source code in <code>fusion_bench/method/ensemble.py</code> <pre><code>@torch.no_grad()\ndef run(self, modelpool: BaseModelPool | List[nn.Module]):\n    \"\"\"\n    Run the weighted ensemble algorithm on the given model pool.\n\n    Args:\n        modelpool (BaseModelPool | List[nn.Module]): The pool of models to ensemble.\n\n    Returns:\n        WeightedEnsembleModule: The weighted ensembled model.\n    \"\"\"\n    if not isinstance(modelpool, BaseModelPool):\n        modelpool = BaseModelPool(models=modelpool)\n\n    log.info(f\"Running weighted ensemble algorithm with {len(modelpool)} models\")\n\n    models = [modelpool.load_model(m) for m in modelpool.model_names]\n    if self.weights is None:\n        weights = np.ones(len(models)) / len(models)\n    else:\n        weights = self.weights\n    ensemble = WeightedEnsembleModule(\n        models,\n        weights=weights,\n        normalize=self.config.get(\"normalize\", True),\n    )\n    return ensemble\n</code></pre>"},{"location":"algorithms/weighted_ensemble/#fusion_bench.method.WeightedEnsembleAlgorithm.run(modelpool)","title":"<code>modelpool</code>","text":"(<code>BaseModelPool | List[Module]</code>)           \u2013            <p>The pool of models to ensemble.</p>"},{"location":"algorithms/pruning/magnitude_pruning/","title":"Magnitude Pruning","text":""},{"location":"algorithms/pruning/magnitude_pruning/#examples","title":"Examples","text":""},{"location":"algorithms/pruning/magnitude_pruning/#pruning-a-llama-model","title":"Pruning a Llama Model","text":""},{"location":"algorithms/pruning/magnitude_pruning/#unstructured-magnitude-pruning","title":"Unstructured Magnitude Pruning","text":"<p>The following command prunes a Llama model with a sparsity ratio of 0.7 (70% of the weights are pruned) using unstructured magnitude pruning. The pruned model is saved to <code>outputs/llama/magnitude_pruning/unstructured/0.7</code>.</p> <pre><code>fusion_bench \\\n    --config-name llama_magnitude_pruning \\\n    method.prune_type=unstructured \\\n    method.sparsity_ratio=0.7 \\\n    modelpool.models.0.path=decapoda-research/llama-7b-hf \\\n    merged_model_save_path=outputs/llama/magnitude_pruning/unstructured/0.7\n</code></pre>"},{"location":"algorithms/pruning/magnitude_pruning/#semi-structured-magnitude-pruning","title":"Semi-Structured Magnitude Pruning","text":"<p>The following command prunes a Llama model with a 2:4 semi-structured pruning ratio using magnitude pruning. The pruned model is saved to <code>outputs/llama/magnitude_pruning/semistructure/2_4</code>.</p> <pre><code>fusion_bench \\\n    --config-name llama_magnitude_pruning \\\n    method.prune_type=semistructured \\\n    method.n=2 method.m=4 \\\n    modelpool.models.0.path=decapoda-research/llama-7b-hf \\\n    merged_model_save_path=outputs/llama/magnitude_pruning/semistructure/2_4\n</code></pre> <p>Below is an example of how to visualize the pruned weights of the first layer of the pruned model.</p> <pre><code>from transformers import AutoModelForCausalLM\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\n\n# Load the pruned model\nmodel = AutoModelForCausalLM.from_pretrained(\"outputs/llama/magnitude_pruning/semistructure/2_4\")\n\n# Extract the tensor data\ntensor_data = model.model.layers[0].self_attn.q_proj.weight[:32, :32]\n\n# Convert to NumPy array\ntensor_data_np = tensor_data.detach().cpu().numpy()\n\n# Plot heatmap\nplt.figure(figsize=(10, 8))\nax = sns.heatmap(tensor_data_np, center=0, cmap=\"coolwarm\", annot=False)\n\n# Add grid lines for 4x4 cells\nfor i in range(0, tensor_data_np.shape[0], 4):\n    ax.axhline(i, color=\"black\", linewidth=0.5)\n    ax.axvline(i, color=\"black\", linewidth=0.5)\n\nplt.title(\"Heatmap of q_proj.weight[:32, :32]\")\nplt.show()\n</code></pre> <p>The following image shows the pruned weights of the first layer of the pruned model.</p> <p></p>"},{"location":"algorithms/pruning/magnitude_pruning/#references","title":"References","text":""},{"location":"algorithms/pruning/magnitude_pruning/#fusion_bench.method.pruning.llama_magnitude_prune.MagnitudePruningForLlama","title":"<code>MagnitudePruningForLlama</code>","text":"<p>               Bases: <code>BaseAlgorithm</code>, <code>SimpleProfilerMixin</code></p> <p>Implements magnitude-based pruning for LLama models.</p> <p>This class supports both unstructured and semistructured pruning methods. It loads a pre-trained model or the first model in the pool and applies the specified pruning technique.</p> <p>Methods:</p> <ul> <li> <code>run</code>             \u2013              <p>LLamaForCausalLMPool) -&gt; nn.Module: Executes the pruning process on the model pool and returns the pruned model.</p> </li> </ul> Source code in <code>fusion_bench/method/pruning/llama_magnitude_prune.py</code> <pre><code>class MagnitudePruningForLlama(BaseAlgorithm, SimpleProfilerMixin):\n    \"\"\"\n    Implements magnitude-based pruning for LLama models.\n\n    This class supports both unstructured and semistructured pruning methods.\n    It loads a pre-trained model or the first model in the pool and applies the specified pruning technique.\n\n    Methods:\n        run(modelpool: LLamaForCausalLMPool) -&gt; nn.Module:\n            Executes the pruning process on the model pool and returns the pruned model.\n    \"\"\"\n\n    _config_mapping = BaseAlgorithm._config_mapping | {\n        \"prune_type\": \"prune_type\",\n        \"device\": \"device\",\n        \"dtype\": \"dtype\",\n        \"sparsity_ratio\": \"sparsity_ratio\",\n        \"n\": \"n\",\n        \"m\": \"m\",\n    }\n\n    def __init__(\n        self,\n        *,\n        prune_type: Literal[\"unstructured\", \"semistructured\"],\n        device: str,\n        dtype: Optional[str],\n        sparsity_ratio: float,\n        n: int,\n        m: int,\n        **kwargs,\n    ):\n        self.prune_type = prune_type\n        self.device = device\n        self.dtype = dtype\n        self.sparsity_ratio = sparsity_ratio\n        self.n = n\n        self.m = m\n        super().__init__(**kwargs)\n\n    @torch.no_grad()\n    def run(self, modelpool: CausalLMPool):\n        \"\"\"\n        Execute the pruning process on the first model from the given model pool.\n\n        Args:\n            modelpool (CausalLMPool): The model pool containing the models to prune.\n\n        Returns:\n            nn.Module: The pruned model.\n        \"\"\"\n        config = self.config\n\n        # load pre-trained model or the first model in the pool\n        base_model = modelpool.load_pretrained_or_first_model()\n\n        dtype = parse_dtype(config.dtype)\n        device = torch.device(config.device)\n\n        if config.prune_type == \"unstructured\":\n            unstructured_magnitude_prune_(\n                base_model, config.sparsity_ratio, dtype=dtype, device=device\n            )\n        elif config.prune_type == \"semistructured\":\n            semistructured_magnitude_prune_(\n                base_model, config.n, config.m, dtype=dtype, device=device\n            )\n        else:\n            raise ValueError(\n                f\"Invalid pruning type: {config.prune_type}\"\n                \"Choose from 'unstructured' or 'semistructured'\"\n            )\n\n        return base_model\n</code></pre>"},{"location":"algorithms/pruning/magnitude_pruning/#fusion_bench.method.pruning.llama_magnitude_prune.MagnitudePruningForLlama.run","title":"<code>run(modelpool)</code>","text":"<p>Execute the pruning process on the first model from the given model pool.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li>           \u2013            <p>nn.Module: The pruned model.</p> </li> </ul> Source code in <code>fusion_bench/method/pruning/llama_magnitude_prune.py</code> <pre><code>@torch.no_grad()\ndef run(self, modelpool: CausalLMPool):\n    \"\"\"\n    Execute the pruning process on the first model from the given model pool.\n\n    Args:\n        modelpool (CausalLMPool): The model pool containing the models to prune.\n\n    Returns:\n        nn.Module: The pruned model.\n    \"\"\"\n    config = self.config\n\n    # load pre-trained model or the first model in the pool\n    base_model = modelpool.load_pretrained_or_first_model()\n\n    dtype = parse_dtype(config.dtype)\n    device = torch.device(config.device)\n\n    if config.prune_type == \"unstructured\":\n        unstructured_magnitude_prune_(\n            base_model, config.sparsity_ratio, dtype=dtype, device=device\n        )\n    elif config.prune_type == \"semistructured\":\n        semistructured_magnitude_prune_(\n            base_model, config.n, config.m, dtype=dtype, device=device\n        )\n    else:\n        raise ValueError(\n            f\"Invalid pruning type: {config.prune_type}\"\n            \"Choose from 'unstructured' or 'semistructured'\"\n        )\n\n    return base_model\n</code></pre>"},{"location":"algorithms/pruning/magnitude_pruning/#fusion_bench.method.pruning.llama_magnitude_prune.MagnitudePruningForLlama.run(modelpool)","title":"<code>modelpool</code>","text":"(<code>CausalLMPool</code>)           \u2013            <p>The model pool containing the models to prune.</p>"},{"location":"cli/fusion_bench/","title":"<code>fusion_bench</code>: The Command Line Interface for FusionBench","text":"<p><code>fusion_bench</code>: FusionBench \u7684\u547d\u4ee4\u884c\u754c\u9762</p> <p><code>fusion_bench</code> is the command line interface for running model fusion benchmarks in the FusionBench project.  It provides a flexible way to configure and execute various fusion algorithms on different model pools and evaluate them across multiple tasks.</p> <p><code>fusion_bench</code> \u662f FusionBench \u9879\u76ee\u4e2d\u7528\u4e8e\u8fd0\u884c\u6a21\u578b\u878d\u5408\u57fa\u51c6\u6d4b\u8bd5\u7684\u547d\u4ee4\u884c\u754c\u9762\u3002 \u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u65b9\u5f0f\u6765\u914d\u7f6e\u548c\u6267\u884c\u5404\u79cd\u878d\u5408\u7b97\u6cd5\uff0c\u5e76\u5728\u4e0d\u540c\u7684\u6a21\u578b\u6c60\u4e0a\u8bc4\u4f30\u5b83\u4eec\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002</p>"},{"location":"cli/fusion_bench/#details-and-options","title":"Details and Options","text":"<p><code>fusion_bench</code> takes a configuration file as input, which specifies the models, fusion method to be used, and the datasets to be evaluated. running <code>fusion_bench</code> is equivalent to running <code>python fusion_bench/scripts/cli.py</code>.</p> <pre><code>fusion_bench [--config-path CONFIG_PATH] [--config-name CONFIG_NAME] \\\n    OPTION_1=VALUE_1 OPTION_2=VALUE_2 ...\n\n# or equivalently\npython fusion_bench/scripts/cli.py [--config-path CONFIG_PATH] [--config-name CONFIG_NAME] \\\n    OPTION_1=VALUE_1 OPTION_2=VALUE_2 ...\n</code></pre> <p><code>fusion_bench</code> has the following options, <code>method</code>, <code>modelpool</code>, and <code>taskpool</code> are the most important ones among these options:</p>"},{"location":"cli/fusion_bench/#hydra-options","title":"Hydra options","text":"<ul> <li>--help, -h: Application's help. Print help message and exit.   <pre><code>fusion_bench --help\n</code></pre></li> <li>--hydra-help: Hydra's help.</li> <li>--version: Show Hydra's version and exit.</li> <li>--cfg, -c: Show config instead of running [job|hydra|all].   This is useful for debugging the configuration. However, this just prints plain text configuration without color highlighting or formatting.   <pre><code>fusion_bench --cfg\n</code></pre>   Or equivalently with the following options:   <pre><code># this will print the configuration using rich library, which provides syntax highlighting and better formatting\nfusion_bench print_config=true dry_run=true\n</code></pre></li> <li>--resolve: Used in conjunction with --cfg, resolve config interpolations before printing.</li> <li>--package, -p: Config package to show. For example, when you only want to see the configuration for <code>method</code>.   <pre><code>fusion_bench --cfg job -p method\n</code></pre></li> <li>--info, -i: Print Hydra information [all|config|defaults|defaults-tree|plugins|searchpath]</li> <li>--config-path, -cp: Overrides the config_path specified in hydra.main(). The config_path is absolute or relative to the Python file declaring @hydra.main(). By default, the config path is the <code>config</code> or <code>fusion_bench_config</code> directory in the project root.</li> <li>--config-name, -cn: Overrides the config_name specified in hydra.main(). By default, the config name is <code>example_config</code> so <code>config/example_config.yaml</code> will be loaded. You can also specify another config name, for example:   <pre><code># this will load the config from `config/llama_weighted_average.yaml`\nfusion_bench --config-name llama_weighted_average.yaml\n</code></pre></li> <li>--config-dir, -cd: Adds an additional config dir to the config search path</li> <li>--multirun, -m: Run multiple jobs with the configured launcher and sweeper. For more information, see Hydra documentation.</li> <li>--experimental-rerun: Rerun a job from a previous config pickle</li> </ul>"},{"location":"cli/fusion_bench/#shell-completion","title":"Shell Completion","text":"<p>This is useful for tab completion in the shell. You can install shell completion for Bash, Fish, and Zsh.</p>  Screenshot of tab completion in the shell.  <ul> <li>--shell-completion, -sc: Install or Uninstall shell completion:</li> <li>Bash - Install:     <pre><code>eval \"$(fusion_bench -sc install=bash)\"\n</code></pre></li> <li>Bash - Uninstall:     <pre><code>eval \"$(fusion_bench -sc uninstall=bash)\"\n</code></pre></li> <li>Fish - Install:     <pre><code>fusion_bench -sc install=fish | source\n</code></pre></li> <li>Fish - Uninstall:     <pre><code>fusion_bench -sc uninstall=fish | source\n</code></pre></li> <li>Zsh - Install:     Zsh is compatible with the Bash shell completion, see the documentation for details.     <pre><code>eval \"$(fusion_bench -sc install=bash)\"\n</code></pre></li> <li>Zsh - Uninstall:     <pre><code>eval \"$(fusion_bench -sc uninstall=bash)\"\n</code></pre></li> </ul>"},{"location":"cli/fusion_bench/#application-options","title":"Application Options","text":"<ul> <li>report_save_path: The path to save the report. If not specified or is <code>false</code>, the report will not be saved. The report will be saved as a JSON file. Default is <code>false</code>.   For example, to save the report to <code>outputs/report.json</code>:   <pre><code>fusion_bench report_save_path=outputs/report.json\n</code></pre></li> <li>print_config: Whether to print the configuration to the console. If not specified or is <code>false</code>, the configuration will not be printed. Default is <code>true</code>.   For example, to print the configuration:   <pre><code>fusion_bench print_config=true\n</code></pre></li> <li>dry_run: Perform a dry run.    This will only validate the configuration without running the actual code. Default is <code>false</code>.   For example, to perform a dry run and print the configuration:   <pre><code>fusion_bench dry_run=true print_config=true\n</code></pre></li> <li> <p>merged_model_save_path: The path to save the merged model. If specified, the merged model will be saved to this path by calling <code>modelpool.save_model</code>.   For example, to save the merged model to <code>outputs/merged_model.pt</code>:   <pre><code>fusion_bench merged_model_save_path=outputs/merged_model.pt\n</code></pre>   Note that the behavior of <code>modelpool.save_model</code> depends on the implementation of the model pool. Take <code>AutoModelForCausalLMPool</code> as an example, it will save the model to the specified path as a dirctory containing the model configuration and safetensor files, i.e., calling <code>model.save_pretrained(merged_model_save_path)</code>.</p> Example of <code>modelpool.save_model</code> <p><code>ModelPool</code> is the base class for model pools. The <code>save_model</code> method is defined in the <code>ModelPool</code> class and can be overridden in the derived classes. For example, <code>AutoModelForCausalLMPool</code> overrides the <code>save_model</code> method to save the model using the <code>save_pretrained</code> method of the model. The following is an example of the <code>save_model</code> method in the <code>ModelPool</code> class and the <code>AutoModelForCausalLMPool</code> class.</p> <p>By passing <code>merged_model_save_path</code> to the <code>fusion_bench</code> command, only the model and the save path will be passed to the <code>modelpool.save_model</code> method. For example, although the <code>AutoModelForCausalLMPool</code> class has a <code>save_model</code> method that can take additional arguments, such as <code>push_to_hub</code> and <code>save_tokenizer</code>, these arguments will not be passed to the <code>save_model</code> method. If you want to pass additional arguments to the <code>save_model</code> method, you need to implement the logic in method class.</p> </li> </ul>"},{"location":"cli/fusion_bench/#fusion_bench.modelpool.CausalLMPool.save_model","title":"<code>fusion_bench.modelpool.CausalLMPool.save_model(model, path, push_to_hub=False, model_dtype=None, save_tokenizer=False, tokenizer_kwargs=None, **kwargs)</code>","text":"<p>Save the model to the specified path.</p> <p>Parameters:</p> Source code in <code>fusion_bench/modelpool/causal_lm/causal_lm.py</code> <pre><code>@override\ndef save_model(\n    self,\n    model: PreTrainedModel,\n    path: str,\n    push_to_hub: bool = False,\n    model_dtype: Optional[str] = None,\n    save_tokenizer: bool = False,\n    tokenizer_kwargs=None,\n    **kwargs,\n):\n    \"\"\"\n    Save the model to the specified path.\n\n    Args:\n        model (PreTrainedModel): The model to be saved.\n        path (str): The path where the model will be saved.\n        push_to_hub (bool, optional): Whether to push the model to the Hugging Face Hub. Defaults to False.\n        save_tokenizer (bool, optional): Whether to save the tokenizer along with the model. Defaults to False.\n        **kwargs: Additional keyword arguments passed to the `save_pretrained` method.\n    \"\"\"\n    path = os.path.expanduser(path)\n    if save_tokenizer:\n        if tokenizer_kwargs is None:\n            tokenizer_kwargs = {}\n        # load the tokenizer\n        tokenizer = self.load_tokenizer(**tokenizer_kwargs)\n        tokenizer.save_pretrained(\n            path,\n            push_to_hub=push_to_hub,\n        )\n    if model_dtype is not None:\n        model.to(dtype=parse_dtype(model_dtype))\n    model.save_pretrained(\n        path,\n        push_to_hub=push_to_hub,\n        **kwargs,\n    )\n</code></pre>"},{"location":"cli/fusion_bench/#fusion_bench.modelpool.CausalLMPool.save_model(model)","title":"<code>model</code>","text":"(<code>PreTrainedModel</code>)           \u2013            <p>The model to be saved.</p>"},{"location":"cli/fusion_bench/#fusion_bench.modelpool.CausalLMPool.save_model(path)","title":"<code>path</code>","text":"(<code>str</code>)           \u2013            <p>The path where the model will be saved.</p>"},{"location":"cli/fusion_bench/#fusion_bench.modelpool.CausalLMPool.save_model(push_to_hub)","title":"<code>push_to_hub</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to push the model to the Hugging Face Hub. Defaults to False.</p>"},{"location":"cli/fusion_bench/#fusion_bench.modelpool.CausalLMPool.save_model(save_tokenizer)","title":"<code>save_tokenizer</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to save the tokenizer along with the model. Defaults to False.</p>"},{"location":"cli/fusion_bench/#fusion_bench.modelpool.CausalLMPool.save_model(**kwargs)","title":"<code>**kwargs</code>","text":"\u2013            <p>Additional keyword arguments passed to the <code>save_pretrained</code> method.</p>"},{"location":"cli/fusion_bench/#method-modelpool-and-taskpool-options","title":"method, modelpool and taskpool options","text":"<p>As mentioned earlier, <code>method</code>, <code>modelpool</code>, and <code>taskpool</code> are the most important options in <code>fusion_bench</code>. The basic usage is as follows:</p> <pre><code>fusion_bench method=&lt;METHOD&gt; modelpool=&lt;MODELPOOL&gt; taskpool=&lt;TASKPOOL&gt;\n</code></pre> <p>To override the default configuration, you can specify additional options as follows:</p> <pre><code>fusion_bench \\\n  method=&lt;METHOD&gt; \\\n    method.&lt;OPTION_1&gt;=&lt;VALUE_1&gt; \\\n      method.&lt;OPTION_1&gt;.&lt;SUBOPTION_1&gt;=&lt;VALUE_1_1&gt; \\\n      method.&lt;OPTION_1&gt;.&lt;SUBOPTION_2&gt;=&lt;VALUE_1_2&gt; \\\n    method.&lt;OPTION_2&gt;=&lt;VALUE_2&gt; \\\n  modelpool=&lt;MODELPOOL&gt; \\\n    ...\n  taskpool=&lt;TASKPOOL&gt; \\\n    ...\n</code></pre> <p>Paremeter Overrides: In the above example, <code>&lt;METHOD&gt;</code>, <code>&lt;MODELPOOL&gt;</code>, and <code>&lt;TASKPOOL&gt;</code> are the names of the method, model pool, and task pool, respectively. <code>&lt;OPTION_1&gt;</code>, <code>&lt;VALUE_1&gt;</code>, <code>&lt;SUBOPTION_1&gt;</code>, <code>&lt;VALUE_1_1&gt;</code>, etc., are the options and values for the method. In particular, the options for the method are prefixed with <code>method.</code>, e.g., <code>method.&lt;OPTION_1&gt;</code>. And the suboptions are prefixed with <code>method.&lt;OPTION_1&gt;.</code>, e.g., <code>method.&lt;OPTION_1&gt;.&lt;SUBOPTION_1&gt;</code>.</p>"},{"location":"cli/fusion_bench/#basic-examples","title":"Basic Examples","text":"<p>merge two CLIP models using task arithmetic:</p> <pre><code>fusion_bench method=task_arithmetic \\\n  modelpool=clip-vit-base-patch32_svhn_and_mnist \\\n  taskpool=clip-vit-base-patch32_svhn_and_mnist\n</code></pre> <p>The overall configuration is as follows:</p> <pre><code>method: # (1)!\n  ...\nmodelpool: # (2)!\n  ...\ntaskpool: # (3)!\n  ...\nfast_dev_run: false\nprint_config: true\nreport_save_path: false\n</code></pre> <ol> <li>Configuration for method, <code>fusion_bench.method.load_algorithm_from_config</code> checks the 'name' attribute of the configuration and returns an instance of the corresponding algorithm.</li> <li>Configuration for model pool, <code>fusion_bench.modelpool.load_modelpool_from_config</code> checks the 'type' attribute of the configuration and returns an instance of the corresponding model pool.</li> <li>Configuration for task pool, <code>fusion_bench.taskpool.load_taskpool_from_config</code> checks the 'type' attribute of the configuration and returns an instance of the corresponding task pool.</li> </ol> <p>merge multiple CLIP models using simple averaging:</p> <pre><code>fusion_bench method=simple_average modelpool=clip-vit-base-patch32_TA8.yaml taskpool=dummy\n</code></pre>"},{"location":"cli/fusion_bench/#running-in-offline-mode","title":"Running in Offline Mode","text":"<p>In the offline mode, the model pool will not download the models from the internet.  Instead, it will use the models that are already downloaded to the local cache.</p> <p>To run <code>fusion_bench</code> in offline mode, you can run the following command before running <code>fusion_bench</code>:</p> <pre><code>source offline_mode.sh\n</code></pre> <p>Or set the environment variable according to the content of <code>offline_mode.sh</code>.</p>"},{"location":"cli/fusion_bench/#debugging-and-troubleshooting","title":"Debugging and Troubleshooting","text":"<p>During algorithm development, you may want to debug the code or inspect the configuration.  Here are some tips for debugging and troubleshooting.</p>"},{"location":"cli/fusion_bench/#debugging-in-vscode","title":"Debugging in VSCode","text":"<p>Visual Studio Code (VSCode) is a popular code editor that supports debugging Python code with Python extension. To debug the code using VSCode, you can use the following configuration in your <code>.vscode/launch.json</code>:</p> <pre><code>{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"FusionBench with Arguments\",\n            \"type\": \"debugpy\",\n            \"request\": \"launch\",\n            \"module\": \"fusion_bench.scripts.cli\", // (1)!\n            \"args\": \"${command:pickArgs}\", // (2)!\n            \"console\": \"integratedTerminal\",\n            \"justMyCode\": true\n        }\n    ]\n}\n</code></pre> <ol> <li>The <code>module</code> field specifies the module to run. In this case, it is <code>fusion_bench.scripts.cli</code>. You can also specify the path to the script directly with <code>program</code> filed, e.g., <code>\"program\": ${workspaceFolder}/fusion_bench/scripts/cli.py</code>.</li> <li>The <code>args</code> field specifies the arguments to pass to the script. You can use <code>${command:pickArgs}</code> to pick the arguments interactively when you run the debugger. Or you can specify the arguments directly, e.g., <code>\"args\": [\"--config-name\", \"example_config\"]</code>.</li> </ol> <p>Once you have the configuration in your <code>launch.json</code>, you can start debugging by selecting the <code>FusionBench with Arguments</code> configuration and pressing <code>F5</code>.</p> <p></p>"},{"location":"cli/fusion_bench/#debugging-in-pycharm","title":"Debugging in PyCharm","text":"<p>Debugging in PyCharm with arguments needs to be configured in the <code>Run/Debug Configurations</code>.</p> <ol> <li>Click on the <code>Run</code> menu click <code>Edit Configurations...</code> </li> <li>Select <code>+</code> in top right corner and select <code>Python</code> </li> <li>Provide the name, absolute path of the script (<code>fusion_bench/scripts/cli.py</code>) or select the script by clicking three dots (green arrow), script parameters, and python interpreter. </li> </ol>"},{"location":"cli/fusion_bench_webui/","title":"FusionBench Command Generator WebUI","text":"<p>FusionBench Command Generator is a user-friendly web interface for generating FusionBench commands based on configuration files.  It provides an interactive way to select and customize FusionBench configurations, making it easier to run experiments with different settings.</p> FusionBench Command Generator WebUI"},{"location":"cli/fusion_bench_webui/#usage","title":"Usage","text":"<p>Run the program with the following command:</p> <pre><code>fusion_bench_webui [OPTIONS]\n</code></pre>"},{"location":"cli/fusion_bench_webui/#options","title":"Options","text":"<ul> <li><code>--config-path PATH</code>: Specify the path to the config directory. If not provided, the default FusionBench config path will be used.</li> <li><code>--print-tree</code>: Print the configuration tree structure before launching the web interface. Default is <code>False</code>.</li> <li><code>--bind-ip IP</code>: Specify the IP address to bind the web UI. Default is <code>127.0.0.1</code>.</li> <li><code>--port PORT</code>: Specify the port to run the web UI. Default is <code>7860</code>.</li> <li><code>--share</code>: Share the web UI. Default is <code>False</code>.</li> </ul> <p>The web interface consists of the following components:</p> <ol> <li>Root Config Dropdown: Select the base configuration file.</li> <li>Configuration Groups: Nested structure of configuration options, allowing you to customize settings for each group.</li> <li>Generated Command: Displays the generated FusionBench command based on your selections.</li> <li>Overall Configuration: Shows the complete configuration in YAML format.</li> </ol>"},{"location":"config/","title":"FusionBench Configuration","text":"<p>This directory contains configuration files for FusionBench.  These configurations are essential for setting up and managing various algorithms and their hyperparameters.</p>"},{"location":"config/#configuration-structure","title":"Configuration Structure","text":"<p>FusionBench employs a modular configuration system, which is divided into three primary groups:</p> <ol> <li>Method Configuration: Defines the fusion algorithm and its associated hyperparameters.</li> <li>Model Pool Configuration: Manages the models involved in the fusion process, including datasets, tokenizers, preprocessors, and other related resources.</li> <li>Task Pool Configuration: Specifies the tasks and their corresponding datasets used for evaluating the fused models.</li> </ol>"},{"location":"config/dataset/image_classification/","title":"Image Classification Dataset Configurations","text":"<p>This folder contains the dataset configuration for image classification tasks.</p> <ul> <li>Each dataset should have 'image' and 'label' columns.</li> <li>If a dataset has no test split, we will use the validation split as the test split and create the validation set from the training set.</li> </ul>"},{"location":"config/model/clip-vit/","title":"Index","text":"<p>This folder contains the configuration for the CLIP-ViT models (managed by <code>fusion_bench.modelpool.CLIPVisionModelPool</code>).</p>"},{"location":"config/model/clip-vit/#expected-configuration","title":"Expected Configuration","text":""},{"location":"config/model/clip-vit/#detailed-configuration","title":"Detailed Configuration","text":"<pre><code>${name_of_model}:\n  _target_: ${function_to_load_model}\n  ... # arguments to pass to the function\n</code></pre> <p>For example, to load the pre-trained CLIP-ViT-B/16 model, you can use the following configuration:</p> <pre><code>_pretrained_: # `_pretrained_` is a special key in FusionBench that indicates the model is pre-trained\n  _target_: transformers.CLIPVisionModel.from_pretrained\n  pretrained_model_name_or_path: openai/clip-vit-base-patch16\n</code></pre> <p>In this case, calling <code>modelpool.load_model(\"_pretrained_\")</code> will return a <code>transformers.CLIPVisionModel</code> instance, which is equivalent to call <code>transformers.CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch16\")</code>.</p> <p>The detailed configuration is more flexible and can be used when you need to pass additional arguments to the <code>from_pretrained</code> function or call custom functions to load and preprocess the model.</p>"},{"location":"config/model/clip-vit/#simplified-configuration","title":"Simplified Configuration","text":"<pre><code>${name_of_model}: ${pretrained_model_name_or_path}\n</code></pre> <p>This is a simplified configuration that is equivalent to the detailed configuration.</p> <p>For example, to load the pre-trained CLIP-ViT-B/16 model, you can use the following configuration:</p> <pre><code>_pretrained_: openai/clip-vit-base-patch16\n</code></pre>"},{"location":"guides/docker/","title":"Build and Run Fusion Benchmarks in a Docker Container","text":""},{"location":"guides/docker/#build-image","title":"Build Image","text":"<p>Build the image using the following command:</p> <pre><code># Using the mirror\ndocker build -t fusion_bench .\n</code></pre> <p>If you want to use the default Docker Hub, you can omit the MIRROR argument or set it to an empty value:</p> <pre><code># Using the default Docker Hub\ndocker build --build-arg MIRROR=docker.io -t fusion_bench .\n</code></pre>"},{"location":"guides/docker/#run-container","title":"Run Container","text":"<p>Test the container using the following command:</p> <pre><code>docker run --gpus all -it --rm fusion_bench\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/","title":"Image Classification with CLIP Models using <code>HFCLIPClassifier</code>","text":""},{"location":"guides/clip_vit/HFCLIPClassifier/#introduction","title":"Introduction","text":"<p>The <code>HFCLIPClassifier</code> class provides a wrapper around the CLIP (Contrastive Language-Image Pre-training) model for image classification tasks. It supports zero-shot learning and can be fine-tuned for specific classification tasks.</p>"},{"location":"guides/clip_vit/HFCLIPClassifier/#basic-steps","title":"Basic Steps","text":""},{"location":"guides/clip_vit/HFCLIPClassifier/#importing-required-modules","title":"Importing Required Modules","text":"<p>First, we need to import the necessary modules for our CLIP-based image classification task:</p> <pre><code>import torch\nfrom transformers import CLIPModel, CLIPProcessor\nfrom fusion_bench.models.hf_clip import HFCLIPClassifier\nfrom torch.utils.data import DataLoader\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#loading-clip-model-and-processor","title":"Loading CLIP Model and Processor","text":"<pre><code>clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#initializing-hfclipclassifier","title":"Initializing HFCLIPClassifier","text":"<pre><code>classifier = HFCLIPClassifier(clip_model, processor)\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#setting-up-the-classification-task","title":"Setting Up the Classification Task","text":"<p>After initializing the classifier, we need to set up the classification task by defining class names and optionally, custom text templates.  The text encoder of CLIP model is used to encode the class names into text embeddings, which are then used to compute the logits for each class.</p> <pre><code>class_names = [\"cat\", \"dog\", \"bird\", \"fish\", \"horse\"]\nclassifier.set_classification_task(class_names)\n</code></pre> <p>By default, <code>set_classification_task</code> uses the following templates:</p> <pre><code>default_templates = [\n    lambda c: f\"a photo of a {c}\",\n]\n</code></pre> <p>You can also use custom templates:</p> <pre><code>custom_templates = [\n    lambda c: f\"a photo of a {c}\",\n    lambda c: f\"an image containing a {c}\",\n]\nclassifier.set_classification_task(class_names, templates=custom_templates)\n</code></pre> <p>Below is the code for <code>set_classification_task</code> and <code>forward</code> method of <code>HFCLIPClassifier</code>:</p>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.models.hf_clip.HFCLIPClassifier","title":"<code>HFCLIPClassifier</code>","text":"<p>               Bases: <code>Module</code></p> <p>A classifier based on the CLIP (Contrastive Language-Image Pre-training) model.</p> <p>This class wraps a CLIP model and provides functionality for image classification using zero-shot learning. It allows setting a classification task with custom class names and text templates.</p> <p>Attributes:</p> <ul> <li> <code>clip_model</code>               (<code>CLIPModel</code>)           \u2013            <p>The underlying CLIP model.</p> </li> <li> <code>processor</code>               (<code>CLIPProcessor</code>)           \u2013            <p>The CLIP processor for preparing inputs.</p> </li> <li> <code>zeroshot_weights</code>               (<code>Tensor</code>)           \u2013            <p>Computed text embeddings for zero-shot classification.</p> </li> <li> <code>classnames</code>               (<code>List[str]</code>)           \u2013            <p>List of class names for the current classification task.</p> </li> <li> <code>templates</code>               (<code>List[Callable[[str], str]]</code>)           \u2013            <p>List of template functions for generating text prompts.</p> </li> </ul> Source code in <code>fusion_bench/models/hf_clip.py</code> <pre><code>class HFCLIPClassifier(nn.Module):\n    \"\"\"\n    A classifier based on the CLIP (Contrastive Language-Image Pre-training) model.\n\n    This class wraps a CLIP model and provides functionality for image classification\n    using zero-shot learning. It allows setting a classification task with custom\n    class names and text templates.\n\n    Attributes:\n        clip_model (CLIPModel): The underlying CLIP model.\n        processor (CLIPProcessor): The CLIP processor for preparing inputs.\n        zeroshot_weights (Tensor): Computed text embeddings for zero-shot classification.\n        classnames (List[str]): List of class names for the current classification task.\n        templates (List[Callable[[str], str]]): List of template functions for generating text prompts.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        clip_model: CLIPModel,\n        processor: CLIPProcessor,\n        extra_module=None,\n    ):\n        \"\"\"\n        Initialize the HFCLIPClassifier.\n\n        Args:\n            clip_model (CLIPModel): The CLIP model to use for classification.\n            processor (CLIPProcessor): The CLIP processor for preparing inputs.\n        \"\"\"\n        super().__init__()\n        # we only fine-tune the vision model\n        clip_model.visual_projection.requires_grad_(False)\n        clip_model.text_model.requires_grad_(False)\n        clip_model.text_projection.requires_grad_(False)\n        clip_model.logit_scale.requires_grad_(False)\n\n        self.clip_model = clip_model\n        self.processor = processor\n        self.register_buffer(\n            \"zeroshot_weights\",\n            None,\n            persistent=False,\n        )\n\n        self.extra_module = extra_module\n\n    @property\n    def text_model(self):\n        \"\"\"Get the text model component of CLIP.\"\"\"\n        return self.clip_model.text_model\n\n    @property\n    def vision_model(self):\n        \"\"\"Get the vision model component of CLIP.\"\"\"\n        return self.clip_model.vision_model\n\n    def set_classification_task(\n        self,\n        classnames: List[str],\n        templates: List[Callable[[str], str]] = default_templates,\n    ):\n        \"\"\"\n        Set up the zero-shot classification task.\n\n        This method computes text embeddings for the given class names using the\n        provided templates. These embeddings are then used for classification.\n\n        Args:\n            classnames (List[str]): List of class names for the classification task.\n            templates (List[Callable[[str], str]], optional): List of template functions\n                for generating text prompts. Defaults to `default_templates`, i.e.\n                [\"a photo of a {classname}\"].\n        \"\"\"\n        processor = self.processor\n\n        self.classnames = classnames\n        self.templates = templates\n\n        with torch.no_grad():\n            zeroshot_weights = []\n            for classname in classnames:\n                text = [template(classname) for template in templates]\n                inputs = processor(text=text, return_tensors=\"pt\", padding=True)\n                inputs = {\n                    k: v.to(get_device(self.text_model)) for k, v in inputs.items()\n                }\n                embeddings = self.text_model(**inputs)[1]\n                embeddings = self.clip_model.text_projection(embeddings)\n\n                # normalize embeddings\n                embeddings = embeddings / embeddings.norm(p=2, dim=-1, keepdim=True)\n\n                embeddings = embeddings.mean(dim=0)\n                embeddings = embeddings / embeddings.norm(p=2, dim=-1, keepdim=True)\n\n                zeroshot_weights.append(embeddings)\n\n            zeroshot_weights = torch.stack(zeroshot_weights, dim=0)\n\n        self.zeroshot_weights = zeroshot_weights\n\n    def forward(\n        self,\n        images: Tensor,\n        return_image_embeds=False,\n        return_dict=False,\n        task_name=None,\n    ):\n        \"\"\"\n        Perform forward pass for zero-shot image classification.\n\n        This method computes image embeddings for the input images and calculates\n        the similarity with the pre-computed text embeddings to produce classification logits.\n\n        Args:\n            images (Tensor): Input images to classify.\n            return_image_embeds (bool): Whether to return the image embeddings.\n            return_dict (bool): Whether to return a dictionary with logits and image embeddings.\n            task_name (Optional[str]): The name of the task.\n\n        Returns:\n            Tensor: Classification logits for each input image.\n\n        Raises:\n            ValueError: If the classification task hasn't been set using set_classification_task.\n        \"\"\"\n        if self.zeroshot_weights is None:\n            raise ValueError(\"Must set classification task before forward pass\")\n        text_embeds = self.zeroshot_weights\n\n        image_embeds = self.get_image_features(images)\n        # normalize embeddings\n        image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n\n        if (\n            hasattr(self.vision_model, \"is_surgery_model\")\n            and self.vision_model.is_surgery_model\n        ):\n            # Dealing with the surgery model, for more details, please refer to:\n            # (ICML 2024) Yang, et.al. Representation Surgery for Multi-Task Model Merging\n            # https://arxiv.org/abs/2402.02705\n            self.vision_model: \"SurgeryModelWrapper\" = self.vision_model\n            image_embeds, _, _ = self.vision_model.compute_surgery_features(\n                image_embeds, dataset_name=task_name\n            )\n\n        # cosine similarity\n        logit_scale = self.clip_model.logit_scale.exp()\n        logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale\n        logits_per_image = logits_per_text.t()\n\n        if return_dict:\n            ret = {\"logits\": logits_per_image}\n            if return_image_embeds:\n                ret.update({\"image_embeds\": image_embeds})\n            return ret\n        else:\n            if return_image_embeds:\n                return logits_per_image, image_embeds\n            else:\n                return logits_per_image\n\n    def get_image_features(self, images: Tensor) -&gt; Tensor:\n        \"\"\"\n        Compute the image embeddings.\n\n        Returns:\n            image_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The image embeddings obtained by\n            applying the projection layer to the pooled output of [`CLIPVisionModel`].\n        \"\"\"\n\n        image_embeds = self.vision_model(images)\n        if isinstance(image_embeds, Tensor):\n            pass\n        elif isinstance(image_embeds, BaseModelOutputWithPooling):\n            image_embeds = image_embeds[1]\n        image_embeds = self.clip_model.visual_projection(image_embeds)\n        return image_embeds\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.models.hf_clip.HFCLIPClassifier.set_classification_task","title":"<code>set_classification_task(classnames, templates=default_templates)</code>","text":"<p>Set up the zero-shot classification task.</p> <p>This method computes text embeddings for the given class names using the provided templates. These embeddings are then used for classification.</p> <p>Parameters:</p> Source code in <code>fusion_bench/models/hf_clip.py</code> <pre><code>def set_classification_task(\n    self,\n    classnames: List[str],\n    templates: List[Callable[[str], str]] = default_templates,\n):\n    \"\"\"\n    Set up the zero-shot classification task.\n\n    This method computes text embeddings for the given class names using the\n    provided templates. These embeddings are then used for classification.\n\n    Args:\n        classnames (List[str]): List of class names for the classification task.\n        templates (List[Callable[[str], str]], optional): List of template functions\n            for generating text prompts. Defaults to `default_templates`, i.e.\n            [\"a photo of a {classname}\"].\n    \"\"\"\n    processor = self.processor\n\n    self.classnames = classnames\n    self.templates = templates\n\n    with torch.no_grad():\n        zeroshot_weights = []\n        for classname in classnames:\n            text = [template(classname) for template in templates]\n            inputs = processor(text=text, return_tensors=\"pt\", padding=True)\n            inputs = {\n                k: v.to(get_device(self.text_model)) for k, v in inputs.items()\n            }\n            embeddings = self.text_model(**inputs)[1]\n            embeddings = self.clip_model.text_projection(embeddings)\n\n            # normalize embeddings\n            embeddings = embeddings / embeddings.norm(p=2, dim=-1, keepdim=True)\n\n            embeddings = embeddings.mean(dim=0)\n            embeddings = embeddings / embeddings.norm(p=2, dim=-1, keepdim=True)\n\n            zeroshot_weights.append(embeddings)\n\n        zeroshot_weights = torch.stack(zeroshot_weights, dim=0)\n\n    self.zeroshot_weights = zeroshot_weights\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.models.hf_clip.HFCLIPClassifier.set_classification_task(classnames)","title":"<code>classnames</code>","text":"(<code>List[str]</code>)           \u2013            <p>List of class names for the classification task.</p>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.models.hf_clip.HFCLIPClassifier.set_classification_task(templates)","title":"<code>templates</code>","text":"(<code>List[Callable[[str], str]]</code>, default:                   <code>default_templates</code> )           \u2013            <p>List of template functions for generating text prompts. Defaults to <code>default_templates</code>, i.e. [\"a photo of a {classname}\"].</p>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.models.hf_clip.HFCLIPClassifier.forward","title":"<code>forward(images, return_image_embeds=False, return_dict=False, task_name=None)</code>","text":"<p>Perform forward pass for zero-shot image classification.</p> <p>This method computes image embeddings for the input images and calculates the similarity with the pre-computed text embeddings to produce classification logits.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Tensor</code>          \u2013            <p>Classification logits for each input image.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the classification task hasn't been set using set_classification_task.</p> </li> </ul> Source code in <code>fusion_bench/models/hf_clip.py</code> <pre><code>def forward(\n    self,\n    images: Tensor,\n    return_image_embeds=False,\n    return_dict=False,\n    task_name=None,\n):\n    \"\"\"\n    Perform forward pass for zero-shot image classification.\n\n    This method computes image embeddings for the input images and calculates\n    the similarity with the pre-computed text embeddings to produce classification logits.\n\n    Args:\n        images (Tensor): Input images to classify.\n        return_image_embeds (bool): Whether to return the image embeddings.\n        return_dict (bool): Whether to return a dictionary with logits and image embeddings.\n        task_name (Optional[str]): The name of the task.\n\n    Returns:\n        Tensor: Classification logits for each input image.\n\n    Raises:\n        ValueError: If the classification task hasn't been set using set_classification_task.\n    \"\"\"\n    if self.zeroshot_weights is None:\n        raise ValueError(\"Must set classification task before forward pass\")\n    text_embeds = self.zeroshot_weights\n\n    image_embeds = self.get_image_features(images)\n    # normalize embeddings\n    image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n\n    if (\n        hasattr(self.vision_model, \"is_surgery_model\")\n        and self.vision_model.is_surgery_model\n    ):\n        # Dealing with the surgery model, for more details, please refer to:\n        # (ICML 2024) Yang, et.al. Representation Surgery for Multi-Task Model Merging\n        # https://arxiv.org/abs/2402.02705\n        self.vision_model: \"SurgeryModelWrapper\" = self.vision_model\n        image_embeds, _, _ = self.vision_model.compute_surgery_features(\n            image_embeds, dataset_name=task_name\n        )\n\n    # cosine similarity\n    logit_scale = self.clip_model.logit_scale.exp()\n    logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale\n    logits_per_image = logits_per_text.t()\n\n    if return_dict:\n        ret = {\"logits\": logits_per_image}\n        if return_image_embeds:\n            ret.update({\"image_embeds\": image_embeds})\n        return ret\n    else:\n        if return_image_embeds:\n            return logits_per_image, image_embeds\n        else:\n            return logits_per_image\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.models.hf_clip.HFCLIPClassifier.forward(images)","title":"<code>images</code>","text":"(<code>Tensor</code>)           \u2013            <p>Input images to classify.</p>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.models.hf_clip.HFCLIPClassifier.forward(return_image_embeds)","title":"<code>return_image_embeds</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the image embeddings.</p>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.models.hf_clip.HFCLIPClassifier.forward(return_dict)","title":"<code>return_dict</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return a dictionary with logits and image embeddings.</p>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.models.hf_clip.HFCLIPClassifier.forward(task_name)","title":"<code>task_name</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The name of the task.</p>"},{"location":"guides/clip_vit/HFCLIPClassifier/#preparing-your-dataset","title":"Preparing Your Dataset","text":"<p>Create a custom dataset class that loads and preprocesses your images:</p> <pre><code>from torchvision import transforms\nfrom PIL import Image\n\nclass SimpleDataset(torch.utils.data.Dataset):\n    def __init__(self, image_paths: List[str], labels: List[int]):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n        ])\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.image_paths[idx]).convert('RGB')\n        image = self.transform(image)\n        return image, self.labels[idx]\n\n# Create DataLoader\ndataset = SimpleDataset(image_paths, labels)  # Replace with your data\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n</code></pre> <p>You can also use <code>fusion_bench.dataset.clip_dataset.CLIPDataset</code> or <code>fusion_bench.dataset.image_dataset.TransformedImageDataset</code> to prepare your dataset. Here is examples of using <code>fusion_bench.dataset.clip_dataset.CLIPDataset</code> and <code>fusion_bench.dataset.image_dataset.TransformedImageDataset</code> to prepare your dataset:</p> <pre><code>from fusion_bench.dataset.clip_dataset import CLIPDataset\n\ndataset = CLIPDataset(dataset, processor)\n</code></pre> <pre><code>from fusion_bench.dataset.image_dataset import TransformedImageDataset\n\ndataset = TransformedImageDataset(dataset, transform)\n</code></pre> <p>Where <code>dataset</code> is your original dataset and <code>transform</code> is the transform you want to apply to the images. Below is the reference code for these two classes:</p>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.dataset.clip_dataset.CLIPDataset","title":"<code>CLIPDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A dataset class for CLIP models that converts a dataset of dictionaries or tuples into a format suitable for CLIP processing.</p> <p>This class wraps an existing dataset and applies CLIP preprocessing to the images. It expects each item in the dataset to be either a dictionary with 'image' and 'label' keys, or a tuple/list of (image, label).</p> <p>Parameters:</p> <p>Attributes:</p> <ul> <li> <code>dataset</code>           \u2013            <p>The wrapped dataset.</p> </li> <li> <code>processor</code>               (<code>CLIPProcessor</code>)           \u2013            <p>The CLIP processor used for image preprocessing.</p> </li> </ul> Source code in <code>fusion_bench/dataset/clip_dataset.py</code> <pre><code>class CLIPDataset(torch.utils.data.Dataset):\n    \"\"\"\n    A dataset class for CLIP models that converts a dataset of dictionaries or tuples\n    into a format suitable for CLIP processing.\n\n    This class wraps an existing dataset and applies CLIP preprocessing to the images.\n    It expects each item in the dataset to be either a dictionary with 'image' and 'label' keys,\n    or a tuple/list of (image, label).\n\n    Args:\n        dataset: The original dataset to wrap.\n        processor (CLIPProcessor): The CLIP processor for preparing inputs. If None, no preprocessing is applied and raw images are returned.\n\n    Attributes:\n        dataset: The wrapped dataset.\n        processor (CLIPProcessor): The CLIP processor used for image preprocessing.\n    \"\"\"\n\n    def __init__(self, dataset, processor: Optional[CLIPProcessor] = None):\n        self.dataset = dataset\n        self.processor = processor\n\n    def __len__(self):\n        \"\"\"Returns the number of items in the dataset.\"\"\"\n        return len(self.dataset)\n\n    def __getitem__(self, idx: int):\n        \"\"\"\n        Retrieves and processes an item from the dataset.\n\n        Args:\n            idx (int): The index of the item to retrieve.\n\n        Returns:\n            tuple: A tuple containing the processed image tensor and the label.\n\n        Raises:\n            ValueError: If the item is neither a dictionary nor a tuple/list of length 2.\n        \"\"\"\n        item = self.dataset[idx]\n        if isinstance(item, dict):\n            item = item\n        elif isinstance(item, (tuple, list)):\n            assert len(item) == 2, \"Each item should be a tuple or list of length 2\"\n            item = {\"image\": item[0], \"label\": item[1]}\n        else:\n            raise ValueError(\"Each item should be a dictionary or a tuple of length 2\")\n        image = item[\"image\"]\n        if self.processor is not None:\n            if isinstance(self.processor, ProcessorMixin):\n                # Apply the processor to the image to get the input tensor\n                inputs = self.processor(images=[image], return_tensors=\"pt\")[\n                    \"pixel_values\"\n                ][0]\n        else:\n            # if processor is None, return the raw image directly\n            inputs = image\n        # convert boolean label to int, this is for the case when the label is a binary classification task\n        if isinstance(item[\"label\"], bool):\n            item[\"label\"] = 1 if item[\"label\"] else 0\n        return inputs, item[\"label\"]\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.dataset.clip_dataset.CLIPDataset(dataset)","title":"<code>dataset</code>","text":"\u2013            <p>The original dataset to wrap.</p>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.dataset.clip_dataset.CLIPDataset(processor)","title":"<code>processor</code>","text":"(<code>CLIPProcessor</code>, default:                   <code>None</code> )           \u2013            <p>The CLIP processor for preparing inputs. If None, no preprocessing is applied and raw images are returned.</p>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.dataset.clip_dataset.CLIPDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Retrieves and processes an item from the dataset.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple</code>          \u2013            <p>A tuple containing the processed image tensor and the label.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the item is neither a dictionary nor a tuple/list of length 2.</p> </li> </ul> Source code in <code>fusion_bench/dataset/clip_dataset.py</code> <pre><code>def __getitem__(self, idx: int):\n    \"\"\"\n    Retrieves and processes an item from the dataset.\n\n    Args:\n        idx (int): The index of the item to retrieve.\n\n    Returns:\n        tuple: A tuple containing the processed image tensor and the label.\n\n    Raises:\n        ValueError: If the item is neither a dictionary nor a tuple/list of length 2.\n    \"\"\"\n    item = self.dataset[idx]\n    if isinstance(item, dict):\n        item = item\n    elif isinstance(item, (tuple, list)):\n        assert len(item) == 2, \"Each item should be a tuple or list of length 2\"\n        item = {\"image\": item[0], \"label\": item[1]}\n    else:\n        raise ValueError(\"Each item should be a dictionary or a tuple of length 2\")\n    image = item[\"image\"]\n    if self.processor is not None:\n        if isinstance(self.processor, ProcessorMixin):\n            # Apply the processor to the image to get the input tensor\n            inputs = self.processor(images=[image], return_tensors=\"pt\")[\n                \"pixel_values\"\n            ][0]\n    else:\n        # if processor is None, return the raw image directly\n        inputs = image\n    # convert boolean label to int, this is for the case when the label is a binary classification task\n    if isinstance(item[\"label\"], bool):\n        item[\"label\"] = 1 if item[\"label\"] else 0\n    return inputs, item[\"label\"]\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.dataset.clip_dataset.CLIPDataset.__getitem__(idx)","title":"<code>idx</code>","text":"(<code>int</code>)           \u2013            <p>The index of the item to retrieve.</p>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.dataset.clip_dataset.CLIPDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of items in the dataset.</p> Source code in <code>fusion_bench/dataset/clip_dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"Returns the number of items in the dataset.\"\"\"\n    return len(self.dataset)\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.dataset.image_dataset.TransformedImageDataset","title":"<code>TransformedImageDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A dataset class for image classification tasks that applies a transform to images.</p> <p>This class wraps an existing dataset and applies a specified transform to the images. It expects each item in the dataset to be either a dictionary with 'image' and 'label' keys, or a tuple/list of (image, label).</p> <p>Parameters:</p> <p>Attributes:</p> <ul> <li> <code>dataset</code>           \u2013            <p>The wrapped dataset.</p> </li> <li> <code>transform</code>               (<code>Callable</code>)           \u2013            <p>The transform to be applied to the images.</p> </li> </ul> Source code in <code>fusion_bench/dataset/image_dataset.py</code> <pre><code>class TransformedImageDataset(Dataset):\n    \"\"\"\n    A dataset class for image classification tasks that applies a transform to images.\n\n    This class wraps an existing dataset and applies a specified transform to the images.\n    It expects each item in the dataset to be either a dictionary with 'image' and 'label' keys,\n    or a tuple/list of (image, label).\n\n    Args:\n        dataset: The original dataset to wrap.\n        transform (Callable): A function/transform to apply on the image.\n\n    Attributes:\n        dataset: The wrapped dataset.\n        transform (Callable): The transform to be applied to the images.\n    \"\"\"\n\n    def __init__(self, dataset, transform: Callable):\n        super().__init__()\n        self.dataset = dataset\n        self.transform = transform\n\n    def __len__(self):\n        \"\"\"Returns the number of items in the dataset.\"\"\"\n        return len(self.dataset)\n\n    def __getitem__(self, idx: int) -&gt; Tuple[Any, Any]:\n        \"\"\"\n        Retrieves and processes an item from the dataset.\n\n        Args:\n            idx (int): The index of the item to retrieve.\n\n        Returns:\n            tuple: A tuple containing the processed image and the label.\n\n        Raises:\n            ValueError: If the item is neither a dictionary nor a tuple/list of length 2.\n        \"\"\"\n        item = self.dataset[idx]\n        if isinstance(item, dict):\n            item = item\n        elif isinstance(item, (tuple, list)):\n            assert len(item) == 2, \"Each item should be a tuple or list of length 2\"\n            item = {\"image\": item[0], \"label\": item[1]}\n        else:\n            raise ValueError(\"Each item should be a dictionary or a tuple of length 2\")\n        image = item[\"image\"]\n        inputs = self.transform(image)\n        return inputs, item[\"label\"]\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.dataset.image_dataset.TransformedImageDataset(dataset)","title":"<code>dataset</code>","text":"\u2013            <p>The original dataset to wrap.</p>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.dataset.image_dataset.TransformedImageDataset(transform)","title":"<code>transform</code>","text":"(<code>Callable</code>)           \u2013            <p>A function/transform to apply on the image.</p>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.dataset.image_dataset.TransformedImageDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Retrieves and processes an item from the dataset.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple</code> (              <code>Tuple[Any, Any]</code> )          \u2013            <p>A tuple containing the processed image and the label.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the item is neither a dictionary nor a tuple/list of length 2.</p> </li> </ul> Source code in <code>fusion_bench/dataset/image_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Tuple[Any, Any]:\n    \"\"\"\n    Retrieves and processes an item from the dataset.\n\n    Args:\n        idx (int): The index of the item to retrieve.\n\n    Returns:\n        tuple: A tuple containing the processed image and the label.\n\n    Raises:\n        ValueError: If the item is neither a dictionary nor a tuple/list of length 2.\n    \"\"\"\n    item = self.dataset[idx]\n    if isinstance(item, dict):\n        item = item\n    elif isinstance(item, (tuple, list)):\n        assert len(item) == 2, \"Each item should be a tuple or list of length 2\"\n        item = {\"image\": item[0], \"label\": item[1]}\n    else:\n        raise ValueError(\"Each item should be a dictionary or a tuple of length 2\")\n    image = item[\"image\"]\n    inputs = self.transform(image)\n    return inputs, item[\"label\"]\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.dataset.image_dataset.TransformedImageDataset.__getitem__(idx)","title":"<code>idx</code>","text":"(<code>int</code>)           \u2013            <p>The index of the item to retrieve.</p>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fusion_bench.dataset.image_dataset.TransformedImageDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of items in the dataset.</p> Source code in <code>fusion_bench/dataset/image_dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"Returns the number of items in the dataset.\"\"\"\n    return len(self.dataset)\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#inference","title":"Inference","text":"<p>Perform inference on your dataset:</p> <pre><code>classifier.eval()\nwith torch.no_grad():\n    for images, labels in dataloader:\n        logits = classifier(images)\n        predictions = torch.argmax(logits, dim=1)\n        # Process predictions as needed\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#fine-tuning-optional","title":"Fine-tuning (Optional)","text":"<p>If you want to fine-tune the model:</p> <pre><code>optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-5)\ncriterion = torch.nn.CrossEntropyLoss()\n\nclassifier.train()\nfor epoch in range(num_epochs):\n    for images, labels in dataloader:\n        optimizer.zero_grad()\n        logits = classifier(images)\n        loss = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#advanced-usage","title":"Advanced Usage","text":""},{"location":"guides/clip_vit/HFCLIPClassifier/#custom-templates","title":"Custom Templates","text":"<p>You can provide custom templates when setting up the classification task:</p> <pre><code>custom_templates = [\n    lambda c: f\"a photo of a {c}\",\n    lambda c: f\"an image containing a {c}\",\n]\nclassifier.set_classification_task(class_names, templates=custom_templates)\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#accessing-model-components","title":"Accessing Model Components","text":"<p>You can access the text and vision models directly:</p> <pre><code>text_model = classifier.text_model\nvision_model = classifier.vision_model\n</code></pre>"},{"location":"guides/clip_vit/HFCLIPClassifier/#working-with-zero-shot-weights","title":"Working with Zero-shot Weights","text":"<p>After setting the classification task, you can access the zero-shot weights:</p> <pre><code>zeroshot_weights = classifier.zeroshot_weights\n</code></pre> <p>These weights represent the text embeddings for each class and can be used for further analysis or custom processing.</p> <p>Remember to adjust the code according to your specific dataset and requirements. This documentation provides a comprehensive guide for using the <code>HFCLIPClassifier</code> for image classification tasks with CLIP models.</p>"},{"location":"guides/clip_vit/classification_templates/","title":"CLIP Template Factory Documentation","text":""},{"location":"guides/clip_vit/classification_templates/#overview","title":"Overview","text":"<p><code>CLIPTemplateFactory</code> is a class designed to facilitate the dynamic creation and management of dataset templates for use with CLIP models. It serves as a factory class that allows users to retrieve class names and templates for various datasets, register new datasets, and obtain a list of all available datasets.</p>"},{"location":"guides/clip_vit/classification_templates/#usage-example","title":"Usage Example","text":"<pre><code>from fusion_bench.tasks.clip_classification import CLIPTemplateFactory\n\n# List all available datasets\navailable_datasets = CLIPTemplateFactory.get_available_datasets()\nprint(available_datasets)\n</code></pre> <p>get class names and templates for image classification</p> <pre><code>classnames, templates = CLIPTemplateFactory.get_classnames_and_templates(\"cifar10\")\n# classnames: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n# templates is a list functions, `templates[0](classnames[0])` will return 'a photo of a airplane.'\n\n# or you can use the `get_classnames_and_templates` function\nfrom fusion_bench.tasks.clip_classification import get_classnames_and_templates\n\nclassnames, templates = get_classnames_and_templates(\"cifar10\")\n</code></pre> <p>or you can register a new dataset</p> <pre><code>CLIPTemplateFactory.register_dataset(\n    \"new_dataset\",\n    dataset_info={\n        \"module\": \"module_name\",\n        \"classnames\": \"classnames\",\n        \"templates\": \"templates\"\n    }\n)\n# Retrieve class names and templates for a registered dataset\n# this is equivalent to:\n# &gt;&gt;&gt; from module_name import classnames, templates\nclassnames, templates = CLIPTemplateFactory.get_classnames_and_templates(\"new_dataset\")\n\n# or pass the classnames and templates directly\nCLIPTemplaetFactory.register_dataset(\n    \"new_dataset\",\n    classnames=[\"class1\", \"class2\", \"class3\"],\n    templates=[\n        lambda x: f\"a photo of a {x}.\",\n        lambda x: f\"a picture of a {x}.\",\n        lambda x: f\"an image of a {x}.\"\n    ]\n)\n</code></pre>"},{"location":"guides/clip_vit/classification_templates/#reference","title":"Reference","text":""},{"location":"guides/clip_vit/classification_templates/#fusion_bench.tasks.clip_classification.CLIPTemplateFactory","title":"<code>CLIPTemplateFactory</code>","text":"<p>A factory class for creating CLIP dataset templates.</p> <p>This class provides methods to retrieve class names and templates for various datasets, register new datasets, and get a list of all available datasets. It uses a mapping from dataset names to their respective module paths or detailed information, facilitating dynamic import and usage of dataset-specific class names and templates.</p> <p>Attributes:</p> <ul> <li> <code>_dataset_mapping</code>               (<code>dict</code>)           \u2013            <p>A mapping from dataset names to their respective module paths</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>get_classnames_and_templates</code>             \u2013              <p>str): Retrieves class names and templates for the specified dataset.</p> </li> <li> <code>register_dataset</code>             \u2013              <p>str, dataset_info: Dict[str, Any] = None, classnames: List[str] = None, templates: List[Callable] = None): Registers a new dataset with its associated information.</p> </li> <li> <code>get_available_datasets</code>             \u2013              <p>Returns a list of all available dataset names.</p> </li> </ul> Source code in <code>fusion_bench/tasks/clip_classification/__init__.py</code> <pre><code>class CLIPTemplateFactory:\n    \"\"\"\n    A factory class for creating CLIP dataset templates.\n\n    This class provides methods to retrieve class names and templates for various datasets,\n    register new datasets, and get a list of all available datasets. It uses a mapping\n    from dataset names to their respective module paths or detailed information, facilitating\n    dynamic import and usage of dataset-specific class names and templates.\n\n    Attributes:\n        _dataset_mapping (dict): A mapping from dataset names to their respective module paths\n        or detailed information including module path, class names, and templates.\n\n    Methods:\n        get_classnames_and_templates(dataset_name: str): Retrieves class names and templates for the specified dataset.\n        register_dataset(dataset_name: str, dataset_info: Dict[str, Any] = None, classnames: List[str] = None, templates: List[Callable] = None): Registers a new dataset with its associated information.\n        get_available_datasets(): Returns a list of all available dataset names.\n    \"\"\"\n\n    _dataset_mapping = {\n        \"mnist\": \".mnist\",\n        \"stanford-cars\": \".stanford_cars\",\n        \"stanford_cars\": \".stanford_cars\",\n        \"tanganke/stanford_cars\": \".stanford_cars\",\n        \"gtsrb\": \".gtsrb\",\n        \"tanganke/gtsrb\": \".gtsrb\",\n        \"resisc45\": \".resisc45\",\n        \"tanganke/resisc45\": \".resisc45\",\n        \"dtd\": \".dtd\",\n        \"tanganke/dtd\": \".dtd\",\n        \"eurosat\": \".eurosat\",\n        \"tanganke/eurosat\": \".eurosat\",\n        \"sun397\": \".sun397\",\n        \"tanganke/sun397\": \".sun397\",\n        \"cifar10\": \".cifar10\",\n        \"svhn\": \".svhn\",\n        \"cifar100\": {\n            \"module\": \".cifar100\",\n            \"classnames\": \"fine_label\",\n            \"templates\": \"templates\",\n        },\n        \"nateraw/rendered-sst2\": \".rendered_sst2\",\n        \"rendered-sst2\": \".rendered_sst2\",\n        \"tanganke/stl10\": \".stl10\",\n        \"stl10\": \".stl10\",\n        \"dpdl-benchmark/oxford_flowers102\": \".flower102\",\n        \"oxford_flowers102\": \".flower102\",\n        \"timm/oxford-iiit-pet\": \".oxford_iiit_pet\",\n        \"oxford-iiit-pet\": \".oxford_iiit_pet\",\n        \"imagenet\": \".imagenet\",\n        \"tiny-imagenet\": \".tiny_imagenet\",\n        \"pcam\": \".pcam\",\n        \"fer2013\": \".fer2013\",\n        \"emnist_mnist\": \".emnist_mnist\",\n        \"emnist_letters\": \".emnist_letters\",\n        \"kmnist\": \".kmnist\",\n        \"food101\": \".food101\",\n        \"fashion_mnist\": \".fashion_mnist\",\n        \"cub-200-2011\": \".cub_200_2011\",\n        \"mango-leaf-disease\": \".mango_leaf_disease\",\n    }\n\n    @staticmethod\n    def get_classnames_and_templates(dataset_name: str):\n        \"\"\"\n        Retrieves class names and templates for the specified dataset.\n\n        This method looks up the dataset information in the internal mapping and dynamically imports\n        the class names and templates from the specified module. It supports both simple string mappings\n        and detailed dictionary mappings for datasets.\n\n        Args:\n            dataset_name (str): The name of the dataset for which to retrieve class names and templates.\n\n        Returns:\n            Tuple[List[str], List[Callable]]: A tuple containing a list of class names and a list of template callables.\n\n        Raises:\n            ValueError: If the dataset_name is not found in the internal mapping.\n        \"\"\"\n        if dataset_name not in CLIPTemplateFactory._dataset_mapping:\n            raise ValueError(\n                f\"Unknown dataset {dataset_name}, available datasets: {CLIPTemplateFactory._dataset_mapping.keys()}. You can register a new dataset using `CLIPTemplateFactory.register_dataset()` method.\"\n            )\n\n        dataset_info = CLIPTemplateFactory._dataset_mapping[dataset_name]\n        # convert dataset_info to dict format: { 'module': str, 'classnames': str, 'templates': str }\n        if isinstance(dataset_info, str):\n            dataset_info = _check_module_name(dataset_info)\n            dataset_info = {\n                \"module\": dataset_info,\n                \"classnames\": \"classnames\",\n                \"templates\": \"templates\",\n            }\n        elif isinstance(dataset_info, dict):\n            if \"module\" in dataset_info:\n                dataset_info[\"module\"] = _check_module_name(dataset_info[\"module\"])\n\n        # import classnames and templates from the specified module\n        # convert to dict format: { 'labels': List[str], 'templates': List[Callable] }\n        if \"module\" in dataset_info:\n            module = importlib.import_module(dataset_info[\"module\"])\n            classnames = getattr(module, dataset_info[\"classnames\"])\n            templates = getattr(module, dataset_info[\"templates\"])\n        else:\n            classnames = dataset_info[\"classnames\"]\n            templates = dataset_info[\"templates\"]\n\n        return classnames, templates\n\n    @staticmethod\n    def register_dataset(\n        dataset_name: str,\n        *,\n        dataset_info: Dict[str, Any] = None,\n        classnames: List[str] = None,\n        templates: List[Callable] = None,\n    ):\n        \"\"\"\n        Registers a new dataset with its associated information.\n\n        This method allows for the dynamic addition of datasets to the internal mapping. It supports\n        registration through either a detailed dictionary (`dataset_info`) or separate lists of class names\n        and templates. If a dataset with the same name already exists, it will be overwritten.\n\n        The expected format and contents of `dataset_info` can vary depending on the needs of the dataset being registered, but typically, it includes the following keys:\n\n        - \"module\": A string specifying the module path where the dataset's related classes and functions are located. This is used for dynamic import of the dataset's class names and templates.\n        - \"classnames\": This key is expected to hold the name of the attribute or variable in the specified module that contains a list of class names relevant to the dataset. These class names are used to label data points in the dataset.\n        - \"templates\": Similar to \"classnames\", this key should specify the name of the attribute or variable in the module that contains a list of template callables. These templates are functions or methods that define how data points should be processed or transformed.\n\n        Args:\n            dataset_name (str): The name of the dataset to register.\n            dataset_info (Dict[str, Any], optional): A dictionary containing the dataset information, including module path, class names, and templates. Defaults to None.\n            classnames (List[str], optional): A list of class names for the dataset. Required if `dataset_info` is not provided. Defaults to None.\n            templates (List[Callable], optional): A list of template callables for the dataset. Required if `dataset_info` is not provided. Defaults to None.\n\n        Raises:\n            AssertionError: If neither `dataset_info` nor both `classnames` and `templates` are provided.\n        \"\"\"\n        assert dataset_info is None or (\n            classnames is not None and templates is not None\n        ), \"You must provide either `dataset_info` or both `classnames` and `templates`.\"\n\n        if dataset_name in CLIPTemplateFactory._dataset_mapping:\n            warnings.warn(\n                f\"Dataset {dataset_name} is already registered, overwriting the existing dataset information.\"\n            )\n        if dataset_info is None:\n            dataset_info = {\"classnames\": classnames, \"temolates\": templates}\n        CLIPTemplateFactory._dataset_mapping[dataset_name] = dataset_info\n\n    @staticmethod\n    def get_available_datasets():\n        \"\"\"\n        Get a list of all available dataset names.\n\n        Returns:\n            List[str]: A list of dataset names.\n        \"\"\"\n        return list(CLIPTemplateFactory._dataset_mapping.keys())\n</code></pre>"},{"location":"guides/clip_vit/classification_templates/#fusion_bench.tasks.clip_classification.CLIPTemplateFactory.get_available_datasets","title":"<code>get_available_datasets()</code>  <code>staticmethod</code>","text":"<p>Get a list of all available dataset names.</p> <p>Returns:</p> <ul> <li>           \u2013            <p>List[str]: A list of dataset names.</p> </li> </ul> Source code in <code>fusion_bench/tasks/clip_classification/__init__.py</code> <pre><code>@staticmethod\ndef get_available_datasets():\n    \"\"\"\n    Get a list of all available dataset names.\n\n    Returns:\n        List[str]: A list of dataset names.\n    \"\"\"\n    return list(CLIPTemplateFactory._dataset_mapping.keys())\n</code></pre>"},{"location":"guides/clip_vit/classification_templates/#fusion_bench.tasks.clip_classification.CLIPTemplateFactory.get_classnames_and_templates","title":"<code>get_classnames_and_templates(dataset_name)</code>  <code>staticmethod</code>","text":"<p>Retrieves class names and templates for the specified dataset.</p> <p>This method looks up the dataset information in the internal mapping and dynamically imports the class names and templates from the specified module. It supports both simple string mappings and detailed dictionary mappings for datasets.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li>           \u2013            <p>Tuple[List[str], List[Callable]]: A tuple containing a list of class names and a list of template callables.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the dataset_name is not found in the internal mapping.</p> </li> </ul> Source code in <code>fusion_bench/tasks/clip_classification/__init__.py</code> <pre><code>@staticmethod\ndef get_classnames_and_templates(dataset_name: str):\n    \"\"\"\n    Retrieves class names and templates for the specified dataset.\n\n    This method looks up the dataset information in the internal mapping and dynamically imports\n    the class names and templates from the specified module. It supports both simple string mappings\n    and detailed dictionary mappings for datasets.\n\n    Args:\n        dataset_name (str): The name of the dataset for which to retrieve class names and templates.\n\n    Returns:\n        Tuple[List[str], List[Callable]]: A tuple containing a list of class names and a list of template callables.\n\n    Raises:\n        ValueError: If the dataset_name is not found in the internal mapping.\n    \"\"\"\n    if dataset_name not in CLIPTemplateFactory._dataset_mapping:\n        raise ValueError(\n            f\"Unknown dataset {dataset_name}, available datasets: {CLIPTemplateFactory._dataset_mapping.keys()}. You can register a new dataset using `CLIPTemplateFactory.register_dataset()` method.\"\n        )\n\n    dataset_info = CLIPTemplateFactory._dataset_mapping[dataset_name]\n    # convert dataset_info to dict format: { 'module': str, 'classnames': str, 'templates': str }\n    if isinstance(dataset_info, str):\n        dataset_info = _check_module_name(dataset_info)\n        dataset_info = {\n            \"module\": dataset_info,\n            \"classnames\": \"classnames\",\n            \"templates\": \"templates\",\n        }\n    elif isinstance(dataset_info, dict):\n        if \"module\" in dataset_info:\n            dataset_info[\"module\"] = _check_module_name(dataset_info[\"module\"])\n\n    # import classnames and templates from the specified module\n    # convert to dict format: { 'labels': List[str], 'templates': List[Callable] }\n    if \"module\" in dataset_info:\n        module = importlib.import_module(dataset_info[\"module\"])\n        classnames = getattr(module, dataset_info[\"classnames\"])\n        templates = getattr(module, dataset_info[\"templates\"])\n    else:\n        classnames = dataset_info[\"classnames\"]\n        templates = dataset_info[\"templates\"]\n\n    return classnames, templates\n</code></pre>"},{"location":"guides/clip_vit/classification_templates/#fusion_bench.tasks.clip_classification.CLIPTemplateFactory.get_classnames_and_templates(dataset_name)","title":"<code>dataset_name</code>","text":"(<code>str</code>)           \u2013            <p>The name of the dataset for which to retrieve class names and templates.</p>"},{"location":"guides/clip_vit/classification_templates/#fusion_bench.tasks.clip_classification.CLIPTemplateFactory.register_dataset","title":"<code>register_dataset(dataset_name, *, dataset_info=None, classnames=None, templates=None)</code>  <code>staticmethod</code>","text":"<p>Registers a new dataset with its associated information.</p> <p>This method allows for the dynamic addition of datasets to the internal mapping. It supports registration through either a detailed dictionary (<code>dataset_info</code>) or separate lists of class names and templates. If a dataset with the same name already exists, it will be overwritten.</p> <p>The expected format and contents of <code>dataset_info</code> can vary depending on the needs of the dataset being registered, but typically, it includes the following keys:</p> <ul> <li>\"module\": A string specifying the module path where the dataset's related classes and functions are located. This is used for dynamic import of the dataset's class names and templates.</li> <li>\"classnames\": This key is expected to hold the name of the attribute or variable in the specified module that contains a list of class names relevant to the dataset. These class names are used to label data points in the dataset.</li> <li>\"templates\": Similar to \"classnames\", this key should specify the name of the attribute or variable in the module that contains a list of template callables. These templates are functions or methods that define how data points should be processed or transformed.</li> </ul> <p>Parameters:</p> <ul> <li> </li> <li> </li> <li> </li> <li> </li> </ul> <p>Raises:</p> <ul> <li> <code>AssertionError</code>             \u2013            <p>If neither <code>dataset_info</code> nor both <code>classnames</code> and <code>templates</code> are provided.</p> </li> </ul> Source code in <code>fusion_bench/tasks/clip_classification/__init__.py</code> <pre><code>@staticmethod\ndef register_dataset(\n    dataset_name: str,\n    *,\n    dataset_info: Dict[str, Any] = None,\n    classnames: List[str] = None,\n    templates: List[Callable] = None,\n):\n    \"\"\"\n    Registers a new dataset with its associated information.\n\n    This method allows for the dynamic addition of datasets to the internal mapping. It supports\n    registration through either a detailed dictionary (`dataset_info`) or separate lists of class names\n    and templates. If a dataset with the same name already exists, it will be overwritten.\n\n    The expected format and contents of `dataset_info` can vary depending on the needs of the dataset being registered, but typically, it includes the following keys:\n\n    - \"module\": A string specifying the module path where the dataset's related classes and functions are located. This is used for dynamic import of the dataset's class names and templates.\n    - \"classnames\": This key is expected to hold the name of the attribute or variable in the specified module that contains a list of class names relevant to the dataset. These class names are used to label data points in the dataset.\n    - \"templates\": Similar to \"classnames\", this key should specify the name of the attribute or variable in the module that contains a list of template callables. These templates are functions or methods that define how data points should be processed or transformed.\n\n    Args:\n        dataset_name (str): The name of the dataset to register.\n        dataset_info (Dict[str, Any], optional): A dictionary containing the dataset information, including module path, class names, and templates. Defaults to None.\n        classnames (List[str], optional): A list of class names for the dataset. Required if `dataset_info` is not provided. Defaults to None.\n        templates (List[Callable], optional): A list of template callables for the dataset. Required if `dataset_info` is not provided. Defaults to None.\n\n    Raises:\n        AssertionError: If neither `dataset_info` nor both `classnames` and `templates` are provided.\n    \"\"\"\n    assert dataset_info is None or (\n        classnames is not None and templates is not None\n    ), \"You must provide either `dataset_info` or both `classnames` and `templates`.\"\n\n    if dataset_name in CLIPTemplateFactory._dataset_mapping:\n        warnings.warn(\n            f\"Dataset {dataset_name} is already registered, overwriting the existing dataset information.\"\n        )\n    if dataset_info is None:\n        dataset_info = {\"classnames\": classnames, \"temolates\": templates}\n    CLIPTemplateFactory._dataset_mapping[dataset_name] = dataset_info\n</code></pre>"},{"location":"guides/clip_vit/classification_templates/#fusion_bench.tasks.clip_classification.CLIPTemplateFactory.register_dataset(dataset_name)","title":"<code>dataset_name</code>","text":"(<code>str</code>)           \u2013            <p>The name of the dataset to register.</p>"},{"location":"guides/clip_vit/classification_templates/#fusion_bench.tasks.clip_classification.CLIPTemplateFactory.register_dataset(dataset_info)","title":"<code>dataset_info</code>","text":"(<code>Dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary containing the dataset information, including module path, class names, and templates. Defaults to None.</p>"},{"location":"guides/clip_vit/classification_templates/#fusion_bench.tasks.clip_classification.CLIPTemplateFactory.register_dataset(classnames)","title":"<code>classnames</code>","text":"(<code>List[str]</code>, default:                   <code>None</code> )           \u2013            <p>A list of class names for the dataset. Required if <code>dataset_info</code> is not provided. Defaults to None.</p>"},{"location":"guides/clip_vit/classification_templates/#fusion_bench.tasks.clip_classification.CLIPTemplateFactory.register_dataset(templates)","title":"<code>templates</code>","text":"(<code>List[Callable]</code>, default:                   <code>None</code> )           \u2013            <p>A list of template callables for the dataset. Required if <code>dataset_info</code> is not provided. Defaults to None.</p>"},{"location":"guides/clip_vit/finetune/","title":"Fine-Tune Your Own Vision Transformer","text":"<p>In this guide, we will show you how to fine-tune your own Vision Transformer (ViT) model on a custom dataset using <code>fusion_bench</code> CLI.  FusionBench provides a simple and easy-to-use interface to fine-tune clip vision transformer in a single-task learning setting or traditional multi-task learning setting.</p>"},{"location":"guides/clip_vit/finetune/#basic-examples","title":"Basic Examples","text":""},{"location":"guides/clip_vit/finetune/#single-task-learning","title":"Single-Task Learning","text":"<p>Refer to <code>examples/clip_finetune/clip_finetune.sh</code> for a complete example of fine-tuning a CLIP-ViT model, including full fine-tuning, lora fine-tuning and linearized lora fine-tuning.</p>"},{"location":"guides/clip_vit/finetune/#multi-task-learning","title":"Multi-Task Learning","text":"<p>Fine-tune CLIP-ViT-B/32:</p> <pre><code>fusion_bench \\\n    method=clip_finetune \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_mtl \\\n    taskpool=dummy\n</code></pre> <p>Fine-tune CLIP-ViT-L/14 on eight GPUs with a per-device per-task batch size of 2.</p> <pre><code>fusion_bench \\\n    fabric.devices=8 \\\n    method=clip_finetune \\\n        method.batch_size=2 \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_mtl \\\n        modelpool.models._pretrained_.pretrained_model_name_or_path=openai/clip-vit-large-patch14 \\\n    taskpool=dummy\n</code></pre> <p>This will save the state dict of the vision model (<code>transformers.models.clip.CLIPVisionModel.CLIPVisionTransformer</code>) to the log directory. Subsequently, we can use <code>fusion_bench/scripts/clip/convert_checkpoint.py</code> to convert the state dict to a HuggingFace model (<code>CLIPVisionModel</code>).</p> method configurationmodel pool configuration config/method/clip_finetune.yaml<pre><code>name: clip_finetune\n\nseed: 42\n\nlearning_rate: 1e-5\nnum_steps: 4000\n\nbatch_size: 32\nnum_workers: 4\n\nsave_interval: 500\n</code></pre> config/modelpool/CLIPVisionModelPool/clip-vit-base-patch32_mtl.yaml<pre><code>defaults:\n  - CLIPVisionModelPool@: _template\n  - /model/clip-vit@models:\n      - clip-vit-base-patch32\n  - /dataset/image_classification/train@train_datasets: the_eight_tasks\n</code></pre> <pre><code># or CLIP-ViT-L/14, add option: --model openai/clip-vit-large-patch14\npython fusion_bench/scripts/clip/convert_checkpoint.py \\\n    --checkpoint /path/to/checkpoint \\\n    --output /path/to/output\n</code></pre> <p>After converting the checkpoint, you can use FusionBench to evaluate the model. For example, you can use the following command to evaluate the model on the eight tasks documented here.</p> <pre><code>path_to_clip_model=/path/to/converted/output\nfusion_bench method=dummy \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_individual \\\n    modelpool.models._pretrained_.pretrained_model_name_or_path=\"'${path_to_clip_model}'\" \\\n  taskpool=clip-vit-classification_TA8\n</code></pre>"},{"location":"guides/clip_vit/finetune/#single-task-learning_1","title":"Single-Task Learning","text":"<p>Simply remove some of the datasets from the <code>train_datasets</code> field in the model pool configuration.</p>"},{"location":"guides/clip_vit/finetune/#references","title":"References","text":""},{"location":"guides/clip_vit/finetune/#fusion_bench.method.classification.clip_finetune.ImageClassificationFineTuningForCLIP","title":"<code>ImageClassificationFineTuningForCLIP</code>","text":"<p>               Bases: <code>CLIPClassificationMixin</code>, <code>SimpleProfilerMixin</code>, <code>ModelFusionAlgorithm</code></p> <p>A class for fine-tuning CLIP models for image classification tasks.</p> Source code in <code>fusion_bench/method/classification/clip_finetune.py</code> <pre><code>class ImageClassificationFineTuningForCLIP(\n    CLIPClassificationMixin,\n    SimpleProfilerMixin,\n    ModelFusionAlgorithm,\n):\n    \"\"\"\n    A class for fine-tuning CLIP models for image classification tasks.\n    \"\"\"\n\n    def run(self, modelpool: CLIPVisionModelPool):\n        \"\"\"\n        Executes the fine-tuning process.\n\n        Args:\n            modelpool (CLIPVisionModelPool): The modelpool is responsible for loading the pre-trained model and training datasets.\n\n        Returns:\n            VisionModel: The fine-tuned vision model.\n        \"\"\"\n        self.modelpool = to_modelpool(modelpool)\n        config = self.config\n        self.log_hyperparams(config, filename=\"method_config.yaml\")\n        self.finetune_method = \"fine-tune\"\n\n        L.seed_everything(config.seed)\n\n        task_names = modelpool.train_dataset_names\n        with self.profile(\"setup model and optimizer\"):\n            processor, classifier, optimizer, lr_scheduler = self.setup_model()\n\n            if config.state_dict_load_path is not None:\n                self.fabric.load(\n                    config.state_dict_load_path,\n                    {\"vision_model\": classifier.clip_model.vision_model},\n                )\n                if config.skip_training:\n                    return classifier.clip_model.vision_model\n\n            self.setup_zero_shot_classification_head(\n                clip_processor=processor,\n                clip_model=classifier.clip_model,\n                task_names=task_names,\n            )\n\n            self.fabric.setup(classifier, optimizer)\n\n        with self.profile(\"setup data\"):\n            train_datasets = [\n                CLIPDataset(modelpool.load_train_dataset(task_name), processor)\n                for task_name in task_names\n            ]\n            train_dataloaders = [\n                DataLoader(\n                    dataset,\n                    shuffle=True,\n                    batch_size=config.batch_size,\n                    num_workers=config.num_workers,\n                )\n                for dataset in train_datasets\n            ]\n            train_dataloaders = self.fabric.setup_dataloaders(*train_dataloaders)\n            if not isinstance(train_dataloaders, (list, tuple)):\n                train_dataloaders = [train_dataloaders]\n            train_dataloader_iters = [\n                iter(InfiniteDataLoader(loader)) for loader in train_dataloaders\n            ]\n\n        # train\n        for step_idx in tqdm(\n            range(config.num_steps),\n            desc=self.finetune_method,\n            disable=not self.fabric.is_global_zero,\n            dynamic_ncols=True,\n        ):\n            optimizer.zero_grad()\n            loss = 0\n            for task, loader in zip(task_names, train_dataloader_iters):\n                with self.profile(\"data loading\"):\n                    batch = next(loader)\n                    images, labels = batch\n                with self.profile(\"forward\"):\n                    classifier.zeroshot_weights = self.zeroshot_weights[task]\n                    logits = classifier(images)\n                loss = loss + nn.functional.cross_entropy(logits, labels)\n\n            with self.profile(\"backward\"):\n                self.fabric.backward(loss)\n            with self.profile(\"optimizer step\"):\n                optimizer.step()\n                lr_scheduler.step()\n\n            metrics = {\"train/loss\": loss}\n\n            self.fabric.log_dict(metrics, step=step_idx)\n\n            if (step_idx + 1) % config.save_interval == 0:\n                save_path = os.path.join(\n                    self.log_dir, \"checkpoints\", f\"step={step_idx}.ckpt\"\n                )\n                self.save_model(classifier, save_path)\n\n        if config.state_dict_save_path is not None:\n            self.save_model(classifier, config.state_dict_save_path)\n        self.print_profile_summary()\n        return classifier.clip_model.vision_model\n\n    def save_model(\n        self,\n        model: HFCLIPClassifier | CLIPModel | CLIPVisionModel | CLIPVisionTransformer,\n        save_path: str,\n    ):\n        \"\"\"\n        Save the vision model to the specified path.\n\n        Args:\n            model (Union[HFCLIPClassifier, CLIPModel, CLIPVisionModel, CLIPVisionTransformer]): The model to save.\n            save_path (str): The path to save the model.\n        \"\"\"\n        if isinstance(model, HFCLIPClassifier):\n            vision_model = model.clip_model.vision_model\n        elif isinstance(model, CLIPModel):\n            vision_model = model.vision_model\n        elif isinstance(model, CLIPVisionModel):\n            vision_model = model.vision_model\n        elif isinstance(model, CLIPVisionTransformer):\n            vision_model = model\n        else:\n            raise ValueError(f\"Unsupported model type: {type(model)}\")\n\n        save_dir = os.path.dirname(save_path)\n        if save_dir and not os.path.exists(save_dir):\n            os.makedirs(save_dir, exist_ok=True)\n        self.fabric.save(save_path, {\"vision_model\": vision_model})\n\n    def setup_model(self):\n        \"\"\"\n        Sets up the model, optimizer, and learning rate scheduler.\n\n        This method initializes the CLIP model, applies LoRA if specified, and configures the optimizer and learning rate scheduler.\n\n        Returns:\n            Tuple: A tuple containing the processor, classifier, optimizer, and learning rate scheduler.\n        \"\"\"\n        config = self.config\n        modelpool = self.modelpool\n\n        clip_model: CLIPModel = modelpool.load_clip_model(\"_pretrained_\")\n        processor = modelpool.load_processor()\n\n        self.finetune_method = \"full fine-tune\"\n        if config.use_lora or config.use_l_lora:\n            self.finetune_method = \"lora fine-tune\"\n            lora_config = LoraConfig(\n                **OmegaConf.to_container(\n                    config.lora_config, resolve=True, enum_to_str=True\n                )\n            )\n            clip_model.vision_model = get_peft_model(\n                clip_model.vision_model, lora_config\n            )\n\n            if config.use_l_lora:\n                # http://arxiv.org/abs/2310.04742\n                # Anke Tang et al. Parameter Efficient Multi-task Model Fusion with Partial Linearization. ICLR 2024.\n                self.finetune_method = \"l-lora fine-tune\"\n                print(\"Linearizing Lora Layers\")\n                linearize_lora_model_(clip_model.vision_model)\n\n        classifier = HFCLIPClassifier(clip_model, processor=processor)\n\n        if self.fabric.is_global_zero:\n            print(\"=== Model Summary (For Vision Model Only) ===\")\n            print_parameters(classifier.clip_model.vision_model)\n        # configure optimizers\n        optimizer = torch.optim.Adam(\n            [\n                p\n                for p in classifier.clip_model.vision_model.parameters()\n                if p.requires_grad\n            ],\n            lr=config.learning_rate,\n            weight_decay=config.weight_decay,\n        )\n        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer=optimizer, T_max=config.num_steps\n        )\n\n        return processor, classifier, optimizer, lr_scheduler\n</code></pre>"},{"location":"guides/clip_vit/finetune/#fusion_bench.method.classification.clip_finetune.ImageClassificationFineTuningForCLIP.run","title":"<code>run(modelpool)</code>","text":"<p>Executes the fine-tuning process.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>VisionModel</code>          \u2013            <p>The fine-tuned vision model.</p> </li> </ul> Source code in <code>fusion_bench/method/classification/clip_finetune.py</code> <pre><code>def run(self, modelpool: CLIPVisionModelPool):\n    \"\"\"\n    Executes the fine-tuning process.\n\n    Args:\n        modelpool (CLIPVisionModelPool): The modelpool is responsible for loading the pre-trained model and training datasets.\n\n    Returns:\n        VisionModel: The fine-tuned vision model.\n    \"\"\"\n    self.modelpool = to_modelpool(modelpool)\n    config = self.config\n    self.log_hyperparams(config, filename=\"method_config.yaml\")\n    self.finetune_method = \"fine-tune\"\n\n    L.seed_everything(config.seed)\n\n    task_names = modelpool.train_dataset_names\n    with self.profile(\"setup model and optimizer\"):\n        processor, classifier, optimizer, lr_scheduler = self.setup_model()\n\n        if config.state_dict_load_path is not None:\n            self.fabric.load(\n                config.state_dict_load_path,\n                {\"vision_model\": classifier.clip_model.vision_model},\n            )\n            if config.skip_training:\n                return classifier.clip_model.vision_model\n\n        self.setup_zero_shot_classification_head(\n            clip_processor=processor,\n            clip_model=classifier.clip_model,\n            task_names=task_names,\n        )\n\n        self.fabric.setup(classifier, optimizer)\n\n    with self.profile(\"setup data\"):\n        train_datasets = [\n            CLIPDataset(modelpool.load_train_dataset(task_name), processor)\n            for task_name in task_names\n        ]\n        train_dataloaders = [\n            DataLoader(\n                dataset,\n                shuffle=True,\n                batch_size=config.batch_size,\n                num_workers=config.num_workers,\n            )\n            for dataset in train_datasets\n        ]\n        train_dataloaders = self.fabric.setup_dataloaders(*train_dataloaders)\n        if not isinstance(train_dataloaders, (list, tuple)):\n            train_dataloaders = [train_dataloaders]\n        train_dataloader_iters = [\n            iter(InfiniteDataLoader(loader)) for loader in train_dataloaders\n        ]\n\n    # train\n    for step_idx in tqdm(\n        range(config.num_steps),\n        desc=self.finetune_method,\n        disable=not self.fabric.is_global_zero,\n        dynamic_ncols=True,\n    ):\n        optimizer.zero_grad()\n        loss = 0\n        for task, loader in zip(task_names, train_dataloader_iters):\n            with self.profile(\"data loading\"):\n                batch = next(loader)\n                images, labels = batch\n            with self.profile(\"forward\"):\n                classifier.zeroshot_weights = self.zeroshot_weights[task]\n                logits = classifier(images)\n            loss = loss + nn.functional.cross_entropy(logits, labels)\n\n        with self.profile(\"backward\"):\n            self.fabric.backward(loss)\n        with self.profile(\"optimizer step\"):\n            optimizer.step()\n            lr_scheduler.step()\n\n        metrics = {\"train/loss\": loss}\n\n        self.fabric.log_dict(metrics, step=step_idx)\n\n        if (step_idx + 1) % config.save_interval == 0:\n            save_path = os.path.join(\n                self.log_dir, \"checkpoints\", f\"step={step_idx}.ckpt\"\n            )\n            self.save_model(classifier, save_path)\n\n    if config.state_dict_save_path is not None:\n        self.save_model(classifier, config.state_dict_save_path)\n    self.print_profile_summary()\n    return classifier.clip_model.vision_model\n</code></pre>"},{"location":"guides/clip_vit/finetune/#fusion_bench.method.classification.clip_finetune.ImageClassificationFineTuningForCLIP.run(modelpool)","title":"<code>modelpool</code>","text":"(<code>CLIPVisionModelPool</code>)           \u2013            <p>The modelpool is responsible for loading the pre-trained model and training datasets.</p>"},{"location":"guides/clip_vit/finetune/#fusion_bench.method.classification.clip_finetune.ImageClassificationFineTuningForCLIP.save_model","title":"<code>save_model(model, save_path)</code>","text":"<p>Save the vision model to the specified path.</p> <p>Parameters:</p> Source code in <code>fusion_bench/method/classification/clip_finetune.py</code> <pre><code>def save_model(\n    self,\n    model: HFCLIPClassifier | CLIPModel | CLIPVisionModel | CLIPVisionTransformer,\n    save_path: str,\n):\n    \"\"\"\n    Save the vision model to the specified path.\n\n    Args:\n        model (Union[HFCLIPClassifier, CLIPModel, CLIPVisionModel, CLIPVisionTransformer]): The model to save.\n        save_path (str): The path to save the model.\n    \"\"\"\n    if isinstance(model, HFCLIPClassifier):\n        vision_model = model.clip_model.vision_model\n    elif isinstance(model, CLIPModel):\n        vision_model = model.vision_model\n    elif isinstance(model, CLIPVisionModel):\n        vision_model = model.vision_model\n    elif isinstance(model, CLIPVisionTransformer):\n        vision_model = model\n    else:\n        raise ValueError(f\"Unsupported model type: {type(model)}\")\n\n    save_dir = os.path.dirname(save_path)\n    if save_dir and not os.path.exists(save_dir):\n        os.makedirs(save_dir, exist_ok=True)\n    self.fabric.save(save_path, {\"vision_model\": vision_model})\n</code></pre>"},{"location":"guides/clip_vit/finetune/#fusion_bench.method.classification.clip_finetune.ImageClassificationFineTuningForCLIP.save_model(model)","title":"<code>model</code>","text":"(<code>Union[HFCLIPClassifier, CLIPModel, CLIPVisionModel, CLIPVisionTransformer]</code>)           \u2013            <p>The model to save.</p>"},{"location":"guides/clip_vit/finetune/#fusion_bench.method.classification.clip_finetune.ImageClassificationFineTuningForCLIP.save_model(save_path)","title":"<code>save_path</code>","text":"(<code>str</code>)           \u2013            <p>The path to save the model.</p>"},{"location":"guides/clip_vit/finetune/#fusion_bench.method.classification.clip_finetune.ImageClassificationFineTuningForCLIP.setup_model","title":"<code>setup_model()</code>","text":"<p>Sets up the model, optimizer, and learning rate scheduler.</p> <p>This method initializes the CLIP model, applies LoRA if specified, and configures the optimizer and learning rate scheduler.</p> <p>Returns:</p> <ul> <li> <code>Tuple</code>          \u2013            <p>A tuple containing the processor, classifier, optimizer, and learning rate scheduler.</p> </li> </ul> Source code in <code>fusion_bench/method/classification/clip_finetune.py</code> <pre><code>def setup_model(self):\n    \"\"\"\n    Sets up the model, optimizer, and learning rate scheduler.\n\n    This method initializes the CLIP model, applies LoRA if specified, and configures the optimizer and learning rate scheduler.\n\n    Returns:\n        Tuple: A tuple containing the processor, classifier, optimizer, and learning rate scheduler.\n    \"\"\"\n    config = self.config\n    modelpool = self.modelpool\n\n    clip_model: CLIPModel = modelpool.load_clip_model(\"_pretrained_\")\n    processor = modelpool.load_processor()\n\n    self.finetune_method = \"full fine-tune\"\n    if config.use_lora or config.use_l_lora:\n        self.finetune_method = \"lora fine-tune\"\n        lora_config = LoraConfig(\n            **OmegaConf.to_container(\n                config.lora_config, resolve=True, enum_to_str=True\n            )\n        )\n        clip_model.vision_model = get_peft_model(\n            clip_model.vision_model, lora_config\n        )\n\n        if config.use_l_lora:\n            # http://arxiv.org/abs/2310.04742\n            # Anke Tang et al. Parameter Efficient Multi-task Model Fusion with Partial Linearization. ICLR 2024.\n            self.finetune_method = \"l-lora fine-tune\"\n            print(\"Linearizing Lora Layers\")\n            linearize_lora_model_(clip_model.vision_model)\n\n    classifier = HFCLIPClassifier(clip_model, processor=processor)\n\n    if self.fabric.is_global_zero:\n        print(\"=== Model Summary (For Vision Model Only) ===\")\n        print_parameters(classifier.clip_model.vision_model)\n    # configure optimizers\n    optimizer = torch.optim.Adam(\n        [\n            p\n            for p in classifier.clip_model.vision_model.parameters()\n            if p.requires_grad\n        ],\n        lr=config.learning_rate,\n        weight_decay=config.weight_decay,\n    )\n    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer=optimizer, T_max=config.num_steps\n    )\n\n    return processor, classifier, optimizer, lr_scheduler\n</code></pre>"},{"location":"guides/fusion_bench/","title":"Fusion Module Documentation","text":"<ul> <li>method: Implements different methods and algorithms for model training, evaluation, and other tasks. This includes base algorithms, ensemble methods, model recombination techniques, and more.  Read More</li> <li>modelpool: Manages different model pools. This module includes classes and functions for handling various models, including sequence-to-sequence language models, CLIP models, GPT-2 models, and models specific to the NYU Depth V2 dataset.  Read More</li> <li>taskpool: Manages different task pools. This module includes classes and functions for handling various tasks, such as image classification with CLIP, text generation with FLAN-T5, and tasks specific to the NYU Depth V2 dataset.  Read More</li> <li>models: Contains model definitions and utilities. This module includes implementations of different models, parameter management, input/output handling, and utility functions for model operations.</li> <li>tasks: Defines various tasks. This module includes implementations of different tasks, such as classification, text generation, and specific tasks for models like CLIP and FLAN-T5.</li> <li>dataset: Handles various datasets used in the project. This module includes dataset loaders and preprocessors for different types of data such as images, text, and specific datasets like NYU Depth V2.</li> <li>metrics: Defines metrics for evaluating models. This module includes specific metrics for different tasks and datasets, such as NYU Depth V2 and text-to-image generation.</li> <li>optim: Defines optimization algorithms. This module includes custom optimization algorithms used for training models.</li> <li>mixins: Provides mixin classes for additional functionalities. These mixins can be used to extend the capabilities of other classes, such as integrating with Lightning Fabric, providing live updates with the Rich library, and simple profiling.<ul> <li><code>LightningFabricMixin</code>: A mixin class for integrating Lightning Fabric into a project.  Read More</li> <li><code>SimpleProfilerMixin</code>: Adding profiling capabilities to your Python classes.  Read More</li> </ul> </li> <li>constants: Contains constant values and configurations used throughout the project.</li> <li>scripts: Contains scripts for various tasks. This includes command-line interface scripts, training scripts, and other utility scripts for managing and running different tasks.</li> <li>utils: Provides utility functions and classes. This module includes general utilities for data handling, device management, logging, timing, and other common operations needed throughout the project.</li> </ul>"},{"location":"guides/fusion_bench/mixins/lightning_fabric/","title":"LightningFabricMixin","text":""},{"location":"guides/fusion_bench/mixins/lightning_fabric/#reference","title":"Reference","text":""},{"location":"guides/fusion_bench/mixins/lightning_fabric/#fusion_bench.mixins.lightning_fabric","title":"<code>lightning_fabric</code>","text":""},{"location":"guides/fusion_bench/mixins/lightning_fabric/#fusion_bench.mixins.lightning_fabric.LightningFabricMixin","title":"<code>LightningFabricMixin</code>","text":"<p>A mixin class for integrating Lightning Fabric into a project.</p> <p>This class provides methods to initialize and manage a Lightning Fabric instance for distributed computing, including setup with optional logging, device management for tensors and modules, and hyperparameter logging. It leverages the Lightning framework to facilitate distributed training and inference across multiple devices and nodes, with support for custom logging via TensorBoard.</p> <p>Attributes: - _fabric (L.Fabric): The Lightning Fabric instance used for distributed computing.</p> <p>Note: This mixin is designed to be used with classes that require distributed computing capabilities and wish to leverage the Lightning Fabric for this purpose. It assumes the presence of a <code>config</code> attribute or parameter in the consuming class for configuration.</p> Source code in <code>fusion_bench/mixins/lightning_fabric.py</code> <pre><code>class LightningFabricMixin:\n    \"\"\"\n    A mixin class for integrating Lightning Fabric into a project.\n\n    This class provides methods to initialize and manage a Lightning Fabric instance for distributed computing,\n    including setup with optional logging, device management for tensors and modules, and hyperparameter logging.\n    It leverages the Lightning framework to facilitate distributed training and inference across multiple devices\n    and nodes, with support for custom logging via TensorBoard.\n\n    Attributes:\n    - _fabric (L.Fabric): The Lightning Fabric instance used for distributed computing.\n\n    Note:\n    This mixin is designed to be used with classes that require distributed computing capabilities and wish to\n    leverage the Lightning Fabric for this purpose. It assumes the presence of a `config` attribute or parameter\n    in the consuming class for configuration.\n    \"\"\"\n\n    _fabric_instance: L.Fabric = None\n\n    def setup_lightning_fabric(self, config: DictConfig):\n        \"\"\"\n        Initializes and launches the Lightning Fabric with optional logging.\n\n        This method sets up the Lightning Fabric for distributed computing based on the provided configuration. If a fabric\n        configuration is not found, it logs a warning and exits. Optionally, if a fabric logger configuration is provided,\n        it initializes a TensorBoardLogger with the specified settings.\n\n        Expected configuration keys:\n        - fabric: The configuration for the Lightning Fabric.\n        - fabric.loggers: The configuration for the TensorBoardLogger.\n        \"\"\"\n        if self._fabric_instance is None:\n            if config.get(\"fabric\", None) is None:\n                log.warning(\"No fabric configuration found. use default settings.\")\n                self._fabric_instance = L.Fabric()\n            else:\n                self._fabric_instance = instantiate(config.fabric)\n            if not _is_using_cli():  # if not using cli, launch the fabric\n                self._fabric_instance.launch()\n            # Set the log directory in config if it is not already set\n            if (\n                self.log_dir is not None\n                and hasattr(config, \"log_dir\")\n                and config.get(\"log_dir\", None) is None\n            ):\n                if self._fabric_instance.is_global_zero:\n                    log.info(f\"Setting log_dir to {self.log_dir}\")\n                config.log_dir = self.log_dir\n\n    @property\n    def fabric(self):\n        if self._fabric_instance is None:\n            self.setup_lightning_fabric(getattr(self, \"config\", DictConfig({})))\n        return self._fabric_instance\n\n    @property\n    def log_dir(self):\n        \"\"\"\n        Retrieves the log directory from the fabric's logger.\n        \"\"\"\n        if self.fabric is not None and len(self.fabric._loggers) &gt; 0:\n            log_dir = self.fabric.logger.log_dir\n            if self.fabric.is_global_zero and not os.path.exists(log_dir):\n                os.makedirs(log_dir, exist_ok=True)\n            return log_dir\n        else:\n            return None\n\n    def to_device(self, obj: TensorOrModule) -&gt; TensorOrModule:\n        \"\"\"\n        Moves a tensor or module to the proper device.\n\n        Args:\n            obj (TensorOrModule): The tensor or module to move to the device.\n\n        Returns:\n            TensorOrModule: the same type of object as the input, moved to the device.\n        \"\"\"\n        return self.fabric.to_device(obj)\n\n    @rank_zero_only\n    def log_hyperparams(\n        self,\n        config: Optional[DictConfig] = None,\n        save_dir: Optional[str] = None,\n        filename: str = \"config.yaml\",\n    ):\n        R\"\"\"\n        Logs the hyperparameters and saves the configuration to a YAML file.\n        The YAML file is saved in the log directory by default with the name `config.yaml`, or in the specified save directory `save_dir`.\n\n        Args:\n            config (Optional[DictConfig]): The configuration to log and save. If not provided, the class's `config` attribute is used.\n            save_dir (Optional[str]): The directory in which to save the configuration file. If not provided, the log directory is used.\n            filename (str): The name of the configuration file. Default is `config.yaml`.\n        \"\"\"\n        if config is None:\n            config = self.config\n        if save_dir is None:\n            save_dir = self.log_dir\n        self.fabric.logger.log_hyperparams(\n            OmegaConf.to_container(config, resolve=True, enum_to_str=True)\n        )\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir, exist_ok=True)\n        OmegaConf.save(\n            config,\n            os.path.join(self.log_dir if save_dir is None else save_dir, filename),\n        )\n\n    @property\n    def tensorboard_summarywriter(\n        self,\n    ) -&gt; \"lightning.fabric.loggers.tensorboard.SummaryWriter\":\n        if isinstance(self.fabric.logger, TensorBoardLogger):\n            return self.fabric.logger.experiment\n        else:\n            raise AttributeError(\"the logger is not a TensorBoardLogger.\")\n\n    @property\n    def is_debug_mode(self):\n        if hasattr(self, \"config\") and self.config.get(\"fast_dev_run\", False):\n            return True\n        elif hasattr(self, \"_program\") and self._program.config.get(\n            \"fast_dev_run\", False\n        ):\n            return True\n        else:\n            return False\n</code></pre>"},{"location":"guides/fusion_bench/mixins/lightning_fabric/#fusion_bench.mixins.lightning_fabric.LightningFabricMixin.log_dir","title":"<code>log_dir</code>  <code>property</code>","text":"<p>Retrieves the log directory from the fabric's logger.</p>"},{"location":"guides/fusion_bench/mixins/lightning_fabric/#fusion_bench.mixins.lightning_fabric.LightningFabricMixin.log_hyperparams","title":"<code>log_hyperparams(config=None, save_dir=None, filename='config.yaml')</code>","text":"<p>Logs the hyperparameters and saves the configuration to a YAML file. The YAML file is saved in the log directory by default with the name <code>config.yaml</code>, or in the specified save directory <code>save_dir</code>.</p> <p>Parameters:</p> <ul> <li> <code>config</code> \u00b6              (<code>Optional[DictConfig]</code>, default:                   <code>None</code> )           \u2013            <p>The configuration to log and save. If not provided, the class's <code>config</code> attribute is used.</p> </li> <li> <code>save_dir</code> \u00b6              (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The directory in which to save the configuration file. If not provided, the log directory is used.</p> </li> <li> <code>filename</code> \u00b6              (<code>str</code>, default:                   <code>'config.yaml'</code> )           \u2013            <p>The name of the configuration file. Default is <code>config.yaml</code>.</p> </li> </ul> Source code in <code>fusion_bench/mixins/lightning_fabric.py</code> <pre><code>@rank_zero_only\ndef log_hyperparams(\n    self,\n    config: Optional[DictConfig] = None,\n    save_dir: Optional[str] = None,\n    filename: str = \"config.yaml\",\n):\n    R\"\"\"\n    Logs the hyperparameters and saves the configuration to a YAML file.\n    The YAML file is saved in the log directory by default with the name `config.yaml`, or in the specified save directory `save_dir`.\n\n    Args:\n        config (Optional[DictConfig]): The configuration to log and save. If not provided, the class's `config` attribute is used.\n        save_dir (Optional[str]): The directory in which to save the configuration file. If not provided, the log directory is used.\n        filename (str): The name of the configuration file. Default is `config.yaml`.\n    \"\"\"\n    if config is None:\n        config = self.config\n    if save_dir is None:\n        save_dir = self.log_dir\n    self.fabric.logger.log_hyperparams(\n        OmegaConf.to_container(config, resolve=True, enum_to_str=True)\n    )\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir, exist_ok=True)\n    OmegaConf.save(\n        config,\n        os.path.join(self.log_dir if save_dir is None else save_dir, filename),\n    )\n</code></pre>"},{"location":"guides/fusion_bench/mixins/lightning_fabric/#fusion_bench.mixins.lightning_fabric.LightningFabricMixin.setup_lightning_fabric","title":"<code>setup_lightning_fabric(config)</code>","text":"<p>Initializes and launches the Lightning Fabric with optional logging.</p> <p>This method sets up the Lightning Fabric for distributed computing based on the provided configuration. If a fabric configuration is not found, it logs a warning and exits. Optionally, if a fabric logger configuration is provided, it initializes a TensorBoardLogger with the specified settings.</p> <p>Expected configuration keys: - fabric: The configuration for the Lightning Fabric. - fabric.loggers: The configuration for the TensorBoardLogger.</p> Source code in <code>fusion_bench/mixins/lightning_fabric.py</code> <pre><code>def setup_lightning_fabric(self, config: DictConfig):\n    \"\"\"\n    Initializes and launches the Lightning Fabric with optional logging.\n\n    This method sets up the Lightning Fabric for distributed computing based on the provided configuration. If a fabric\n    configuration is not found, it logs a warning and exits. Optionally, if a fabric logger configuration is provided,\n    it initializes a TensorBoardLogger with the specified settings.\n\n    Expected configuration keys:\n    - fabric: The configuration for the Lightning Fabric.\n    - fabric.loggers: The configuration for the TensorBoardLogger.\n    \"\"\"\n    if self._fabric_instance is None:\n        if config.get(\"fabric\", None) is None:\n            log.warning(\"No fabric configuration found. use default settings.\")\n            self._fabric_instance = L.Fabric()\n        else:\n            self._fabric_instance = instantiate(config.fabric)\n        if not _is_using_cli():  # if not using cli, launch the fabric\n            self._fabric_instance.launch()\n        # Set the log directory in config if it is not already set\n        if (\n            self.log_dir is not None\n            and hasattr(config, \"log_dir\")\n            and config.get(\"log_dir\", None) is None\n        ):\n            if self._fabric_instance.is_global_zero:\n                log.info(f\"Setting log_dir to {self.log_dir}\")\n            config.log_dir = self.log_dir\n</code></pre>"},{"location":"guides/fusion_bench/mixins/lightning_fabric/#fusion_bench.mixins.lightning_fabric.LightningFabricMixin.to_device","title":"<code>to_device(obj)</code>","text":"<p>Moves a tensor or module to the proper device.</p> <p>Parameters:</p> <ul> <li> <code>obj</code> \u00b6              (<code>TensorOrModule</code>)           \u2013            <p>The tensor or module to move to the device.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TensorOrModule</code> (              <code>TensorOrModule</code> )          \u2013            <p>the same type of object as the input, moved to the device.</p> </li> </ul> Source code in <code>fusion_bench/mixins/lightning_fabric.py</code> <pre><code>def to_device(self, obj: TensorOrModule) -&gt; TensorOrModule:\n    \"\"\"\n    Moves a tensor or module to the proper device.\n\n    Args:\n        obj (TensorOrModule): The tensor or module to move to the device.\n\n    Returns:\n        TensorOrModule: the same type of object as the input, moved to the device.\n    \"\"\"\n    return self.fabric.to_device(obj)\n</code></pre>"},{"location":"guides/fusion_bench/mixins/lightning_fabric/#fusion_bench.mixins.lightning_fabric.get_policy","title":"<code>get_policy(*args)</code>","text":"<p>Get the policy from the provided list of policy names.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>set</code> (              <code>set</code> )          \u2013            <p>A set of policy objects.</p> </li> </ul> Source code in <code>fusion_bench/mixins/lightning_fabric.py</code> <pre><code>def get_policy(*args: str) -&gt; set:\n    \"\"\"\n    Get the policy from the provided list of policy names.\n\n    Args:\n        *args (str): A list of policy names.\n\n    Returns:\n        set: A set of policy objects.\n    \"\"\"\n    return {import_object(arg) for arg in args}\n</code></pre>"},{"location":"guides/fusion_bench/mixins/lightning_fabric/#fusion_bench.mixins.lightning_fabric.get_policy(*args)","title":"<code>*args</code>","text":"(<code>str</code>, default:                   <code>()</code> )           \u2013            <p>A list of policy names.</p>"},{"location":"guides/fusion_bench/mixins/simple_profiler/","title":"SimpleProfilerMixin","text":"<p>The <code>SimpleProfilerMixin</code> is a powerful and easy-to-use tool for adding profiling capabilities to your Python classes. This mixin is particularly useful for developers who need to optimize performance in their applications or want to gain insights into the time consumption of different parts of their code. By incorporating <code>SimpleProfilerMixin</code> into your classes, you can easily track and analyze the execution time of various operations, helping you identify bottlenecks and optimize your code effectively.</p>"},{"location":"guides/fusion_bench/mixins/simple_profiler/#reference","title":"Reference","text":""},{"location":"guides/fusion_bench/mixins/simple_profiler/#fusion_bench.mixins.simple_profiler.SimpleProfilerMixin","title":"<code>SimpleProfilerMixin</code>","text":"<p>A mixin class that provides simple profiling capabilities.</p> <p>This mixin allows for easy profiling of code blocks using a context manager. It also provides methods to start and stop profiling actions, and to print a summary of the profiling results.</p> <p>Examples:</p> <pre><code>class MyClass(SimpleProfilerMixin):\n    def do_something(self):\n        with self.profile(\"work\"):\n            # do some work here\n            ...\n        with self.profile(\"more work\"):\n            # do more work here\n            ...\n\n        # print the profiling summary\n        self.print_profile_summary()\n</code></pre> <p>Attributes:</p> <ul> <li> <code>_profiler</code>               (<code>SimpleProfiler</code>)           \u2013            <p>An instance of the SimpleProfiler class used for profiling.</p> </li> </ul> Source code in <code>fusion_bench/mixins/simple_profiler.py</code> <pre><code>class SimpleProfilerMixin:\n    \"\"\"\n    A mixin class that provides simple profiling capabilities.\n\n    This mixin allows for easy profiling of code blocks using a context manager.\n    It also provides methods to start and stop profiling actions, and to print\n    a summary of the profiling results.\n\n    Examples:\n\n    ```python\n    class MyClass(SimpleProfilerMixin):\n        def do_something(self):\n            with self.profile(\"work\"):\n                # do some work here\n                ...\n            with self.profile(\"more work\"):\n                # do more work here\n                ...\n\n            # print the profiling summary\n            self.print_profile_summary()\n    ```\n\n    Attributes:\n        _profiler (SimpleProfiler): An instance of the SimpleProfiler class used for profiling.\n    \"\"\"\n\n    _profiler: SimpleProfiler = None\n\n    @property\n    def profiler(self):\n        # Lazy initialization of the profiler instance\n        if self._profiler is None:\n            self._profiler = SimpleProfiler()\n        return self._profiler\n\n    @contextmanager\n    def profile(self, action_name: str) -&gt; Generator:\n        \"\"\"\n        Context manager for profiling a code block\n\n        Example:\n\n        ```python\n        with self.profile(\"work\"):\n            # do some work here\n            ...\n        ```\n        \"\"\"\n        try:\n            self.start_profile(action_name)\n            yield action_name\n        finally:\n            self.stop_profile(action_name)\n\n    def start_profile(self, action_name: str):\n        self.profiler.start(action_name)\n\n    def stop_profile(self, action_name: str):\n        self.profiler.stop(action_name)\n\n    @rank_zero_only\n    def print_profile_summary(self):\n        print(self.profiler.summary())\n\n    def __del__(self):\n        if self._profiler is not None:\n            del self._profiler\n            self._profiler = None\n</code></pre>"},{"location":"guides/fusion_bench/mixins/simple_profiler/#fusion_bench.mixins.simple_profiler.SimpleProfilerMixin.profile","title":"<code>profile(action_name)</code>","text":"<p>Context manager for profiling a code block</p> <p>Example:</p> <pre><code>with self.profile(\"work\"):\n    # do some work here\n    ...\n</code></pre> Source code in <code>fusion_bench/mixins/simple_profiler.py</code> <pre><code>@contextmanager\ndef profile(self, action_name: str) -&gt; Generator:\n    \"\"\"\n    Context manager for profiling a code block\n\n    Example:\n\n    ```python\n    with self.profile(\"work\"):\n        # do some work here\n        ...\n    ```\n    \"\"\"\n    try:\n        self.start_profile(action_name)\n        yield action_name\n    finally:\n        self.stop_profile(action_name)\n</code></pre>"},{"location":"guides/nlp/question_answering/","title":"Question Answering","text":""},{"location":"guides/nlp/question_answering/#key-concepts","title":"Key Concepts","text":""},{"location":"guides/nlp/question_answering/#overlapping-tokens","title":"Overlapping Tokens","text":"<p>Overlapping tokens are segments of text that are repeated between consecutive chunks when a long text needs to be split into smaller pieces due to model's maximum token limit.</p> <p>Here's a detailed explanation:</p> <ol> <li> <p>Why we need overlapping:</p> <ul> <li>When a text is too long for the model's context window (max_length)</li> <li>To maintain continuity and context between chunks</li> <li>To avoid losing information that might be split between chunks</li> </ul> </li> <li> <p>Key parameters in the code:</p> <ul> <li>max_length: Maximum number of tokens allowed</li> <li>stride: Number of overlapping tokens between chunks</li> <li>return_overflowing_tokens: Tells tokenizer to return multiple chunks</li> <li>truncation=\"only_second\": Only truncates the context, not the question</li> </ul> </li> </ol> <p>Let's illustrate with an example:</p> <p>Suppose we have a text: \"The quick brown fox jumps over the lazy sleeping dog\". The tokenization might look like this:</p> <pre><code>Chunk 1: [The quick brown fox jumps over]\n                    \u2193 overlap \u2193\nChunk 2:            [brown fox jumps over the lazy]\n                                \u2193 overlap \u2193\nChunk 3:                        [jumps over the lazy sleeping dog]\n</code></pre> <p>Real-world example with actual tokens:</p> <pre><code>from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\nquestion = \"What did the fox do?\"\ncontext = \"The quick brown fox jumps over the lazy sleeping dog. It was a beautiful sunny day.\"\n\ntokenized = tokenizer(\n    question,\n    context,\n    max_length=16,\n    truncation=\"only_second\",\n    return_overflowing_tokens=True,\n    stride=4\n)\n\n# Print the decoded tokens for each chunk\nfor encoding in tokenized[\"input_ids\"]:\n    print(tokenizer.decode(encoding))\n</code></pre>"},{"location":"guides/nlp/question_answering/#offset-mapping","title":"Offset Mapping","text":"<p>Offset mapping is a feature that provides the character-level mapping between the original text and the tokenized output. It returns a list of tuples (start, end) where:</p> <ul> <li>start: starting character position in the original text</li> <li>end: ending character position in the original text</li> </ul> <p>Here's a detailed breakdown:</p> <ol> <li> <p>Structure of offset_mapping:</p> <pre><code>[(0, 0),    # [CLS] token - special token, maps to nothing\n(0, 3),     # \"how\" - maps to characters 0-3 in original text\n(4, 8),     # \"many\" - maps to characters 4-8\n...]\n</code></pre> </li> <li> <p>Special tokens mapping:</p> <ul> <li>[CLS], [SEP], [PAD]: represented as (0, 0)</li> <li>These tokens don't correspond to any actual text in the input</li> </ul> </li> <li> <p>Usage example:</p> <pre><code># Example showing how to use offset_mapping\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\ntext = \"How many cats?\"\ntokenized = tokenizer(text, return_offsets_mapping=True)\n\nfor token_id, offset in zip(tokenized[\"input_ids\"], tokenized[\"offset_mapping\"]):\n    token = tokenizer.decode([token_id])\n    start, end = offset\n    original_text = text[start:end] if start != end else \"[SPECIAL]\"\n    print(f\"Token: {token}, Offset: {offset}, Original text: {original_text}\")\n</code></pre> </li> </ol> <p>Main purposes of offset_mapping:</p> <ol> <li> <p>Answer span location:</p> <ul> <li>Helps locate exact position of answers in QA tasks</li> <li>Maps token positions back to original text positions</li> </ul> </li> <li> <p>Token-text alignment:</p> <ul> <li>Enables precise tracking of which parts of original text correspond to which tokens</li> <li>Useful for tasks requiring character-level precision</li> </ul> </li> <li> <p>Handling overlapping chunks:</p> <ul> <li>Helps maintain correct position information when text is split into chunks</li> <li>Essential for combining predictions from multiple chunks</li> </ul> </li> </ol> <p>Common operations with offset_mapping: <pre><code># Finding original text for a token\ndef get_original_text(text, offset):\n    start, end = offset\n    return text[start:end] if start != end else \"[SPECIAL]\"\n\n# Finding token position for a text span\ndef find_token_position(offset_mapping, char_start, char_end):\n    for idx, (start, end) in enumerate(offset_mapping):\n        if start == char_start and end == char_end:\n            return idx\n    return None\n</code></pre></p> <p>This feature is particularly important in Question Answering tasks where you need to:</p> <ul> <li>Map predicted token positions back to original text</li> <li>Handle answer spans across multiple chunks</li> <li>Maintain precise position information for answer extraction</li> </ul>"},{"location":"guides/nlp/question_answering/#overflow_to_sample_mapping","title":"overflow_to_sample_mapping","text":"<p><code>overflow_to_sample_mapping</code> is an index list that maps each feature in the overflowing tokens back to its original sample. It's particularly useful when processing multiple examples with overflow.</p> <p>Here's a detailed explanation:</p> <ul> <li>When a text is split into multiple chunks due to length</li> <li>Each chunk needs to be traced back to its original example</li> <li><code>overflow_to_sample_mapping</code> provides this tracking mechanism</li> </ul> <p>Here's a comprehensive example:</p> <pre><code>from transformers import AutoTokenizer\nimport pandas as pd\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Multiple examples\nexamples = {\n    \"question\": [\n        \"What is the capital?\",\n        \"Who won the game?\"\n    ],\n    \"context\": [\n        \"Paris is the capital of France. It is known for the Eiffel Tower. The city has many historic monuments.\" * 5,  # Made longer by repeating\n        \"The Lakers won the game against the Bulls. It was a close match.\" * 2\n    ]\n}\n\n# Tokenize with overflow\ntokenized_examples = []\nfor q, c in zip(examples[\"question\"], examples[\"context\"]):\n    tokenized = tokenizer(\n        q,\n        c,\n        max_length=50,  # Small max_length for demonstration\n        stride=10,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n        truncation=\"only_second\"\n    )\n    tokenized_examples.append(tokenized)\n\n# Let's see how many chunks each example was split into\nfor i, tokenized in enumerate(tokenized_examples):\n    print(f\"\\nExample {i}:\")\n    print(f\"Number of chunks: {len(tokenized['input_ids'])}\")\n    print(f\"Overflow to sample mapping: {tokenized.overflow_to_sample_mapping}\")\n</code></pre> <p>This might output something like:</p> <pre><code>Example 0:\nNumber of chunks: 4\nOverflow to sample mapping: [0, 0, 0, 0]  # All chunks belong to first example\n\nExample 1:\nNumber of chunks: 2\nOverflow to sample mapping: [0, 0]  # All chunks belong to first example\n</code></pre> <p>Practical Use Case:</p> <pre><code>def prepare_train_features(examples):\n    # Tokenize our examples with truncation and padding, but keep the overflows using a stride\n    tokenized_examples = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=384,\n        stride=128,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context\n    sample_mapping = tokenized_examples.overflow_to_sample_mapping\n\n    # For each feature, we need to know from which example it came from\n    for i, sample_idx in enumerate(sample_mapping):\n        # Get the example's original question\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_start = sequence_ids.index(1)  # Find where context starts\n\n        # Set example ID for this feature\n        tokenized_examples[i][\"example_id\"] = examples[\"id\"][sample_idx]\n\n        # Set offset mappings for answer spans\n        tokenized_examples[i][\"offset_mapping\"] = [\n            (o if sequence_ids[k] == 1 else None)\n            for k, o in enumerate(tokenized_examples[i][\"offset_mapping\"])\n        ]\n\n    return tokenized_examples\n</code></pre> <p>Key Benefits:</p> <ol> <li> <p>Tracking Features: </p> <ul> <li>Maps each feature back to its source example</li> <li>Maintains relationship between chunks and original data</li> </ul> </li> <li> <p>Data Processing:</p> <ul> <li>Helps in maintaining example-level information</li> <li>Essential for combining predictions from multiple chunks</li> </ul> </li> <li> <p>Batch Processing:</p> <ul> <li>Enables proper batching of features</li> <li>Maintains data integrity during training</li> </ul> </li> </ol> <p>Common Use Pattern:</p> <pre><code># Example of using overflow_to_sample_mapping in a training loop\nfor i, sample_idx in enumerate(tokenized_examples.overflow_to_sample_mapping):\n    # Get original example ID\n    original_example_id = examples[\"id\"][sample_idx]\n\n    # Get original answer\n    original_answer = examples[\"answers\"][sample_idx]\n\n    # Process feature while maintaining connection to original example\n    process_feature(tokenized_examples[i], original_example_id, original_answer)\n</code></pre> <p>This feature is particularly important in Question Answering tasks where:</p> <ul> <li>Long contexts need to be split into multiple chunks</li> <li>Each chunk needs to be processed separately</li> <li>Results need to be combined while maintaining reference to original examples</li> </ul>"},{"location":"modelpool/","title":"Introduction to Model Pool Module","text":"<p>A modelpool is a collection of models that are utilized in the process of model fusion. In the context of straightforward model fusion techniques, like averaging, only models with the same architecture are used. While for more complex methods, such as AdaMerging <sup>1</sup>, each model is paired with a unique set of unlabeled test data. This data is used during the test-time adaptation phase.</p>"},{"location":"modelpool/#yaml-configuration","title":"Yaml Configuration","text":"<p>A modelpool is specified by a <code>yaml</code> configuration file, which often contains the following fields:</p> <ul> <li><code>type</code>: The name of the modelpool.</li> <li><code>models</code>: A list of models, each model is dict with the following fields:<ul> <li><code>name</code>: The name of the model. There are some special names that are reserved for specific purposes, such as <code>_pretrained_</code> for the pretrained model.</li> <li><code>path</code>: The path to the model file.</li> <li><code>type</code>: The type of the model. If this field is not specified, the type is inferred from the <code>model_type</code>.</li> </ul> </li> </ul> <p>For more complex model fusion techniques that requires data, the modelpool configuration file may also contain the following fields:</p> <ul> <li><code>dataset_type</code>: The type of the dataset used for training the models in the modelpool.</li> <li><code>datasets</code>: A list of datasets, each dataset is dict with the following fields:<ul> <li><code>name</code>: The name of the dataset, which is used to pair the dataset with the corresponding model. The name of the dataset should match the name of the model.</li> <li><code>path</code>: The path to the dataset file.</li> <li><code>type</code>: The type of the dataset. If this field is not specified, the type is inferred from the <code>dataset_type</code>.</li> </ul> </li> </ul> <p>We provide a list of modelpools that contain models trained on different datasets and with different architectures. Each modelpool is described in a separate document.</p>"},{"location":"modelpool/#basic-usage","title":"Basic Usage","text":"<p>The model is not loaded by default when you initialize a modelpool, you can load a model from a modelpool by calling the <code>load_model</code> method:</p> <pre><code>model = modelpool.load_model('model_name')\n</code></pre>"},{"location":"modelpool/#references","title":"References","text":"<ol> <li> <p>AdaMerging: Adaptive Model Merging for Multi-Task Learning. http://arxiv.org/abs/2310.02575\u00a0\u21a9</p> </li> </ol>"},{"location":"modelpool/#fusion_bench.modelpool.BaseModelPool","title":"<code>BaseModelPool</code>","text":"<p>               Bases: <code>BaseYAMLSerializableModel</code></p> <p>A class for managing and interacting with a pool of models along with their associated datasets or other specifications. For example, a model pool may contain multiple models, each with its own training, validation, and testing datasets. As for the specifications, a vision model pool may contain image preprocessor, and a language model pool may contain a tokenizer.</p> <p>Attributes:</p> <ul> <li> <code>_models</code>               (<code>DictConfig</code>)           \u2013            <p>Configuration for all models in the pool.</p> </li> <li> <code>_train_datasets</code>               (<code>Optional[DictConfig]</code>)           \u2013            <p>Configuration for training datasets.</p> </li> <li> <code>_val_datasets</code>               (<code>Optional[DictConfig]</code>)           \u2013            <p>Configuration for validation datasets.</p> </li> <li> <code>_test_datasets</code>               (<code>Optional[DictConfig]</code>)           \u2013            <p>Configuration for testing datasets.</p> </li> <li> <code>_usage_</code>               (<code>Optional[str]</code>)           \u2013            <p>Optional usage information.</p> </li> <li> <code>_version_</code>               (<code>Optional[str]</code>)           \u2013            <p>Optional version information.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/base_pool.py</code> <pre><code>class BaseModelPool(BaseYAMLSerializableModel):\n    \"\"\"\n    A class for managing and interacting with a pool of models along with their associated datasets or other specifications. For example, a model pool may contain multiple models, each with its own training, validation, and testing datasets. As for the specifications, a vision model pool may contain image preprocessor, and a language model pool may contain a tokenizer.\n\n    Attributes:\n        _models (DictConfig): Configuration for all models in the pool.\n        _train_datasets (Optional[DictConfig]): Configuration for training datasets.\n        _val_datasets (Optional[DictConfig]): Configuration for validation datasets.\n        _test_datasets (Optional[DictConfig]): Configuration for testing datasets.\n        _usage_ (Optional[str]): Optional usage information.\n        _version_ (Optional[str]): Optional version information.\n    \"\"\"\n\n    _program = None\n    _models: Union[DictConfig, Dict[str, nn.Module]]\n    _config_mapping = BaseYAMLSerializableModel._config_mapping | {\n        \"_models\": \"models\",\n        \"_train_datasets\": \"train_datasets\",\n        \"_val_datasets\": \"val_datasets\",\n        \"_test_datasets\": \"test_datasets\",\n    }\n\n    def __init__(\n        self,\n        models: Union[DictConfig, Dict[str, nn.Module], List[nn.Module]],\n        *,\n        train_datasets: Optional[DictConfig] = None,\n        val_datasets: Optional[DictConfig] = None,\n        test_datasets: Optional[DictConfig] = None,\n        **kwargs,\n    ):\n        if isinstance(models, List):\n            models = {str(model_idx): model for model_idx, model in enumerate(models)}\n        self._models = models\n        self._train_datasets = train_datasets\n        self._val_datasets = val_datasets\n        self._test_datasets = test_datasets\n        super().__init__(**kwargs)\n\n    @property\n    def has_pretrained(self):\n        \"\"\"\n        Check if the model pool contains a pretrained model.\n\n        Returns:\n            bool: True if a pretrained model is available, False otherwise.\n        \"\"\"\n        return \"_pretrained_\" in self._models\n\n    @property\n    def all_model_names(self) -&gt; List[str]:\n        \"\"\"\n        Get the names of all models in the pool, including special models.\n\n        Returns:\n            List[str]: A list of all model names.\n        \"\"\"\n        return [name for name in self._models]\n\n    @property\n    def model_names(self) -&gt; List[str]:\n        \"\"\"\n        Get the names of regular models, excluding special models.\n\n        Returns:\n            List[str]: A list of regular model names.\n        \"\"\"\n        return [name for name in self._models if not self.is_special_model(name)]\n\n    @property\n    def train_dataset_names(self) -&gt; List[str]:\n        \"\"\"\n        Get the names of training datasets.\n\n        Returns:\n            List[str]: A list of training dataset names.\n        \"\"\"\n        return (\n            list(self._train_datasets.keys())\n            if self._train_datasets is not None\n            else []\n        )\n\n    @property\n    def val_dataset_names(self) -&gt; List[str]:\n        \"\"\"\n        Get the names of validation datasets.\n\n        Returns:\n            List[str]: A list of validation dataset names.\n        \"\"\"\n        return list(self._val_datasets.keys()) if self._val_datasets is not None else []\n\n    @property\n    def test_dataset_names(self) -&gt; List[str]:\n        \"\"\"\n        Get the names of testing datasets.\n\n        Returns:\n            List[str]: A list of testing dataset names.\n        \"\"\"\n        return (\n            list(self._test_datasets.keys()) if self._test_datasets is not None else []\n        )\n\n    def __len__(self):\n        return len(self.model_names)\n\n    @staticmethod\n    def is_special_model(model_name: str):\n        \"\"\"\n        Determine if a model is special based on its name.\n\n        Args:\n            model_name (str): The name of the model.\n\n        Returns:\n            bool: True if the model name indicates a special model, False otherwise.\n        \"\"\"\n        return model_name.startswith(\"_\") and model_name.endswith(\"_\")\n\n    def get_model_config(self, model_name: str, return_copy: bool = True) -&gt; DictConfig:\n        \"\"\"\n        Get the configuration for the specified model.\n\n        Args:\n            model_name (str): The name of the model.\n\n        Returns:\n            DictConfig: The configuration for the specified model.\n        \"\"\"\n        model_config = self._models[model_name]\n        if return_copy:\n            model_config = deepcopy(model_config)\n        return model_config\n\n    def load_model(\n        self, model_name_or_config: Union[str, DictConfig], *args, **kwargs\n    ) -&gt; nn.Module:\n        \"\"\"\n        Load a model from the pool based on the provided configuration.\n\n        Args:\n            model (Union[str, DictConfig]): The model name or configuration.\n\n        Returns:\n            nn.Module: The instantiated model.\n        \"\"\"\n        log.debug(f\"Loading model: {model_name_or_config}\", stacklevel=2)\n        if isinstance(self._models, DictConfig):\n            model_config = (\n                self._models[model_name_or_config]\n                if isinstance(model_name_or_config, str)\n                else model_name_or_config\n            )\n            model = instantiate(model_config, *args, **kwargs)\n        elif isinstance(self._models, Dict) and isinstance(model_name_or_config, str):\n            model = self._models[model_name_or_config]\n        else:\n            raise ValueError(\n                \"The model pool configuration is not in the expected format.\"\n                f\"We expected a DictConfig or Dict, but got {type(self._models)}.\"\n            )\n        return model\n\n    def load_pretrained_model(self, *args, **kwargs):\n        assert (\n            self.has_pretrained\n        ), \"No pretrained model available. Check `_pretrained_` is in the `models` key.\"\n        model = self.load_model(\"_pretrained_\", *args, **kwargs)\n        return model\n\n    def load_pretrained_or_first_model(self, *args, **kwargs):\n        \"\"\"\n        Load the pretrained model if available, otherwise load the first available model.\n\n        Returns:\n            nn.Module: The loaded model.\n        \"\"\"\n        if self.has_pretrained:\n            model = self.load_model(\"_pretrained_\", *args, **kwargs)\n        else:\n            model = self.load_model(self.model_names[0], *args, **kwargs)\n        return model\n\n    def models(self):\n        for model_name in self.model_names:\n            yield self.load_model(model_name)\n\n    def named_models(self):\n        for model_name in self.model_names:\n            yield model_name, self.load_model(model_name)\n\n    def load_train_dataset(self, dataset_name: str, *args, **kwargs) -&gt; Dataset:\n        \"\"\"\n        Load the training dataset for the specified model.\n\n        Args:\n            dataset_name (str): The name of the model.\n\n        Returns:\n            Dataset: The instantiated training dataset.\n        \"\"\"\n        return instantiate(self._train_datasets[dataset_name], *args, **kwargs)\n\n    def train_datasets(self):\n        for dataset_name in self.train_dataset_names:\n            yield self.load_train_dataset(dataset_name)\n\n    def load_val_dataset(self, dataset_name: str, *args, **kwargs) -&gt; Dataset:\n        \"\"\"\n        Load the validation dataset for the specified model.\n\n        Args:\n            dataset_name (str): The name of the model.\n\n        Returns:\n            Dataset: The instantiated validation dataset.\n        \"\"\"\n        return instantiate(self._val_datasets[dataset_name], *args, **kwargs)\n\n    def val_datasets(self):\n        for dataset_name in self.val_dataset_names:\n            yield self.load_val_dataset(dataset_name)\n\n    def load_test_dataset(self, dataset_name: str, *args, **kwargs) -&gt; Dataset:\n        \"\"\"\n        Load the testing dataset for the specified model.\n\n        Args:\n            dataset_name (str): The name of the model.\n\n        Returns:\n            Dataset: The instantiated testing dataset.\n        \"\"\"\n        return instantiate(self._test_datasets[dataset_name], *args, **kwargs)\n\n    def test_datasets(self):\n        for dataset_name in self.test_dataset_names:\n            yield self.load_test_dataset(dataset_name)\n\n    def save_model(self, model: nn.Module, path: str):\n        \"\"\"\n        Save the state dictionary of the model to the specified path.\n\n        Args:\n            model (nn.Module): The model whose state dictionary is to be saved.\n            path (str): The path where the state dictionary will be saved.\n        \"\"\"\n        with timeit_context(f\"Saving the state dict of model to {path}\"):\n            torch.save(model.state_dict(), path)\n</code></pre>"},{"location":"modelpool/#fusion_bench.modelpool.BaseModelPool.all_model_names","title":"<code>all_model_names</code>  <code>property</code>","text":"<p>Get the names of all models in the pool, including special models.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: A list of all model names.</p> </li> </ul>"},{"location":"modelpool/#fusion_bench.modelpool.BaseModelPool.has_pretrained","title":"<code>has_pretrained</code>  <code>property</code>","text":"<p>Check if the model pool contains a pretrained model.</p> <p>Returns:</p> <ul> <li> <code>bool</code>          \u2013            <p>True if a pretrained model is available, False otherwise.</p> </li> </ul>"},{"location":"modelpool/#fusion_bench.modelpool.BaseModelPool.model_names","title":"<code>model_names</code>  <code>property</code>","text":"<p>Get the names of regular models, excluding special models.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: A list of regular model names.</p> </li> </ul>"},{"location":"modelpool/#fusion_bench.modelpool.BaseModelPool.test_dataset_names","title":"<code>test_dataset_names</code>  <code>property</code>","text":"<p>Get the names of testing datasets.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: A list of testing dataset names.</p> </li> </ul>"},{"location":"modelpool/#fusion_bench.modelpool.BaseModelPool.train_dataset_names","title":"<code>train_dataset_names</code>  <code>property</code>","text":"<p>Get the names of training datasets.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: A list of training dataset names.</p> </li> </ul>"},{"location":"modelpool/#fusion_bench.modelpool.BaseModelPool.val_dataset_names","title":"<code>val_dataset_names</code>  <code>property</code>","text":"<p>Get the names of validation datasets.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: A list of validation dataset names.</p> </li> </ul>"},{"location":"modelpool/#fusion_bench.modelpool.BaseModelPool.get_model_config","title":"<code>get_model_config(model_name, return_copy=True)</code>","text":"<p>Get the configuration for the specified model.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DictConfig</code> (              <code>DictConfig</code> )          \u2013            <p>The configuration for the specified model.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/base_pool.py</code> <pre><code>def get_model_config(self, model_name: str, return_copy: bool = True) -&gt; DictConfig:\n    \"\"\"\n    Get the configuration for the specified model.\n\n    Args:\n        model_name (str): The name of the model.\n\n    Returns:\n        DictConfig: The configuration for the specified model.\n    \"\"\"\n    model_config = self._models[model_name]\n    if return_copy:\n        model_config = deepcopy(model_config)\n    return model_config\n</code></pre>"},{"location":"modelpool/#fusion_bench.modelpool.BaseModelPool.get_model_config(model_name)","title":"<code>model_name</code>","text":"(<code>str</code>)           \u2013            <p>The name of the model.</p>"},{"location":"modelpool/#fusion_bench.modelpool.BaseModelPool.is_special_model","title":"<code>is_special_model(model_name)</code>  <code>staticmethod</code>","text":"<p>Determine if a model is special based on its name.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>bool</code>          \u2013            <p>True if the model name indicates a special model, False otherwise.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/base_pool.py</code> <pre><code>@staticmethod\ndef is_special_model(model_name: str):\n    \"\"\"\n    Determine if a model is special based on its name.\n\n    Args:\n        model_name (str): The name of the model.\n\n    Returns:\n        bool: True if the model name indicates a special model, False otherwise.\n    \"\"\"\n    return model_name.startswith(\"_\") and model_name.endswith(\"_\")\n</code></pre>"},{"location":"modelpool/#fusion_bench.modelpool.BaseModelPool.is_special_model(model_name)","title":"<code>model_name</code>","text":"(<code>str</code>)           \u2013            <p>The name of the model.</p>"},{"location":"modelpool/#fusion_bench.modelpool.BaseModelPool.load_model","title":"<code>load_model(model_name_or_config, *args, **kwargs)</code>","text":"<p>Load a model from the pool based on the provided configuration.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>nn.Module: The instantiated model.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/base_pool.py</code> <pre><code>def load_model(\n    self, model_name_or_config: Union[str, DictConfig], *args, **kwargs\n) -&gt; nn.Module:\n    \"\"\"\n    Load a model from the pool based on the provided configuration.\n\n    Args:\n        model (Union[str, DictConfig]): The model name or configuration.\n\n    Returns:\n        nn.Module: The instantiated model.\n    \"\"\"\n    log.debug(f\"Loading model: {model_name_or_config}\", stacklevel=2)\n    if isinstance(self._models, DictConfig):\n        model_config = (\n            self._models[model_name_or_config]\n            if isinstance(model_name_or_config, str)\n            else model_name_or_config\n        )\n        model = instantiate(model_config, *args, **kwargs)\n    elif isinstance(self._models, Dict) and isinstance(model_name_or_config, str):\n        model = self._models[model_name_or_config]\n    else:\n        raise ValueError(\n            \"The model pool configuration is not in the expected format.\"\n            f\"We expected a DictConfig or Dict, but got {type(self._models)}.\"\n        )\n    return model\n</code></pre>"},{"location":"modelpool/#fusion_bench.modelpool.BaseModelPool.load_model(model)","title":"<code>model</code>","text":"(<code>Union[str, DictConfig]</code>)           \u2013            <p>The model name or configuration.</p>"},{"location":"modelpool/#fusion_bench.modelpool.BaseModelPool.load_pretrained_or_first_model","title":"<code>load_pretrained_or_first_model(*args, **kwargs)</code>","text":"<p>Load the pretrained model if available, otherwise load the first available model.</p> <p>Returns:</p> <ul> <li>           \u2013            <p>nn.Module: The loaded model.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/base_pool.py</code> <pre><code>def load_pretrained_or_first_model(self, *args, **kwargs):\n    \"\"\"\n    Load the pretrained model if available, otherwise load the first available model.\n\n    Returns:\n        nn.Module: The loaded model.\n    \"\"\"\n    if self.has_pretrained:\n        model = self.load_model(\"_pretrained_\", *args, **kwargs)\n    else:\n        model = self.load_model(self.model_names[0], *args, **kwargs)\n    return model\n</code></pre>"},{"location":"modelpool/#fusion_bench.modelpool.BaseModelPool.load_test_dataset","title":"<code>load_test_dataset(dataset_name, *args, **kwargs)</code>","text":"<p>Load the testing dataset for the specified model.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Dataset</code> (              <code>Dataset</code> )          \u2013            <p>The instantiated testing dataset.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/base_pool.py</code> <pre><code>def load_test_dataset(self, dataset_name: str, *args, **kwargs) -&gt; Dataset:\n    \"\"\"\n    Load the testing dataset for the specified model.\n\n    Args:\n        dataset_name (str): The name of the model.\n\n    Returns:\n        Dataset: The instantiated testing dataset.\n    \"\"\"\n    return instantiate(self._test_datasets[dataset_name], *args, **kwargs)\n</code></pre>"},{"location":"modelpool/#fusion_bench.modelpool.BaseModelPool.load_test_dataset(dataset_name)","title":"<code>dataset_name</code>","text":"(<code>str</code>)           \u2013            <p>The name of the model.</p>"},{"location":"modelpool/#fusion_bench.modelpool.BaseModelPool.load_train_dataset","title":"<code>load_train_dataset(dataset_name, *args, **kwargs)</code>","text":"<p>Load the training dataset for the specified model.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Dataset</code> (              <code>Dataset</code> )          \u2013            <p>The instantiated training dataset.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/base_pool.py</code> <pre><code>def load_train_dataset(self, dataset_name: str, *args, **kwargs) -&gt; Dataset:\n    \"\"\"\n    Load the training dataset for the specified model.\n\n    Args:\n        dataset_name (str): The name of the model.\n\n    Returns:\n        Dataset: The instantiated training dataset.\n    \"\"\"\n    return instantiate(self._train_datasets[dataset_name], *args, **kwargs)\n</code></pre>"},{"location":"modelpool/#fusion_bench.modelpool.BaseModelPool.load_train_dataset(dataset_name)","title":"<code>dataset_name</code>","text":"(<code>str</code>)           \u2013            <p>The name of the model.</p>"},{"location":"modelpool/#fusion_bench.modelpool.BaseModelPool.load_val_dataset","title":"<code>load_val_dataset(dataset_name, *args, **kwargs)</code>","text":"<p>Load the validation dataset for the specified model.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Dataset</code> (              <code>Dataset</code> )          \u2013            <p>The instantiated validation dataset.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/base_pool.py</code> <pre><code>def load_val_dataset(self, dataset_name: str, *args, **kwargs) -&gt; Dataset:\n    \"\"\"\n    Load the validation dataset for the specified model.\n\n    Args:\n        dataset_name (str): The name of the model.\n\n    Returns:\n        Dataset: The instantiated validation dataset.\n    \"\"\"\n    return instantiate(self._val_datasets[dataset_name], *args, **kwargs)\n</code></pre>"},{"location":"modelpool/#fusion_bench.modelpool.BaseModelPool.load_val_dataset(dataset_name)","title":"<code>dataset_name</code>","text":"(<code>str</code>)           \u2013            <p>The name of the model.</p>"},{"location":"modelpool/#fusion_bench.modelpool.BaseModelPool.save_model","title":"<code>save_model(model, path)</code>","text":"<p>Save the state dictionary of the model to the specified path.</p> <p>Parameters:</p> Source code in <code>fusion_bench/modelpool/base_pool.py</code> <pre><code>def save_model(self, model: nn.Module, path: str):\n    \"\"\"\n    Save the state dictionary of the model to the specified path.\n\n    Args:\n        model (nn.Module): The model whose state dictionary is to be saved.\n        path (str): The path where the state dictionary will be saved.\n    \"\"\"\n    with timeit_context(f\"Saving the state dict of model to {path}\"):\n        torch.save(model.state_dict(), path)\n</code></pre>"},{"location":"modelpool/#fusion_bench.modelpool.BaseModelPool.save_model(model)","title":"<code>model</code>","text":"(<code>Module</code>)           \u2013            <p>The model whose state dictionary is to be saved.</p>"},{"location":"modelpool/#fusion_bench.modelpool.BaseModelPool.save_model(path)","title":"<code>path</code>","text":"(<code>str</code>)           \u2013            <p>The path where the state dictionary will be saved.</p>"},{"location":"modelpool/clip_vit/","title":"CLIP-ViT Models for Open Vocabulary Image Classification","text":"<p>Here we provides a list of CLIP-ViT models that are trained for open vocabulary image classification. </p>"},{"location":"modelpool/clip_vit/#the-eight-tasks","title":"The Eight Tasks","text":"<p>The most common eight tasks used in the research community are SUN397, Cars, RESISC45, EuroSAT, SVHN, GTSRB, MNIST, and DTD. These tasks cover a wide range of domains, including natural images, satellite images, and digit recognition. You can download the datasets from this HuggingFace Collection or using the <code>datasets</code> library as follows:</p> <pre><code>from datasets import load_dataset\n\n# take `gtsrb` as an example\ndataset = load_dataset(\"tanganke/gtsrb\")\n\ntrain_dataset = dataset[\"train\"]\ntest_dataset = dataset[\"test\"]\n</code></pre> <p>The authors of Task Arithmetic have fine-tuned the CLIP-ViT models from the open_clip library on these eight tasks and provide the models publicly on Google Drive.  However, these models rely on a specific version of the open_clip library. </p> <p>To make experiments more convenient and avoid dependency on a specific library version, we have re-trained these models and made them publicly available on the HuggingFace Model Hub. We use the Adam Optimizer with a fixed learning rate of 1e-5 over 4000 training steps (batch_size=32). Only the vision encoder is fine-tuned, while the text encoder remains fixed to preserve the open-vocabulary property of the model.</p> <ul> <li>fine-tuned CLIP-ViT-B/32 models</li> <li>fine-tuned CLIP-ViT-B/16 models</li> <li>fine-tuned CLIP-ViT-L/14 models</li> </ul> <p>To use these models, you can load them from the Transformers library as follows:</p> <p>load vision backbone</p> <pre><code>from transformers import CLIPVisionModel\n\n# load the CLIP-ViT-B/32 model, take `gtsrb` as an example\nvision_model = CLIPVisionModel.from_pretrained('tanganke/clip-vit-base-patch32_gtsrb')\n</code></pre> <p>substitute the vision encoder of clip</p> <pre><code>from transformers import CLIPProcessor, CLIPModel\n\n# load pre-trained CLIP model\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n# substitute the vision model with the fine-tuned one\nclip_model.vision_model.load_state_dict(vision_model.vision_model.state_dict())\n</code></pre>"},{"location":"modelpool/clip_vit/#performance-of-the-fine-tuned-models","title":"Performance of the Fine-tuned Models","text":"<p>evaluate the fine-tuned CLIP-ViT-B/32 models on the eight tasks:</p> <pre><code># evaluate singlue fine-tuned models\nfor task in sun397 stanford-cars resisc45 eurosat svhn gtsrb mnist dtd\ndo\n    fusion_bench method=dummy \\\n        modelpool=clip-vit-base-patch32_individual \\\n            modelpool.models.0.path=tanganke/clip-vit-base-patch32_${task} \\\n        taskpool=clip-vit-classification_TA8 \\\n        report_save_path=\"outputs/ViT-B-32/single-task/clip-vit-base-patch32_${task}.json\"\ndone\n</code></pre> <p>evaluate the fine-tuned CLIP-ViT-L/14 models on the eight tasks:</p> <pre><code># assume you have eight GPUs, and you can evaluate the models on the eight tasks in parallel\ntasks=(sun397 stanford-cars resisc45 eurosat svhn gtsrb mnist dtd)\nCUDA_DEVICES=(0 1 2 3 4 5 6 7)  # List of CUDA devices to use\n\nfor i in \"${!CUDA_DEVICES[@]}\"; do\n    task=${tasks[$i]}\n    CUDA_VISIBLE_DEVICES=${CUDA_DEVICES[$i]} fusion_bench method=dummy \\\n        modelpool=clip-vit-large-patch14_individual \\\n            modelpool.models.0.path=tanganke/clip-vit-large-patch14_${task} \\\n        taskpool=clip-vit-classification_TA8 \\\n            taskpool.clip_model=openai/clip-vit-large-patch14 \\\n        report_save_path=\"outputs/ViT-L-14/single-task/clip-vit-large-patch14_${task}.json\" &amp;\ndone\n</code></pre> Performance of the fine-tuned CLIP-ViT-B/32 modelsPerformance of the fine-tuned CLIP-ViT-B/16 modelsPerformance of the fine-tuned CLIP-ViT-L/14 models Model SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD Average Pre-trained 63.2 59.8 60.7 46.0 31.6 32.5 48.3 43.9 48.2 SUN397 75.0 47.0 54.3 46.5 28.3 26.4 44.3 41.6 45.4 Cars 56.6 78.3 50.9 38.4 30.2 30.6 49.7 41.8 47.1 RESISC45 52.0 47.2 95.2 56.9 23.9 24.3 39.7 35.9 46.9 EuroSAT 49.0 39.9 33.5 99.0 11.8 22.9 33.8 35.5 40.7 SVHN 40.5 36.3 18.9 9.8 97.3 27.3 81.8 23.2 41.9 GTSRB 36.8 33.0 20.6 21.3 41.2 98.9 30.9 23.9 38.3 MNIST 50.3 40.0 31.3 17.7 50.1 19.3 99.6 30.7 42.4 DTD 54.6 51.3 36.9 25.0 28.9 21.8 47.3 79.7 43.2 Model SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD Average SUN397 78.9 56.2 58.9 46.6 42.7 39.9 59.3 40.8 52.9 Cars 62.2 85.9 60.8 48.7 47.1 44.8 61.6 43.2 56.8 RESISC45 60.5 57.8 96.6 65.7 28.4 35.6 71.5 39.0 56.9 EuroSAT 58.3 59.2 37.4 99.0 40.5 38.9 57.4 37.7 53.6 SVHN 57.6 55.4 42.8 19.6 97.6 32.6 90.0 33.1 53.6 GTSRB 54.0 50.5 25.3 13.2 52.0 99.0 56.9 33.9 48.1 MNIST 58.7 52.4 47.0 23.6 65.0 27.6 99.7 37.7 51.5 DTD 57.7 58.1 53.5 43.0 44.2 36.2 70.4 82.3 55.7 Model SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD Average Pre-trained 68.3 77.8 71.0 58.9 58.4 50.6 76.4 55.5 64.6 SUN397 82.8 68.4 58.1 49.9 55.0 46.3 79.5 52.8 61.6 Cars 67.8 92.9 68.7 56.4 51.7 47.7 80.5 55.6 65.2 RESISC45 65.6 69.0 97.4 64.3 38.3 46.6 77.7 49.9 63.6 EuroSAT 65.2 69.0 40.6 99.2 33.4 45.6 73.5 47.1 59.2 SVHN 66.4 69.0 54.0 19.7 97.9 48.7 92.2 50.1 62.3 GTSRB 63.4 64.8 38.7 19.6 71.0 99.2 75.1 45.8 59.7 MNIST 56.0 49.8 53.5 26.6 48.2 33.1 99.8 47.1 51.7 DTD 66.8 75.3 65.5 43.7 49.5 45.0 68.5 85.5 62.5"},{"location":"modelpool/clip_vit/#model-pool-configuration","title":"Model Pool Configuration","text":"<p>To use these models from our FusionBench library, you can specify the modelpool configuration file as follows:</p> config/modelpool/CLIPVisionModelPool/clip-vit-base-patch32_TA8.yaml<pre><code>defaults:\n  - CLIPVisionModelPool@: _template\n  - /model/clip-vit@models: clip-vit-base-patch32_eight_tasks\n  - /dataset/image_classification/train@train_datasets: the_eight_tasks\n  - /dataset/image_classification/test@test_datasets: the_eight_tasks\n</code></pre> <p>The configuration uses YAML's inheritance feature with the defaults key. It inherits from a template (<code>_template.yaml</code>) and overrides specific values. Some values are set to <code>???</code> or null, indicating that they need to be specified or can be optionally set when using this configuration. This configuration structure allows for modular and reusable setups, making it easier to manage different model configurations within the FusionBench library.</p> config/modelpool/CLIPVisionModelPool/_template.yaml<pre><code>_usage_: |\n  defaults:\n    - CLIPVisionModelPool@: _template\n_target_: fusion_bench.modelpool.CLIPVisionModelPool\n_version_: \"0.2\"\n_recursive_: False\nmodels: ???\ntrain_datasets: null\ntest_datasets: null\nprocessor:\n  _target_: transformers.CLIPProcessor.from_pretrained\n  pretrained_model_name_or_path: openai/clip-vit-base-patch32\n</code></pre> <p>The type of the modelpool is <code>fusion_bench.modelpool.CLIPVisionModelPool</code>.</p>"},{"location":"modelpool/clip_vit/#lora-and-l-lora","title":"LoRA and L-LoRA","text":"<p>Here we fine-tuned the CLIP-ViT-B/16 models on the eight image classification tasks using the LoRA and L-LoRA methods. <code>q_proj</code> and <code>v_proj</code> are fine-tuned with a learning rate of 1e-5 using the Adam optimizer for 2000 steps. You can find the script for fine-tuning the models at <code>examples/clip_finetune/clip_finetune.sh</code>.</p> <ul> <li>CLIP-ViT-B/16 on the eight image classification tasks (LoRA)</li> <li>CLIP-ViT-B/16 on eight image classification tasks (L-LoRA)</li> </ul> <p>Load LoRA models (see load_lora_vision_model_hf):</p> <pre><code>base_model = CLIPVisionModel.from_pretrained('openai/clip-vit-base-patch16').vision_model\nmodel = PeftModel.from_pretrained(base_model, peft_model_id)\n</code></pre> <p>Load L-LoRA models, refer to load_l_lora_vision_model_hf.</p> Performance of the fine-tuned CLIP-ViT-B/16 models (LoRA-16)Performance of the fine-tuned CLIP-ViT-B/16 models (L-LoRA-16) Model SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD Average SUN397 70.8 64.8 66.7 55.4 51.8 44.0 52.2 46.0 56.5 Cars 65.8 72.3 65.7 54.5 52.3 44.1 54.1 45.3 56.8 RESISC45 66.2 64.6 88.9 65.4 51.8 43.6 54.7 45.6 60.1 EuroSAT 65.6 64.6 59.4 97.1 48.2 43.6 60.5 46.0 60.6 SVHN 65.5 64.1 65.3 39.1 93.2 45.5 83.0 45.1 62.6 GTSRB 65.5 63.9 64.2 28.6 56.9 91.0 71.3 45.5 60.9 MNIST 65.3 64.1 65.7 51.9 57.6 46.6 98.4 45.2 61.9 DTD 64.5 64.2 61.0 49.1 54.2 44.2 68.0 67.9 59.1 Model SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD Average SUN397 69.0 65.0 66.7 56.0 52.6 44.0 53.3 45.4 56.5 Cars 65.8 69.7 65.7 54.4 52.0 43.7 52.6 45.3 56.2 RESISC45 65.9 64.3 83.6 66.3 51.7 43.4 51.9 45.6 59.1 EuroSAT 65.5 64.8 64.2 95.4 50.7 43.8 58.4 45.9 61.1 SVHN 65.3 64.4 65.2 46.6 90.1 45.8 80.0 45.4 62.9 GTSRB 65.5 64.4 64.2 43.8 59.5 78.6 72.6 45.2 61.7 MNIST 65.3 64.5 65.0 53.3 57.6 45.6 96.4 45.5 61.7 DTD 65.7 64.7 65.9 54.5 51.6 44.4 58.2 56.2 57.7 <p> </p>"},{"location":"modelpool/clip_vit/#basic-examples","title":"Basic Examples","text":"<p>Here are some basic examples of using the CLIP-ViT models for open vocabulary image classification with different fusion methods, using the <code>fusion_bench</code> command line interface.</p>"},{"location":"modelpool/clip_vit/#inspection-of-model-information","title":"Inspection of Model Information","text":"<p>Print the basic information of the CLIP-ViT-B/32 model and CLIP-ViT-L/14 model</p> <pre><code>fusion_bench \\\n  method=dummy \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_individual \\\n  taskpool=dummy  # a dummy task that just report the basic information of model (e.g., number of parameters)\n\n# Output:\n# {'model_info': {'trainable_params': 87456000, 'all_params': 87456000, 'trainable_percentage': 1.0}}\n\n# or use the following command to inspect the CLIP-ViT-L/14 model\nfusion_bench \\\n  method=dummy \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_individual \\\n  taskpool=dummy\n\n# Output:\n# {'model_info': {'trainable_params': 303179776, 'all_params': 303179776, 'trainable_percentage': 1.0}}\n</code></pre>"},{"location":"modelpool/clip_vit/#single-model-evaluation","title":"Single Model Evaluation","text":"<p>evaluate a single CLIP-ViT-B/32 model on the eight downstream tasks:</p> <pre><code>path_to_clip_model=\"tanganke/clip-vit-base-patch32_sun397\"\n\nfusion_bench \\\n  method=dummy \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_individual \\\n    modelpool.models._pretrained_.pretrained_model_name_or_path=\"'${path_to_clip_model}'\" \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre> <p>Here:</p> <ul> <li>The <code>dummy</code> method is a special method used to skip the model merging process, it loads the pre-trained model in the modelpool and return the model without any modification (or the first model when a model with the name <code>_pretrained_</code> does not exist in modelpool), see dummy method for more information. </li> <li>The <code>CLIPVisionModelPool/clip-vit-base-patch32_individual</code> modelpool contains a single model. By passing argument <code>modelpool.models.0.path=...</code>, we override the path of the model with the specified path. config/modelpool/CLIPVisionModelPool/clip-vit-base-patch32_individual.yaml<pre><code>defaults:\n  - CLIPVisionModelPool@: _template\n\nmodels:\n  _pretrained_:\n    _target_: transformers.CLIPVisionModel.from_pretrained\n    pretrained_model_name_or_path: ${...base_model}\n\nprocessor:\n  _target_: transformers.CLIPProcessor.from_pretrained\n  pretrained_model_name_or_path: ${..base_model}\n\nbase_model: openai/clip-vit-base-patch32\n</code></pre></li> <li>The <code>CLIPVisionModelTaskPool/clip-vit-classification_TA8</code> taskpool is used to evaluate the model on the eight tasks.   if <code>$path_to_clip_model</code> is not specified, the pre-trained model from HuggingFace will be used by default. config/taskpool/CLIPVisionModelTaskPool/clip-vit-classification_TA8.yaml<pre><code>defaults:\n  - CLIPVisionModelTaskPool@: _template\n  - /dataset/image_classification/test@test_datasets:\n      - sun397\n      - stanford-cars\n      - resisc45\n      - eurosat\n      - svhn\n      - gtsrb\n      - mnist\n      - dtd\n</code></pre></li> </ul> <p>Use a for loop to evaluate multiple CLIP-ViT-B/32 model on the eight tasks, and save reports to json files:</p> <pre><code>for task in sun397 stanford-cars resisc45 eurosat svhn gtsrb mnist dtd\ndo\n    fusion_bench method=dummy \\\n        modelpool=CLIPVisionModelPool/clip-vit-base-patch32_individual \\\n            modelpool.models._pretrained_.pretrained_model_name_or_path=tanganke/clip-vit-base-patch32_${task} \\\n        taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \\\n        report_save_path=\"outputs/ViT-B-32/single-task/clip-vit-base-patch32_${task}.json\"\ndone\n</code></pre> <p>evaluate the CLIP-ViT-L/14 model on the eight tasks</p> <pre><code>fusion_bench method=dummy \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_individual \\\n    modelpool.models._pretrained_.pretrained_model_name_or_path=\"'${path_to_clip_model}'\" \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8_L14\n</code></pre>"},{"location":"modelpool/clip_vit/#simple-averaging","title":"Simple Averaging","text":"<p>merge CLIP-ViT-B/32 models using simple average and evaluate on the eight tasks</p> <pre><code>fusion_bench \\\n  method=simple_average \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8_model_only \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \n\n# results\n{\n    \"svhn\": {\"accuracy\": 0.6451674699783325, \"loss\": 1.128771424293518},\n    \"stanford_cars\": {\"accuracy\": 0.625668466091156, \"loss\": 1.135254979133606},\n    \"resisc45\": {\"accuracy\": 0.7079365253448486, \"loss\": 0.9697789549827576},\n    \"eurosat\": {\"accuracy\": 0.7685185074806213, \"loss\": 0.6301173567771912},\n    \"gtsrb\": {\"accuracy\": 0.5494061708450317, \"loss\": 1.492265224456787},\n    \"mnist\": {\"accuracy\": 0.8626000285148621, \"loss\": 0.5933865308761597},\n    \"dtd\": {\"accuracy\": 0.5090425610542297, \"loss\": 1.79731023311615},\n    \"sun397\": {\"accuracy\": 0.6543576717376709, \"loss\": 1.1993952989578247},\n}\n</code></pre> <p>merge CLIP-ViT-L/14 models using simple average and evaluate on the eight tasks</p> <pre><code>fusion_bench method=simple_average \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8_model_only \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8_L14\n</code></pre>"},{"location":"modelpool/clip_vit/#fisher-merging","title":"Fisher Merging","text":"<p>merge CLIP-ViT-B/32 models using Fisher Merging and evaluate on the eight tasks</p> <pre><code>fusion_bench \\\n  method=fisher_merging/clip_fisher_merging \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre> <p>merge CLIP-ViT-L/14 models using Fisher Merging and evaluate on the eight tasks</p> <pre><code>fusion_bench \\\n  method=fisher_merging/clip_fisher_merging \\\n    method.dataloader_kwargs.batch_size=8 method.dataloader_kwargs.num_workers=4 \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8_L14\n</code></pre>"},{"location":"modelpool/clip_vit/#regmean","title":"RegMean","text":"<p>merge CLIP-ViT-B/32 models using RegMean and evaluate on the eight tasks</p> <pre><code>fusion_bench method=regmean/clip_regmean \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre> <p>For CLIP-ViT-L/14 models:</p> <pre><code>fusion_bench \\\n  method=regmean/clip_regmean \\\n    method.dataloader_kwargs.batch_size=8 method.dataloader_kwargs.num_workers=4 \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8_L14\n</code></pre>"},{"location":"modelpool/clip_vit/#task-arithmetic","title":"Task Arithmetic","text":"<p>merge CLIP-ViT-B/32 models using task arithmetic and evaluate on the eight tasks</p> <pre><code>fusion_bench method=task_arithmetic method.scaling_factor=0.3\\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n\n# results\n{\n    \"svhn\": {\"accuracy\": 0.77927166223526, \"loss\": 0.7050645351409912},\n    \"stanford_cars\": {\"accuracy\": 0.5565228462219238, \"loss\": 1.4873239994049072},\n    \"resisc45\": {\"accuracy\": 0.6487301588058472, \"loss\": 1.3709946870803833},\n    \"eurosat\": {\"accuracy\": 0.7674074172973633, \"loss\": 0.6550557017326355},\n    \"gtsrb\": {\"accuracy\": 0.6850356459617615, \"loss\": 1.2349143028259277},\n    \"mnist\": {\"accuracy\": 0.9606999754905701, \"loss\": 0.1570172756910324},\n    \"dtd\": {\"accuracy\": 0.471808522939682, \"loss\": 2.1495635509490967},\n    \"sun397\": {\"accuracy\": 0.571083128452301, \"loss\": 1.7016042470932007},\n}\n</code></pre> <pre><code># or use a for loop to try different scaling factors \n# and save the results to different files\nfor scaling_factor in 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\ndo\n  fusion_bench \\\n    method=task_arithmetic method.scaling_factor=$scaling_factor \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \\\n    report_save_path=outputs/clip-vit-base-patch32_TA8_task_arithmetic_scaling_factor_${scaling_factor}.json\ndone\n</code></pre> <p>merge CLIP-ViT-L/14 models using task arithmetic and evaluate on the eight tasks</p> <pre><code>fusion_bench method=task_arithmetic method.scaling_factor=0.3\\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8_L14\n</code></pre>"},{"location":"modelpool/clip_vit/#ties-merging","title":"Ties-Merging","text":"<p>merge CLIP-ViT-B/32 models using Ties-Merging and evaluate on the eight tasks</p> <pre><code>fusion_bench method=ties_merging method.scaling_factor=0.3 method.threshold=20 \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre> <pre><code># or use a for loop to try different scaling factors\n# and save the results to different files\nfor scaling_factor in 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\ndo\n  fusion_bench \\\n    method=ties_merging method.scaling_factor=$scaling_factor method.threshold=20 \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \\\n    report_save_path=outputs/clip-vit-base-patch32_TA8_ties_merging_scaling_factor_${scaling_factor}.json\ndone\n</code></pre> <p>merge CLIP-ViT-L/14 models using Ties-Merging and evaluate on the eight tasks</p> <pre><code>fusion_bench method=ties_merging method.scaling_factor=0.3 method.threshold=20 \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8_L14\n</code></pre>"},{"location":"modelpool/clip_vit/#adamerging","title":"AdaMerging","text":"<p>merge CLIP-ViT-B/32 models using task-wise AdaMerging and evaluate on the eight tasks, and save the merging weights by specifying the <code>method.save_merging_weights</code> parameter</p> <pre><code>fusion_bench \\\n  method=adamerging \\\n    method.name=clip_task_wise_adamerging \\\n    method.save_merging_weights=outputs/clip-vit-base-patch32_TA8_task_wise_adamerging_weights.pt \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre> <p>merge CLIP-ViT-L/14 models using task-wise AdaMerging and evaluate on the eight tasks, and save the merging weights by specifying the <code>method.save_merging_weights</code> parameter. Here we split the training process into two stages, the first stage is to train the merging weights, and the second stage is to evaluate the model with the learned merging weights.</p> <pre><code># learn the merging weights.\n# the per-device batch size is 4, and the total batch size is 4*4=16\nfusion_bench print_config=false \\\n  method=adamerging \\\n    method.name=clip_task_wise_adamerging \\\n    method.save_merging_weights=outputs/clip-vit-large-patch14_TA8_task_wise_adamerging_weights.pt \\\n    method.devices=4 method.batch_size=4 \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8 \\\n  taskpool=dummy # dummy taskpool is used to skip the evaluation process\n\n# by specifying the learned merging weights, we skip the training process and directly evaluate the model\nfusion_bench print_config=false \\\n  method=adamerging \\\n    method.name=clip_task_wise_adamerging \\\n    method.weights=outputs/clip-vit-large-patch14_TA8_task_wise_adamerging_weights.pt \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8_L14\n</code></pre> <p>merge CLIP-ViT-B/32 models using layer-wise AdaMerging and evaluate on the eight tasks</p> <pre><code>fusion_bench \\\n    method=adamerging \\\n        method.name=clip_layer_wise_adamerging \\\n        method.save_merging_weights=merging_weights.pt \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n    taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8 \\\n    fabric.loggers.root_dir=outputs/logs/ViT-B-32 \\\n    fabric.loggers.name=clip_layer_wise_adamerging_adam\n</code></pre> <p>merge CLIP-ViT-L/14 models using layer-wise AdaMerging and evaluate on the eight tasks</p> <pre><code># learn the merging weights.\n# the per-device batch size is 4, and the total batch size is 4*4=16\nfusion_bench print_config=false \\\n  method=adamerging \\\n    method.name=clip_layer_wise_adamerging \\\n    method.save_merging_weights=outputs/clip-vit-large-patch14_TA8_layer_wise_adamerging_weights.pt \\\n    method.devices=4 method.batch_size=4 \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8 \\\n  taskpool=dummy # dummy taskpool is used to skip the evaluation process\n\n# by specifying the learned merging weights, we skip the training process and directly evaluate the model\nfusion_bench \\\n  method=adamerging \\\n    method.name=clip_layer_wise_adamerging \\\n    method.weights=outputs/clip-vit-large-patch14_TA8_layer_wise_adamerging_weights.pt \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8_L14\n</code></pre>"},{"location":"modelpool/clip_vit/#weight-ensembling-moe","title":"Weight-Ensembling MoE","text":"<p>fuse CLIP-ViT-B/32 models using Weight-Ensembling Mixture of Experts and evaluate on the eight tasks</p> <pre><code>fusion_bench \\\n  method=weight_ensembling_moe \\\n    method.name=clip_weight_ensembling_moe \\\n    method.use_grad_accumulate=false \\\n    method.save_checkpoint=outputs/clip-vit-base-patch32_TA8_weight_ensembling_moe_checkpoint.ckpt \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8\n</code></pre> <p>fuse CLIP-ViT-L/14 models using Weight-Ensembling Mixture of Experts and evaluate on the eight tasks</p> <pre><code># merge eight CLIP-ViT-L/14 models using WE MoE, fine-tune the routers\nfusion_bench print_config=false \\\n  method=weight_ensembling_moe \\\n    method.name=clip_weight_ensembling_moe \\\n    method.use_grad_accumulate=true \\\n    method.save_checkpoint=outputs/clip-vit-large-patch14_TA8_weight_ensembling_moe_checkpoint.ckpt \\\n    method.batch_size=4 method.devices=4 \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8 \\\n  taskpool=dummy &amp;&amp;\n\n# load the checkpoint and evaluate the model\nfusion_bench \\\n  method=weight_ensembling_moe \\\n    method.name=clip_weight_ensembling_moe \\\n    method.checkpoint=outputs/clip-vit-large-patch14_TA8_weight_ensembling_moe_checkpoint.ckpt \\\n  modelpool=CLIPVisionModelPool/clip-vit-large-patch14_TA8 \\\n  taskpool=CLIPVisionModelTaskPool/clip-vit-classification_TA8_L14\n</code></pre>"},{"location":"modelpool/clip_vit/#experimental-results","title":"Experimental Results","text":"<p>We provide the experimental results of the CLIP-ViT models for open vocabulary image classification on the eight tasks in the following table.</p> <p>Hyperparameters not fully optimized</p> <p>The hyperparameters used in these merging methods are not fully optimized and should be considered as preliminary results only. We welcome any discoveries of more effective parameters and would be grateful for your contributions to help us improve our results.</p> <p>Please note that some model merging paper results were obtained using OpenCLIP models, which may show discrepancies with the results presented here. In such cases, the results reported in the original papers should be considered authoritative.</p> Table: Multi-task model merging methods using CLIP-ViT-B/32 models.Table: Multi-task model merging methods using CLIP-ViT-B/16 models.Table: Multi-task model merging methods using CLIP-ViT-L/14 models. Method SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD Average Reference Results Pre-trained 63.2 59.8 60.7 46.0 31.6 32.5 48.2 43.9 48.2 Fine-tuned (STL) 75.0 78.3 95.2 99.0 97.3 98.9 99.6 79.7 90.3 Traditional MTL 72.3 76.6 92.2 97.9 95.5 97.7 99.3 77.7 88.6 Model Merging Simple Averaging 65.4 62.6 70.8 76.9 64.5 54.9 86.3 50.9 66.5 Fisher Merging 66.7 64.0 72.2 91.6 69.0 64.3 83.5 53.7 70.6 RegMean 67.8 68.9 82.5 94.4 90.6 79.2 97.6 63.2 80.5 Task Arithmetic (\\(\\lambda=0.3\\)) 57.1 55.7 64.9 76.7 77.9 68.5 96.1 47.2 68.0 Concrete Task Arithmetic (\\(\\lambda=0.3\\)) 64.2 63.3 75.6 94.1 90.3 82.9 98.0 52.5 77.6 Ties-Merging (\\(\\lambda=0.3\\)) 67.1 64.2 74.1 76.8 77.7 69.4 94.1 54.0 72.2 Task-wise AdaMerging (\\(\\lambda=0.3\\)) 58.6 56.9 69.8 82.4 70.3 58.9 97.2 55.3 68.7 Layer-wise AdaMerging (\\(\\lambda=0.3\\)) 67.9 71.3 83.5 92.7 87.4 92.9 98.2 67.0 82.6 Concrete Layer-wise AdaMerging (\\(\\lambda=0.3\\)) 69.1 72.7 85.9 94.7 91.3 95.7 98.7 66.8 84.4 Model Mixing Efficient Weight-Ensembling MoE (\\(90\\%\\)) 74.3 76.3 92.7 97.9 96.1 98.6 99.5 77.8 89.1 Weight-Ensembling MoE 73.7 76.8 93.4 98.2 96.8 98.2 99.6 76.6 89.2 Method SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD Average Reference Results Pre-trained 65.5 64.6 66.3 54.1 51.9 43.4 51.7 44.9 55.3 Fine-tuned (STL) 78.9 85.9 96.6 99.0 97.6 99.0 99.7 82.3 92.3 Model Merging Simple Averaging 68.7 69.0 75.0 83.2 74.9 62.5 93.7 51.1 72.3 Fisher Merging 70.8 71.8 76.2 93.4 77.4 61.2 90.7 52.3 74.2 RegMean 71.1 76.4 86.0 95.4 93.9 86.5 98.4 64.3 84.0 Task Arithmetic (\\(\\lambda=0.3\\)) 65.9 68.3 75.4 84.5 88.8 81.9 98.0 53.9 77.1 Ties-Merging (\\(\\lambda=0.3\\)) 70.6 71.2 79.8 87.5 83.2 76.2 96.4 55.4 77.5 Layer-wise AdaMerging (\\(\\lambda=0.3\\)) 70.6 79.6 86.1 93.6 93.5 95.4 98.1 62.9 85.0 Model Mixing Efficient Weight-Ensembling MoE (\\(90\\%\\)) 77.7 85.0 94.9 98.2 97.2 98.9 99.5 81.4 91.6 Weight-Ensembling MoE 77.2 85.0 94.8 98.3 97.3 98.9 99.6 80.8 91.5 Method SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD Average Reference Results Pre-trained 68.3 77.8 71.0 58.9 58.4 50.6 76.4 55.5 64.6 Fine-tuned (STL) 82.8 92.9 97.4 99.2 97.9 99.2 99.8 85.5 94.3 Traditional MTL 79.0 89.3 94.5 98.4 96.4 98.1 99.4 83.7 92.4 Model Merging Simple Averaging 72.5 81.5 82.2 90.0 81.6 74.0 96.6 61.8 80.0 Fisher Merging 70.6 79.4 84.1 98.1 74.7 85.0 89.5 61.0 80.3 RegMean 75.3 88.4 90.0 97.1 95.9 92.4 98.5 72.6 88.8 Task Arithmetic (\\(\\lambda=0.3\\)) 72.0 79.0 80.5 86.0 87.5 83.5 98.0 58.8 80.7 Ties-Merging (\\(\\lambda=0.3\\)) 74.7 83.3 86.4 91.3 89.7 85.2 97.8 63.9 84.0 Task-wise AdaMerging (\\(\\lambda=0.3\\)) 75.8 80.1 77.2 83.6 68.4 93.5 93.1 69.0 80.1 Layer-wise AdaMerging (\\(\\lambda=0.3\\)) 78.1 90.7 90.8 96.5 94.8 97.5 98.6 81.3 91.0 Model Mixing Efficient Weight-Ensembling MoE (\\(90\\%\\)) 81.5 92.0 96.0 97.8 97.7 99.1 99.5 84.1 93.5 Weight-Ensembling MoE 81.5 92.3 96.5 98.8 97.6 99.4 99.6 84.5 93.8"},{"location":"modelpool/clip_vit/#scope","title":"Scope","text":""},{"location":"modelpool/clip_vit/#task-vector-cosine-similarity","title":"Task Vector Cosine Similarity","text":"<p>Compute the cosine similarities between the task vectors and save the results to a CSV file.</p> <pre><code># CLIP-ViT-B/32 models\nfusion_bench \\\n  method=task_vector_cos_similarity \\\n    method.save_to_csv='outputs/clip-vit-base-patch32_cos.csv' \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n  taskpool=dummy  # do not evaluate the model\n\n# CLIP-ViT-L/14 models\nfusion_bench \\\n  method=task_vector_cos_similarity \\\n    method.save_to_csv='outputs/clip-vit-large-patch14_cos.csv' \\\n  modelpool=CLIPVisionModelPool/clip-vit-base-patch32_TA8 \\\n  tsakpool=dummy\n</code></pre> Cosine similarity matrices of task vectors for CLIP-ViT-B/32 and CLIP-ViT-L/14 models."},{"location":"modelpool/clip_vit/#generalization-and-robustness-evaluation","title":"Generalization and Robustness Evaluation","text":"<p>You can also evaluate the generalization and robustness of different multi-task model fusion methods by change the configurations.</p> <p>Instruction for running the generalization experiments:</p> <pre><code>fusion_bench \\\n    method=... \\\n    modelpool=CLIPVisionModelPool/clip-vit-base-patch32_generalization_exp1 # or `clip-vit-base-patch32_generalization_exp2`\n</code></pre> <p>Instruction for running the robustness experiments:</p> <pre><code># corription can be one of the following values: \n# contrast, gaussian_noise, impulse_noise, jpeg_compression, motion_blur, pixelate, spatter\n# or pass `taskpool=clip-vit-base-patch32_robustness_clean` to evaluate the model on clean data\ncorruption=contrast\nfusion_bench \\\n    --config-name clip-vit-base-patch32_robustness_corrupted \\\n    corruption=${corruption} \\\n    method=... \\\n</code></pre> <p>Below is an example of different types of corruptions:</p>  An example of corruption data visualization, in which the corruption image generation method refers to Hendrycks &amp; Dietterich (2019) <sup>1</sup>."},{"location":"modelpool/clip_vit/#experimental-results_1","title":"Experimental Results","text":"<p>Hyperparameters not fully optimized</p> <p>The hyperparameters used in these merging methods are not fully optimized and should be considered as preliminary results only. We welcome any discoveries of more effective parameters and would be grateful for your contributions to help us improve our results.</p> <p>Please note that some model merging paper results were obtained using OpenCLIP models, which may show discrepancies with the results presented here. In such cases, the results reported in the original papers should be considered authoritative.</p> Table: Results of the generalization experiments (Exp1).Table: Results of the generalization experiments (Exp2). Seen Tasks Unseen Tasks Method SUN397 Cars RESISC45 DTD SVHN GTSRB Avg. MNIST EuroSAT Avg. Pre-trained 63.2 59.9 60.6 43.9 23.5 30.4 46.9 47.6 45.6 46.6 Fisher Merging 65.5 67.2 78.2 57.6 84.2 75.9 71.4 71.8 49.4 60.6 RegMean 68.7 70.0 86.5 65.9 93.9 86.7 78.6 82.2 49.3 65.7 Task Arithmetic 64.3 63.0 73.2 54.9 84.7 79.5 69.9 75.5 42.6 59.1 Ties-Merging 68.3 65.5 76.9 54.9 75.4 72.0 68.9 73.1 47.3 60.2 Layer-wise AdaMerging 68.4 71.9 87.9 69.1 92.2 93.8 80.5 77.7 47.3 62.5 Weight-Ensembling MoE 75.4 77.5 94.3 77.0 96.8 98.7 86.6 78.3 44.0 61.1 Seen Tasks Unseen Tasks Method SUN397 Cars GTSRB EuroSAT DTD MNIST Avg. RESISC45 SVHN Avg. Pre-trained 63.2 59.9 30.4 45.6 43.9 47.6 48.4 60.6 23.5 40.1 Fisher Merging 68.1 67.4 67.2 86.4 58.6 81.6 71.5 60.2 42.5 51.3 RegMean 69.4 70.5 86.9 97.0 67.1 98.3 81.5 50.2 51.5 50.8 Task Arithmetic 65.2 63.6 76.1 87.1 56.4 94.2 73.8 52.4 45.2 48.8 Ties-Merging 68.2 65.9 70.0 81.2 56.0 89.0 71.7 60.3 47.3 53.8 Layer-wise AdaMerging 69.8 72.4 95.5 95.1 70.7 98.1 83.6 48.7 60.7 54.7 Weight-Ensembling MoE 74.3 78.1 98.8 98.7 75.1 99.5 87.4 47.3 51.3 49.3 <p>Table: Results of the robustness experiments (\\(\\lambda=0.3\\)).</p> Method Cars EuroSAT RESISC45 GTSRB Avg. Cars EuroSAT RESISC45 GTSRB Avg. Clean Test set Motion Blur Fisher Merging 66.0 92.7 83.7 78.7 80.3 60.7 57.6 81.7 78.4 69.6 RegMean 72.1 97.5 88.9 93.9 88.1 70.0 71.3 87.5 86.8 78.9 Task Arithmetic 64.6 91.8 80.2 74.8 77.9 62.4 59.2 78.5 63.3 65.9 Ties-Merging 65.2 83.3 78.1 67.4 73.5 64.4 53.9 76.4 57.1 62.9 Layer-wise AdaMerging 75.2 94.3 87.6 96.7 88.5 72.4 72.7 85.3 94.3 81.2 Weight-Ensembling MoE 77.4 98.9 94.4 99.0 92.4 76.5 74.2 93.7 97.4 85.5 Impulse Noise Gaussian Noise Fisher Merging 61.5 50.0 74.7 52.6 59.7 61.6 48.1 76.0 51.3 59.3 RegMean 66.9 51.0 80.6 68.7 66.8 69.4 41.8 84.0 67.7 65.7 Task Arithmetic 59.8 53.3 72.3 45.0 57.6 61.5 52.5 75.0 50.1 59.8 Ties-Merging 60.2 45.6 69.8 38.3 53.5 61.8 47.3 73.1 42.3 56.1 Layer-wise AdaMerging 69.2 40.0 79.6 83.3 68.0 70.0 53.3 82.1 80.0 71.4 Weight-Ensembling MoE 75.1 9.7 91.5 91.8 67.0 76.5 9.6 92.7 88.7 66.8 Pixelate Spatter Fisher Merging 2.2 34.0 17.0 63.2 29.1 61.4 64.2 74.6 47.3 61.9 RegMean 2.3 38.3 18.2 89.4 37.0 67.7 60.0 81.3 81.9 72.7 Task Arithmetic 2.3 33.2 19.1 65.6 30.0 61.0 62.5 72.8 57.0 63.3 Ties-Merging 3.3 31.8 18.0 58.5 27.9 61.3 52.9 70.3 48.1 58.2 Layer-wise AdaMerging 1.3 52.9 21.0 91.0 41.5 68.4 55.9 78.3 92.3 73.7 Weight-Ensembling MoE 0.5 11.6 2.3 97.5 28.0 75.1 9.7 91.4 96.3 68.1 Contrast JPEG Compression Fisher Merging 63.8 58.4 75.5 70.4 67.0 66.3 67.6 82.6 58.9 68.8 RegMean 69.6 64.8 84.4 90.0 77.2 71.5 72.6 88.7 82.2 78.7 Task Arithmetic 62.3 55.7 75.3 70.8 66.0 63.9 66.1 80.1 61.0 67.8 Ties-Merging 64.2 52.4 74.8 63.5 63.7 65.0 59.5 77.9 53.2 63.9 Layer-wise AdaMerging 73.1 67.4 83.0 96.2 79.9 72.9 70.7 86.3 90.6 80.1 Weight-Ensembling MoE 77.2 34.7 93.1 98.4 75.9 77.3 61.0 94.1 95.7 82.0"},{"location":"modelpool/clip_vit/#references","title":"References","text":"<ol> <li> <p>Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. Proceedings of the International Conference on Learning Representations, 2019.\u00a0\u21a9</p> </li> </ol>"},{"location":"modelpool/clip_vit/#fusion_bench.modelpool.clip_vision","title":"<code>clip_vision</code>","text":""},{"location":"modelpool/clip_vit/#fusion_bench.modelpool.clip_vision.modelpool","title":"<code>modelpool</code>","text":""},{"location":"modelpool/clip_vit/#fusion_bench.modelpool.clip_vision.modelpool.CLIPVisionModelPool","title":"<code>CLIPVisionModelPool</code>","text":"<p>               Bases: <code>BaseModelPool</code></p> <p>A model pool for managing Hugging Face's CLIP Vision models.</p> <p>This class extends the base <code>ModelPool</code> class and overrides its methods to handle the specifics of the CLIP Vision models provided by the Hugging Face Transformers library.</p> Source code in <code>fusion_bench/modelpool/clip_vision/modelpool.py</code> <pre><code>class CLIPVisionModelPool(BaseModelPool):\n    \"\"\"\n    A model pool for managing Hugging Face's CLIP Vision models.\n\n    This class extends the base `ModelPool` class and overrides its methods to handle\n    the specifics of the CLIP Vision models provided by the Hugging Face Transformers library.\n    \"\"\"\n\n    _config_mapping = BaseModelPool._config_mapping | {\"_processor\": \"processor\"}\n\n    def __init__(\n        self,\n        models: DictConfig,\n        *,\n        processor: Optional[DictConfig] = None,\n        **kwargs,\n    ):\n        super().__init__(models, **kwargs)\n\n        self._processor = processor\n\n    def load_processor(self, *args, **kwargs) -&gt; CLIPProcessor:\n        assert self._processor is not None, \"Processor is not defined in the config\"\n        if isinstance(self._processor, str):\n            log.info(f\"Loading `transformers.CLIPProcessor`: {self._processor}\")\n            processor = CLIPProcessor.from_pretrained(self._processor)\n        else:\n            processor = instantiate(self._processor, *args, **kwargs)\n        return processor\n\n    def load_clip_model(self, model_name: str, *args, **kwargs) -&gt; CLIPModel:\n        model_config = self._models[model_name]\n\n        if isinstance(model_config, str):\n            log.info(f\"Loading `transformers.CLIPModel`: {model_config}\")\n            clip_model = CLIPModel.from_pretrained(model_config, *args, **kwargs)\n            return clip_model\n        else:\n            assert isinstance(\n                model_config, DictConfig\n            ), \"Model config must be a DictConfig\"\n            model_config = deepcopy(model_config)\n            with open_dict(model_config):\n                model_config._target_ = \"transformers.CLIPModel.from_pretrained\"\n            clip_model = instantiate(model_config, *args, **kwargs)\n            return clip_model\n\n    @override\n    def save_model(self, model: CLIPVisionModel, path: str):\n        \"\"\"\n        Save a CLIP Vision model to the given path.\n\n        Args:\n            model (CLIPVisionModel): The model to save.\n            path (str): The path to save the model to.\n        \"\"\"\n        with timeit_context(f'Saving clip vision model to \"{path}\"'):\n            model.save_pretrained(path)\n\n    def load_model(\n        self, model_name_or_config: Union[str, DictConfig], *args, **kwargs\n    ) -&gt; CLIPVisionModel:\n        \"\"\"\n        This method is used to load a CLIPVisionModel from the model pool.\n\n        Example configuration could be:\n\n        ```yaml\n        models:\n            cifar10: tanganke/clip-vit-base-patch32_cifar10\n            sun397: tanganke/clip-vit-base-patch32_sun397\n            stanford-cars: tanganke/clip-vit-base-patch32_stanford-cars\n        ```\n\n        Args:\n            model_name_or_config (Union[str, DictConfig]): The name of the model or the model configuration.\n\n        Returns:\n            CLIPVisionModel: The loaded CLIPVisionModel.\n        \"\"\"\n        if (\n            isinstance(model_name_or_config, str)\n            and model_name_or_config in self._models\n        ):\n            model = self._models[model_name_or_config]\n            if isinstance(model, str):\n                log.info(f\"Loading `transformers.CLIPVisionModel`: {model}\")\n                return CLIPVisionModel.from_pretrained(model, *args, **kwargs)\n            if isinstance(model, nn.Module):\n                log.info(f\"Returning existing model: {model}\")\n                return model\n\n        # If the model is not a string, we use the default load_model method\n        return super().load_model(model_name_or_config, *args, **kwargs)\n\n    def load_train_dataset(self, dataset_name: str, *args, **kwargs):\n        dataset_config = self._train_datasets[dataset_name]\n        if isinstance(dataset_config, str):\n            log.info(\n                f\"Loading train dataset using `datasets.load_dataset`: {dataset_config}\"\n            )\n            dataset = load_dataset(dataset_config, split=\"train\")\n        else:\n            dataset = super().load_train_dataset(dataset_name, *args, **kwargs)\n        return dataset\n\n    def load_val_dataset(self, dataset_name: str, *args, **kwargs):\n        dataset_config = self._val_datasets[dataset_name]\n        if isinstance(dataset_config, str):\n            log.info(\n                f\"Loading validation dataset using `datasets.load_dataset`: {dataset_config}\"\n            )\n            dataset = load_dataset(dataset_config, split=\"validation\")\n        else:\n            dataset = super().load_val_dataset(dataset_name, *args, **kwargs)\n        return dataset\n\n    def load_test_dataset(self, dataset_name: str, *args, **kwargs):\n        dataset_config = self._test_datasets[dataset_name]\n        if isinstance(dataset_config, str):\n            log.info(\n                f\"Loading test dataset using `datasets.load_dataset`: {dataset_config}\"\n            )\n            dataset = load_dataset(dataset_config, split=\"test\")\n        else:\n            dataset = super().load_test_dataset(dataset_name, *args, **kwargs)\n        return dataset\n</code></pre> <code></code> <code>load_model(model_name_or_config, *args, **kwargs)</code> \u00b6 <p>This method is used to load a CLIPVisionModel from the model pool.</p> <p>Example configuration could be:</p> <pre><code>models:\n    cifar10: tanganke/clip-vit-base-patch32_cifar10\n    sun397: tanganke/clip-vit-base-patch32_sun397\n    stanford-cars: tanganke/clip-vit-base-patch32_stanford-cars\n</code></pre> <p>Parameters:</p> <ul> <li> <code>model_name_or_config</code> \u00b6              (<code>Union[str, DictConfig]</code>)           \u2013            <p>The name of the model or the model configuration.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>CLIPVisionModel</code> (              <code>CLIPVisionModel</code> )          \u2013            <p>The loaded CLIPVisionModel.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/clip_vision/modelpool.py</code> <pre><code>def load_model(\n    self, model_name_or_config: Union[str, DictConfig], *args, **kwargs\n) -&gt; CLIPVisionModel:\n    \"\"\"\n    This method is used to load a CLIPVisionModel from the model pool.\n\n    Example configuration could be:\n\n    ```yaml\n    models:\n        cifar10: tanganke/clip-vit-base-patch32_cifar10\n        sun397: tanganke/clip-vit-base-patch32_sun397\n        stanford-cars: tanganke/clip-vit-base-patch32_stanford-cars\n    ```\n\n    Args:\n        model_name_or_config (Union[str, DictConfig]): The name of the model or the model configuration.\n\n    Returns:\n        CLIPVisionModel: The loaded CLIPVisionModel.\n    \"\"\"\n    if (\n        isinstance(model_name_or_config, str)\n        and model_name_or_config in self._models\n    ):\n        model = self._models[model_name_or_config]\n        if isinstance(model, str):\n            log.info(f\"Loading `transformers.CLIPVisionModel`: {model}\")\n            return CLIPVisionModel.from_pretrained(model, *args, **kwargs)\n        if isinstance(model, nn.Module):\n            log.info(f\"Returning existing model: {model}\")\n            return model\n\n    # If the model is not a string, we use the default load_model method\n    return super().load_model(model_name_or_config, *args, **kwargs)\n</code></pre> <code></code> <code>save_model(model, path)</code> \u00b6 <p>Save a CLIP Vision model to the given path.</p> <p>Parameters:</p> <ul> <li> <code>model</code> \u00b6              (<code>CLIPVisionModel</code>)           \u2013            <p>The model to save.</p> </li> <li> <code>path</code> \u00b6              (<code>str</code>)           \u2013            <p>The path to save the model to.</p> </li> </ul> Source code in <code>fusion_bench/modelpool/clip_vision/modelpool.py</code> <pre><code>@override\ndef save_model(self, model: CLIPVisionModel, path: str):\n    \"\"\"\n    Save a CLIP Vision model to the given path.\n\n    Args:\n        model (CLIPVisionModel): The model to save.\n        path (str): The path to save the model to.\n    \"\"\"\n    with timeit_context(f'Saving clip vision model to \"{path}\"'):\n        model.save_pretrained(path)\n</code></pre>"},{"location":"modelpool/clip_vit/#fusion_bench.models.linearized.vision_model","title":"<code>vision_model</code>","text":""},{"location":"modelpool/clip_vit/#fusion_bench.models.linearized.vision_model.linearize_lora_model_","title":"<code>linearize_lora_model_(model)</code>","text":"<p>Linearizes the LoraLayer modules in a PyTorch model according to the PETA paper.</p> Source code in <code>fusion_bench/models/linearized/vision_model.py</code> <pre><code>def linearize_lora_model_(model):\n    \"\"\"\n    Linearizes the LoraLayer modules in a PyTorch model according to the PETA paper.\n    \"\"\"\n    for key, module in model.named_modules():\n        # if isinstance(module, LoraLayer) and isinstance(module, nn.Linear):\n        if isinstance(module, LoraLayer):\n            # print(\"L-LoRA MODULE : \", module)\n            parent, target, target_name = _get_submodules(model, key)\n            setattr(parent, target_name, LinearizedModelWraper(target))\n            # print(\"Linearized Lora Layer\")\n    return model\n</code></pre>"},{"location":"modelpool/clip_vit/#fusion_bench.models.linearized.vision_model.load_fft_vision_model_hf","title":"<code>load_fft_vision_model_hf(model_name, return_vison_model=True)</code>","text":"<p>Load a CLIP vision model from Hugging Face.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Union[CLIPVisionTransformer, CLIPVisionModel]</code>           \u2013            <p>Union[CLIPVisionTransformer, CLIPVisionModel]: The vision model.</p> </li> </ul> Source code in <code>fusion_bench/models/linearized/vision_model.py</code> <pre><code>def load_fft_vision_model_hf(\n    model_name: str, return_vison_model=True\n) -&gt; Union[CLIPVisionTransformer, CLIPVisionModel]:\n    \"\"\"\n    Load a CLIP vision model from Hugging Face.\n\n    Args:\n        model_name (str): The name of the CLIP vision model to load from Hugging Face.\n        return_vison_model (bool, optional): If False, the full CLIPVisionModel is returned. If True, only the vision model (`CLIPVisionTransformer`) is returned. Defaults to True.\n\n    Returns:\n        Union[CLIPVisionTransformer, CLIPVisionModel]: The vision model.\n    \"\"\"\n    model = CLIPVisionModel.from_pretrained(model_name)\n\n    if return_vison_model:\n        return CLIPVisionModel.from_pretrained(model_name).vision_model\n    else:\n        return model\n</code></pre>"},{"location":"modelpool/clip_vit/#fusion_bench.models.linearized.vision_model.load_fft_vision_model_hf(model_name)","title":"<code>model_name</code>","text":"(<code>str</code>)           \u2013            <p>The name of the CLIP vision model to load from Hugging Face.</p>"},{"location":"modelpool/clip_vit/#fusion_bench.models.linearized.vision_model.load_fft_vision_model_hf(return_vison_model)","title":"<code>return_vison_model</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If False, the full CLIPVisionModel is returned. If True, only the vision model (<code>CLIPVisionTransformer</code>) is returned. Defaults to True.</p>"},{"location":"modelpool/clip_vit/#fusion_bench.models.linearized.vision_model.load_l_lora_vision_model_hf","title":"<code>load_l_lora_vision_model_hf(base_model_name, peft_name)</code>","text":"<p>Load a linearized L-LoRA model from a base model and a Peft model (HuggingFace).</p> Source code in <code>fusion_bench/models/linearized/vision_model.py</code> <pre><code>def load_l_lora_vision_model_hf(base_model_name: str, peft_name: str):\n    \"\"\"\n    Load a linearized L-LoRA model from a base model and a Peft model (HuggingFace).\n    \"\"\"\n    base_model = CLIPVisionModel.from_pretrained(base_model_name).vision_model\n    peft_config = LoraConfig.from_pretrained(peft_name)\n    peft_config.inference_mode = False  # This is important, make the model trainable\n    model = get_peft_model(base_model, peft_config)\n    linearize_lora_model_(model)\n    for filename in [\"linearized_adapter_model.safetensors\"]:\n        path = get_file_path(peft_name, filename)\n        state_dict = load_file(path)\n        for name, param in state_dict.items():\n            model.get_parameter(name).data = param\n\n    return model\n</code></pre>"},{"location":"modelpool/clip_vit/#fusion_bench.models.linearized.vision_model.load_lora_vision_model_hf","title":"<code>load_lora_vision_model_hf(base_model_name, peft_name, merge_and_unload=False, return_vison_model=True)</code>","text":"<p>Load a LoRA (Low-Rank Adaptation) vision model from Hugging Face.</p> <p>This function loads a vision model and applies a LoRA adaptation to it. The model can be optionally merged and unloaded.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>PeftModel</code>          \u2013            <p>The adapted vision model, optionally merged and unloaded.</p> </li> </ul> Source code in <code>fusion_bench/models/linearized/vision_model.py</code> <pre><code>def load_lora_vision_model_hf(\n    base_model_name: str,\n    peft_name: str,\n    merge_and_unload: bool = False,\n    return_vison_model=True,\n):\n    \"\"\"\n    Load a LoRA (Low-Rank Adaptation) vision model from Hugging Face.\n\n    This function loads a vision model and applies a LoRA adaptation to it. The model can be optionally merged and unloaded.\n\n    Parameters:\n        base_model_name (str): The name of the base vision model to load from Hugging Face.\n        peft_name (str): The name of the LoRA adaptation to apply to the base model.\n        merge_and_unload (bool, optional): If True, the LoRA adaptation is merged into the base model and the LoRA layers are removed. Defaults to False.\n        return_vison_model (bool, optional): If False, the full CLIPVisionModel is returned. If True, only the vision model (`CLIPVisionTransformer`) is returned. Defaults to True.\n\n    Returns:\n        PeftModel: The adapted vision model, optionally merged and unloaded.\n    \"\"\"\n    model = CLIPVisionModel.from_pretrained(base_model_name)\n\n    # Load the Peft model\n    # note that we apply lora on type `CLIPVisionTransformer` instead of `CLIPVisionModel`\n    vision_model = model.vision_model\n    peft_model = PeftModel.from_pretrained(vision_model, peft_name, is_trainable=True)\n    if merge_and_unload:\n        vision_model = peft_model.merge_and_unload()\n    else:\n        vision_model = peft_model\n\n    # Return the vision model\n    if return_vison_model:\n        return vision_model\n    else:\n        model.vision_model = vision_model\n        return model\n</code></pre>"},{"location":"modelpool/clip_vit/#fusion_bench.models.linearized.vision_model.load_lora_vision_model_hf(base_model_name)","title":"<code>base_model_name</code>","text":"(<code>str</code>)           \u2013            <p>The name of the base vision model to load from Hugging Face.</p>"},{"location":"modelpool/clip_vit/#fusion_bench.models.linearized.vision_model.load_lora_vision_model_hf(peft_name)","title":"<code>peft_name</code>","text":"(<code>str</code>)           \u2013            <p>The name of the LoRA adaptation to apply to the base model.</p>"},{"location":"modelpool/clip_vit/#fusion_bench.models.linearized.vision_model.load_lora_vision_model_hf(merge_and_unload)","title":"<code>merge_and_unload</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the LoRA adaptation is merged into the base model and the LoRA layers are removed. Defaults to False.</p>"},{"location":"modelpool/clip_vit/#fusion_bench.models.linearized.vision_model.load_lora_vision_model_hf(return_vison_model)","title":"<code>return_vison_model</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If False, the full CLIPVisionModel is returned. If True, only the vision model (<code>CLIPVisionTransformer</code>) is returned. Defaults to True.</p>"},{"location":"modelpool/flan-t5_generation/","title":"Flan-T5 Models for Text Generation","text":""},{"location":"modelpool/flan-t5_generation/#model-information","title":"Model Information","text":"<p>Prompt-based fine-tuned Flan-T5 models on GLUE benchmark tasks. The models are fine-tuned in a text-to-text setting, and the prompt templates are provided below. The source code for the prompt templates can be found in the repository.</p> fusion_bench/tasks/flan_t5_text_generation/glue_prompt_templates.py<pre><code>cola = {\n    \"description\": \"template used by GLUE-CoLA\",\n    \"input_text\": \"Indicate if the following sentence is grammatically correct or not: \\\"{sentence}\\\". Answere 'acceptable' or 'unacceptable'.\",\n    \"target_text\": {\"0\": \"unacceptable\", \"1\": \"acceptable\"},\n}\nmnli = {\n    \"input_text\": \"Does the premise: '{premise}' logically imply, contradict, or is neutral to the hypothesis: '{hypothesis}'? Answere with 'entailment', 'contradiction', or 'neutral'.\",\n    \"target_text\": {\"0\": \"entailment\", \"1\": \"neutral\", \"2\": \"contradiction\"},\n}\nmrpc = {\n    \"input_text\": \"Are the following sentences '{sentence1}' and '{sentence2}' conveying the same meaning? Answere with 'yes' or 'no'.\",\n    \"target_text\": {\"0\": \"no\", \"1\": \"yes\"},\n}\nqnli = {\n    \"input_text\": \"Given the context: '{sentence}', does the question '{question}' have an answer based on the information provided? Answer with 'yes' or 'no'.\",\n    \"target_text\": {\"0\": \"yes\", \"1\": \"no\"},\n}\nqqp = {\n    \"input_text\": \"Do the questions '{question1}' and '{question2}' have the same intent? Answere with 'yes' or 'no'.\",\n    \"target_text\": {\"0\": \"no\", \"1\": \"yes\"},\n}\nrte = {\n    \"description\": \"Template used by GLUE-RTE\",\n    \"input_text\": \"Does the text: '{sentence1}' entail that '{sentence2}' is true? Provide 'yes' or 'no'.\",\n    \"target_text\": {\"0\": \"yes\", \"1\": \"no\"},\n}\nsst2 = {\n    \"input_text\": \"Given the sentence '{sentence}', determine the sentiment. Is it positive or negative?\",\n    \"target_text\": {\"0\": \"negative\", \"1\": \"positive\"},\n}\nstsb = {\n    \"input_text\": \"Consider the sentences '{sentence1}' and '{sentence2}'. On a scale from 1 (completely different) to 5 (completely similar), rate the similarity.\",\n    \"target_text\": \"{:.1f}\",\n}\n</code></pre>"},{"location":"modelpool/flan-t5_generation/#flan-t5-base","title":"Flan-T5-base","text":""},{"location":"modelpool/flan-t5_generation/#full-fine-tuned-models","title":"Full Fine-tuned Models","text":"<p>full fine-tuned Flan-T5-base models on tasks from GLUE benchmark</p> Model GLUE-COLA GLUE-MNLI GLUE-MRPC GLUE-QNLI GLUE-QQP GLUE-RTE GLUE-SST2 GLUE-STSB Average Pre-trained 69.127517 56.454407 76.225490 88.449570 82.119713 80.144404 91.169725 62.190453 75.735160 GLUE-COLA 74.976031 37.208355 72.794118 87.552627 80.415533 76.895307 91.399083 63.583974 73.103128 GLUE-MNLI 65.867689 83.413143 75.735294 89.236683 82.616869 77.978339 90.596330 66.215025 78.957422 GLUE-MRPC 63.374880 48.293428 87.500000 85.831960 81.100668 72.563177 88.073394 76.062875 75.350048 GLUE-QNLI 68.744008 39.246052 75.490196 91.488193 81.291120 78.339350 91.628440 68.200428 74.303474 GLUE-QQP 59.060403 50.412634 73.774510 88.339740 85.369775 81.227437 90.825688 75.948390 75.619822 GLUE-RTE 65.388303 51.115639 69.607843 88.705839 80.774178 85.920578 90.252294 68.944418 75.088636 GLUE-SST2 67.785235 53.958227 76.470588 87.772286 83.415780 80.505415 93.577982 63.612718 75.887279 GLUE-STSB 69.319271 49.302089 76.470588 88.962109 81.662132 77.617329 90.137615 88.695433 77.770821"},{"location":"modelpool/flan-t5_generation/#lora-fine-tuned-models-r16","title":"LoRA Fine-tuned Models (r=16)","text":"<p>LoRA fine-tuned (r=16) Flan-T5-base models on tasks from GLUE benchmark:</p> Model GLUE-COLA GLUE-MNLI GLUE-MRPC GLUE-QNLI GLUE-QQP GLUE-RTE GLUE-SST2 GLUE-STSB Average Pre-trained 69.1 56.5 76.2 88.4 82.1 80.1 91.2 62.2 75.7 GLUE-COLA 69.1 39.9 75.2 89.1 81.1 81.9 90.7 54.0 GLUE-MNLI 69.4 82.7 73.8 89.3 82.0 79.4 90.9 68.1 GLUE-MRPC 64.0 44.9 85.5 82.6 81.0 69.0 88.6 73.6 GLUE-QNLI 68.9 52.7 76.7 90.9 82.8 79.8 91.5 68.9 GLUE-QQP 65.0 54.6 75.7 89.0 84.0 81.6 90.7 75.3 GLUE-RTE 64.9 51.8 69.4 89.2 79.8 84.5 90.6 70.1 GLUE-SST2 68.3 56.6 76.0 88.5 83.4 79.8 92.9 62.6 GLUE-STSB 65.7 1.7 67.4 89.3 80.1 79.8 90.8 87.4"},{"location":"modelpool/flan-t5_generation/#flan-t5-large","title":"Flan-T5-Large","text":""},{"location":"modelpool/flan-t5_generation/#lora-fine-tuned-models-r16_1","title":"LoRA Fine-tuned Models (r=16)","text":"<p>LoRA fine-tuned (r=16) Flan-T5-large models on tasks from GLUE benchmark:</p> Model GLUE-COLA GLUE-MNLI GLUE-MRPC GLUE-QNLI GLUE-QQP GLUE-RTE GLUE-SST2 GLUE-STSB Average Pretrained 73.7 56.6 82.4 91.1 85.5 85.6 94.3 87.5 82.1 GLUE-COLA 80.2 53.9 81.4 90.8 84.5 84.1 93.9 87.1 GLUE-MNLI 73.7 88.5 77.9 92.4 85.2 87.7 94.4 86.7 GLUE-MRPC 75.6 52.6 89.2 92.6 84.4 86.3 94.3 86.3 GLUE-QNLI 73.5 54.5 82.8 94.4 85.8 85.2 93.7 87.1 GLUE-QQP 74.0 53.8 82.8 92.5 87.2 85.6 94.5 88.3 GLUE-RTE 75.6 57.5 69.9 92.8 83.8 91.7 94.6 86.0 GLUE-SST2 73.6 55.3 82.1 91.6 85.5 85.2 95.2 86.9 GLUE-STSB 73.4 39.3 82.1 92.6 86.1 83.4 94.0 90.9"},{"location":"modelpool/flan-t5_generation/#basic-examples","title":"Basic Examples","text":""},{"location":"modelpool/flan-t5_generation/#inverstigate-model-information","title":"Inverstigate Model Information","text":"<p>Load pre-trained Flan-T5-base model and print the model information</p> <pre><code>fusion_bench \\\n    modelpool=Seq2SeqLMPool/flan-t5-base_glue \\\n    method=dummy taskpool=dummy\n# {'model_info': {'trainable_params': 247577856, 'all_params': 247577856, 'trainable_percentage': 1.0}}\n</code></pre> <p>Load pre-trained Flan-T5-large model and print the model information</p> <pre><code>fusion_bench \\\n    modelpool=Seq2SeqLMPool/flan-t5-large_glue_lora16 \\\n    method=dummy taskpool=dummy\n</code></pre>"},{"location":"modelpool/flan-t5_generation/#evaluate-single-model","title":"Evaluate Single Model","text":"<p>Evaluate the pre-trained Flan-T5-base model on GLUE tasks</p> <pre><code>fusion_bench \\\n    method=dummy \\\n    modelpool=Seq2SeqLMPool/flan-t5-base_individual \\\n        modelpool.models._pretrained_.pretrained_model_name_or_path=google/flan-t5-base \\\n    taskpool=flan-t5_glue_text_generation \\\n    report_save_path=outputs/flan-t5-base/pretrained.json\n</code></pre> <p>or evaluate the fine-tuned Flan-T5-base model on GLUE tasks</p> <pre><code>for task in cola mnli mrpc qnli qqp rte sst2 stsb; do\n    fusion_bench \\\n        method=dummy \\\n        modelpool=Seq2SeqLMPool/flan-t5-base_individual \\\n            modelpool.models._pretrained_.pretrained_model_name_or_path=tanganke/flan-t5-base_glue-${task} \\\n        taskpool=flan-t5_glue_text_generation \\\n        report_save_path=outputs/flan-t5-base/glue-$task.json\ndone\n</code></pre>"},{"location":"modelpool/flan-t5_generation/#simple-average","title":"Simple Average","text":"<p>Merge the Flan-T5 models on GLUE tasks using simple average and evaluate on the Flan-T5 text generation task</p> <pre><code># for full fine-tuned models\nfusion_bench \\\n    method=simple_average \\\n    modelpool=Seq2SeqLMPool/flan-t5-base_glue \\\n    taskpool=flan-t5_glue_text_generation\n\n# or using the LoRA fine-tuned models\nfusion_bench \\\n    method=simple_average \\\n    modelpool=Seq2SeqLMPool/flan-t5-base_glue_lora16 \\\n    taskpool=flan-t5_glue_text_generation\n</code></pre>"},{"location":"modelpool/flan-t5_generation/#task-arithmetic","title":"Task Arithmetic","text":"<p>Merge the Flan-T5 models on GLUE tasks using task arithmetic and evaluate on the Flan-T5 text generation task, with scaling factor from 0.0 to 1.0</p> <pre><code># full fine-tuned models with scaling factor set to 0.3\nfusion_bench \\\n    method=task_arithmetic \\\n        method.scaling_factor=0.3 \\\n    modelpool=Seq2SeqLMPool/flan-t5-base_glue \\\n    taskpool=flan-t5_glue_text_generation\n\n# use a for loop to evaluate the performance of task arithmetic with different scaling factors (LoRA fine-tuned models)\nfor scaling_factor in $(seq 0.0 0.1 1.0)\ndo\n    fusion_bench \\\n        method=task_arithmetic \\\n            method.scaling_factor=$scaling_factor \\\n        modelpool=Seq2SeqLMPool/flan-t5-base_glue_lora16 \\\n        taskpool=flan-t5_glue_text_generation\ndone\n</code></pre> scaling_coef glue-cola glue-mnli glue-mrpc glue-qnli glue-qqp glue-rte glue-sst2 glue-stsb Average 0.0 69.1 56.5 76.2 88.4 82.1 80.1 91.2 62.2 75.7 0.1 69.5 59.8 78.7 89.7 83.6 80.5 91.1 70.7 78.0 0.2 69.3 59.0 78.7 90.1 83.8 79.1 91.5 72.9 78.1 0.3 68.8 55.2 78.7 89.8 83.7 79.1 91.5 72.4 77.4 0.4 68.1 31.3 77.7 88.7 83.3 78.7 91.2 68.9 73.5 0.5 66.0 2.2 78.7 86.3 82.6 78.0 90.4 74.2 0.6 5.9 0.0 78.4 81.2 81.6 74.4 88.2 74.9 0.7 0.0 0.0 77.0 74.1 79.8 66.1 70.8 74.7 0.8 0.0 0.0 74.0 67.5 77.0 62.1 8.1 72.5 0.9 0.0 0.0 66.7 60.5 71.7 58.5 0.0 71.6 1.0 0.0 0.0 46.3 50.6 56.3 52.3 0.0 69.8"},{"location":"modelpool/flan-t5_generation/#ties-merging","title":"Ties-Merging","text":"<p>or using ties-merging</p> <pre><code># for full fine-tuned models with scaling factor set to 0.3\nfusion_bench \\\n    method=ties_merging \\\n        method.scaling_factor=0.3 \\\n    modelpool=Seq2SeqLMPool/flan-t5-base_glue \\\n    taskpool=flan-t5_glue_text_generation\n\n# use a for loop to evaluate the performance of ties-merging with different scaling factors (LoRA fine-tuned models)\nfor scaling_factor in $(seq 0.0 0.1 1.0)\ndo\n    fusion_bench \\\n        method=ties_merging \\\n            method.scaling_factor=$scaling_factor \\\n        modelpool=Seq2SeqLMPool/flan-t5-base_glue_lora16 \\\n        taskpool=flan-t5_glue_text_generation\ndone\n</code></pre>"},{"location":"modelpool/flan-t5_generation/#experimental-results","title":"Experimental Results","text":"<p>Flan-T5-Base models:</p> Table: Multi-task model merging methods using Flan-T5-Base (full fine-tuned) models.Table: Multi-task model merging methods using Flan-T5-Base (LoRA r=16) models. Method CoLA MNLI MRPC QNLI QQP RTE SST-2 STSB Average Reference Results Pre-trained 69.1 56.5 76.2 88.4 82.1 80.1 91.2 62.2 75.7 Fine-tuned (STL) 75.0 83.4 87.5 91.5 85.4 85.9 93.6 88.7 86.4 Model Merging Methods Simple Average 69.1 62.6 79.4 89.8 83.9 81.2 91.7 73.2 78.9 Task Arithmetic (\\(\\lambda=0.3\\)) 70.5 57.8 78.4 90.2 83.6 80.5 92.3 77.8 78.9 Ties-Merging (\\(\\lambda=0.3\\)) 70.3 65.0 78.9 90.2 83.5 81.6 91.7 78.3 79.9 Method CoLA MNLI MRPC QNLI QQP RTE SST-2 STSB Average Reference Results Pre-trained 69.1 56.5 76.2 88.4 82.1 80.1 91.2 62.2 75.7 Fine-tuned (STL) 69.1 82.7 85.5 90.9 84.0 84.4 92.9 87.4 84.6 Model Merging Methods Simple Average 69.7 59.7 78.9 90.1 83.8 90.5 91.2 72.0 78.2 Task Arithmetic (\\(\\lambda=0.3\\)) 68.8 55.2 78.7 89.8 83.7 79.1 91.5 72.4 77.4 Ties-Merging (\\(\\lambda=0.3\\)) 68.3 56.3 79.4 89.8 83.7 79.4 91.6 71.2 77.5 <p>Flan-T5-Large models:</p> Table: Multi-task model merging methods using Flan-T5-Large (LoRA r=16) models Method CoLA MNLI MRPC QNLI QQP RTE SST-2 STSB Average Reference Results Pre-trained 73.7 56.6 82.4 91.1 85.5 85.6 94.3 87.5 82.1 Fine-tuned (STL) 80.2 88.5 89.2 94.4 87.2 91.7 95.2 90.9 89.6 Model Merging Methods Simple Average 74.6 84.3 84.1 92.8 86.3 87.4 94.8 88.0 86.5 Task Arithmetic (\\(\\lambda=0.3\\)) 76.9 85.4 85.3 93.9 85.8 88.1 95.2 87.8 87.3 Ties-Merging (\\(\\lambda=0.3\\)) 77.1 85.1 86.3 93.9 86.0 87.7 95.1 88.0 87.4"},{"location":"modelpool/flan-t5_generation/#references","title":"References","text":""},{"location":"modelpool/flan-t5_generation/#fusion_bench.modelpool.Seq2SeqLMPool","title":"<code>Seq2SeqLMPool</code>","text":"<p>               Bases: <code>BaseModelPool</code></p> Source code in <code>fusion_bench/modelpool/seq2seq_lm/modelpool.py</code> <pre><code>class Seq2SeqLMPool(BaseModelPool):\n    _config_mapping = BaseModelPool._config_mapping | {\n        \"_tokenizer\": \"tokenizer\",\n        \"_model_kwargs\": \"model_kwargs\",\n    }\n\n    def __init__(\n        self,\n        models: DictConfig,\n        *,\n        tokenizer: Optional[DictConfig],\n        model_kwargs: Optional[DictConfig] = None,\n        **kwargs,\n    ):\n        super().__init__(models, **kwargs)\n        self._tokenizer = tokenizer\n        self._model_kwargs = model_kwargs\n        if self._model_kwargs is None:\n            self._model_kwargs = DictConfig({})\n        with flag_override(self._model_kwargs, \"allow_objects\", True):\n            if hasattr(self._model_kwargs, \"torch_dtype\"):\n                self._model_kwargs.torch_dtype = parse_dtype(\n                    self._model_kwargs.torch_dtype\n                )\n\n    def load_model(self, model_name_or_config: str | DictConfig, *args, **kwargs):\n        model_kwargs = deepcopy(self._model_kwargs)\n        model_kwargs.update(kwargs)\n        return super().load_model(model_name_or_config, *args, **model_kwargs)\n\n    def load_tokenizer(self, *args, **kwargs):\n        assert self._tokenizer is not None, \"Tokenizer is not defined in the config\"\n        tokenizer = isinstance(self._tokenizer, *args, **kwargs)\n        return tokenizer\n</code></pre>"},{"location":"modelpool/gpt2_classification/","title":"GPT-2 Models for Text Classification","text":"<p>Here we provide a series of GPT-2 models fine-tuned for text classification tasks.</p>"},{"location":"modelpool/gpt2_classification/#the-seven-tasks-from-glue-benchmark","title":"The Seven Tasks from GLUE Benchmark","text":"<p>We provide seven GPT-2 models fine-tuned on the following tasks from GLUE Benchmark: CoLA, SST-2, MRPC, QQP, MNLI, RTE, and QNLI. These models are fine-tuned with the learning rate of 5e-5 for 3 epochs. The models are available on HuggingFace as Pytorch models.</p> <p>Evaluation results of these single-task models on the GLUE Benchmark are as follows:</p> Model CoLA MNLI MRPC QNLI QQP RTE SST-2 Avg. CoLA 76.8 32.8 68.4 50.4 39.2 48.0 51.0 52.4 MNLI 59.5 82.1 33.8 46.5 24.9 57.4 40.5 49.2 MRPC 30.8 25.9 80.4 47.1 65.9 49.1 49.1 49.8 QNLI 58.7 38.9 30.6 88.3 39.9 48.7 47.0 50.3 QQP 31.4 25.7 62.3 45.0 89.6 49.1 49.1 50.3 RTE 52.8 47.7 37.5 53.5 33.7 65.3 54.9 49.3 SST-2 51.8 32.9 40.2 49.8 56.8 44.4 91.2 52.4"},{"location":"modelpool/gpt2_classification/#model-pool-configuration","title":"Model Pool Configuration","text":"<p>To use these models from our FusionBench library, you can specify the modelpool configuration file as follows:</p> config/modelpool/gpt-2_glue.yaml<pre><code>type: HF_GPT2ForSequenceClassification\nmodels:\n  - name: _pretrained_\n    path: gpt2\n  - name: cola\n    path: tanganke/gpt2_cola\n  - name: mnli\n    path: tanganke/gpt2_mnli\n  - name: mrpc\n    path: tanganke/gpt2_mrpc\n  - name: qnli\n    path: tanganke/gpt2_qnli\n  - name: qqp\n    path: tanganke/gpt2_qqp\n  - name: rte\n    path: tanganke/gpt2_rte\n  - name: sst2\n    path: tanganke/gpt2_sst2\n</code></pre>"},{"location":"modelpool/gpt2_classification/#basic-examples","title":"Basic Examples","text":"<p>Here are some basic examples of using our CLI tool <code>fusion_bench</code> to merge the GPT-2 models.</p>"},{"location":"modelpool/gpt2_classification/#simple-ensemble","title":"Simple Ensemble","text":"<p>construct an ensemble of GPT-2 models using simple ensemble and evaluate on the seven tasks</p> <pre><code>fusion_bench method=simple_ensemble \\\n  modelpool=gpt-2_glue \\\n  taskpool=gpt-2_glue\n</code></pre>"},{"location":"modelpool/gpt2_classification/#simpleaverage","title":"SimpleAverage","text":"<p>merge GPT-2 models using simple average and evluate on the seven tasks</p> <pre><code>fusion_bench method=simple_average \\\n  modelpool=gpt-2_glue \\\n  taskpool=gpt-2_glue\n</code></pre>"},{"location":"modelpool/gpt2_classification/#fisher-merging","title":"Fisher merging","text":"<p>merge GPT-2 models using Fisher Merging and evluate the merged model</p> <pre><code>fusion_bench \\\n  method=fisher_merging/gpt2_fisher_merging \\\n    method.batch_size=8 method.num_fisher_examples=512 \\\n  modelpool=gpt-2_glue \\\n  taskpool=gpt-2_glue\n</code></pre>"},{"location":"modelpool/gpt2_classification/#regmean","title":"RegMean","text":"<p>merge GPT-2 models using RegMean and evaluate the merged model</p> <pre><code>fusion_bench \\\n  method=regmean/gpt2_regmean \\\n  modelpool=gpt-2_glue \\\n  taskpool=gpt-2_glue\n</code></pre>"},{"location":"modelpool/gpt2_classification/#task-arithmetic","title":"Task Arithmetic","text":"<p>merge using Task Arithmetic on the seven tasks</p> <pre><code># set the scaling factor to 0.3\nfusion_bench \\\n  method=task_arithmetic \\\n    method.scaling_factor=0.3 \\\n  modelpool=gpt-2_glue \\\n  taskpool=gpt-2_glue\n\n# or run the following script to evaluate the model with different scaling factors,\n# and save the results to different files\n# or \"for scaling_factor in $(seq 0 0.1 1.0)\", I use the following for loop for better readability for readers who are not familiar with bash\nfor scaling_factor in 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 \ndo\nfusion_bench report_save_path=outputs/gpt2_glue_task_arithmetic_scaling_factor_${scaling_factor}.json \\\n  method=task_arithmetic \\\n    method.scaling_factor=${scaling_factor} \\\n  modelpool=gpt-2_glue \\\n  taskpool=gpt-2_glue\ndone\n</code></pre> <p>After running the above commands, you will get the following results:</p> <p>Table: Task Arithmetic with different scaling factors</p> scaling_coef cola mnli mrpc qnli qqp rte sst2 Avg. 0.0 0.308725 0.330107 0.313725 0.491671 0.63166 0.527076 0.509174 0.444591 0.1 0.426654 0.501375 0.367647 0.556654 0.739105 0.494585 0.509174 0.513599 0.2 0.658677 0.585532 0.698529 0.602599 0.785258 0.472924 0.669725 0.639035 0.3 0.682646 0.639837 0.718137 0.669046 0.807915 0.462094 0.792431 0.68173 0.4 0.690316 0.673867 0.70098 0.702178 0.817067 0.472924 0.819954 0.696755 0.5 0.68744 0.685583 0.696078 0.704924 0.81818 0.472924 0.836009 0.700163 0.6 0.688399 0.680998 0.678922 0.700531 0.808978 0.472924 0.850917 0.697381 0.7 0.684564 0.665003 0.669118 0.702361 0.789612 0.480144 0.853211 0.692002 0.8 0.677852 0.619154 0.659314 0.673989 0.748776 0.501805 0.819954 0.671549 0.9 0.644295 0.503515 0.654412 0.540912 0.637942 0.487365 0.78555 0.607713 1.0 0.627996 0.411004 0.54902 0.496614 0.478234 0.530686 0.71445 0.544"},{"location":"modelpool/gpt2_classification/#ties-merging","title":"Ties-Merging","text":"<p>merge using Ties-Merging on the seven tasks</p> <pre><code>fusion_bench \\\n  method=ties_merging \\\n    method.scaling_factor=0.3 \\\n  modelpool=gpt-2_glue \\\n  taskpool=gpt-2_glue\\\n\n# or run the following script to evaluate the model with different scaling factors,\n# and save the results to different files\nfor scaling_factor in 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\ndo fusion_bench report_save_path=outputs/gpt2_glue_ties_merging_scaling_factor_${scaling_factor}.json \\\n  method=ties_merging \\\n    method.scaling_factor=${scaling_factor} \\\n  modelpool=gpt-2_glue \\\n  taskpool=gpt-2_glue\ndone\n</code></pre> scaling_coef cola mnli mrpc qnli qqp rte sst2 Avg. 0.0 0.308725 0.330107 0.313725 0.491671 0.63166 0.527076 0.509174 0.444591 0.1 0.348035 0.45624 0.328431 0.542559 0.70554 0.523466 0.509174 0.487635 0.2 0.489933 0.589913 0.416667 0.596559 0.788647 0.501805 0.510321 0.556264 0.3 0.646213 0.648497 0.632353 0.641406 0.810611 0.516245 0.618119 0.644778 0.4 0.670182 0.691594 0.669118 0.683141 0.821815 0.490975 0.736239 0.680438 0.5 0.681687 0.710036 0.678922 0.696504 0.82466 0.476534 0.77867 0.69243 0.6 0.683605 0.713805 0.683824 0.695589 0.823967 0.476534 0.817661 0.699284 0.7 0.685523 0.700968 0.64951 0.689365 0.816893 0.487365 0.829128 0.694107 0.8 0.686481 0.68538 0.64951 0.693209 0.801608 0.483755 0.837156 0.691014 0.9 0.684564 0.650229 0.671569 0.69687 0.775587 0.516245 0.833716 0.689826 1.0 0.667306 0.576566 0.661765 0.645616 0.72372 0.490975 0.822248 0.655456"},{"location":"modelpool/gpt2_classification/#experimental-results","title":"Experimental Results","text":"<p>Table: Multi-task model merging methods using GPT-2 models</p> Method CoLA MNLI MRPC QNLI QQP RTE SST-2 Avg. Fine-tuned (STL) 76.8 82.1 80.4 88.3 89.6 65.3 91.2 82.0 Model Merging Simple Average 55.0 55.1 51.0 57.6 76.7 44.8 52.5 56.1 Fisher Merging 54.8 58.0 39.5 63.3 81.5 49.1 64.7 58.7 RegMean 61.7 70.4 65.4 69.7 78.8 56.0 79.7 68.8 Task Arithmetic (\\(\\lambda=0.5\\)) 68.7 68.6 69.6 70.5 81.8 47.3 83.6 70.0 Ties-Merging (\\(\\lambda=0.6\\)) 68.4 71.4 68.4 69.6 82.4 47.7 81.8 70.0"},{"location":"modelpool/llama_models/","title":"LLaMa / Mistral Models","text":""},{"location":"modelpool/nyuv2/","title":"Scene Understanding on NYUv2 tasks","text":""},{"location":"modelpool/nyuv2/#examples","title":"Examples","text":""},{"location":"modelpool/nyuv2/#simple-average","title":"Simple Average","text":"<pre><code>fusion_bench --config-name nyuv2_config \\\n    method=simple_average\n</code></pre>"},{"location":"modelpool/nyuv2/#task-arithmetic","title":"Task Arithmetic","text":"<pre><code>fusion_bench --config-name nyuv2_config \\\n    method=task_arithmetic \\\n        method.scaling_factor=0.3\n</code></pre>"},{"location":"modelpool/nyuv2/#ties-merging","title":"Ties-Merging","text":"<pre><code>fusion_bench --config-name nyuv2_config \\\n    method=ties_merging \\\n        method.scaling_factor=0.3\n</code></pre>"},{"location":"modelpool/nyuv2/#experimental-results","title":"Experimental Results","text":"Method Segmentation (mIoU \\(\\uparrow\\)) Segmentation (Pix Acc \\(\\uparrow\\)) Depth Estimation (Abs Err \\(\\downarrow\\)) Depth Estimation (Rel Err \\(\\downarrow\\)) Normal (Mean \\(\\downarrow\\)) Single-Task Learning Segmentation 52.0 73.8 242.8 88.7 82.8 Depth Estimation 2.3 6.2 42.5 17.7 82.8 Normal 2.0 4.9 264.0 98.1 24.7 Multi-Task Model Fusion Methods Weight Averaging 39.0 67.0 55.1 22.7 30.4 Task Arithmetic (\\(\\lambda=0.3\\)) 33.6 63.3 56.3 23.2 31.3 Ties-Merging (\\(\\lambda=0.3\\)) 36.3 61.7 60.5 24.5 33.1"},{"location":"readinglist/","title":"Reading Lists","text":"<p>Info</p> <p>working in progress. Any suggestions are welcome.</p> <p>I've been compiling a comprehensive list of papers and resources that have been instrumental in my research journey.  This collection is designed to serve as a valuable starting point for those interested in delving into the field of deep model fusion. If you have any suggestions for papers to add, please feel free to raise an issue or submit a pull request.</p> <p>Note</p> <p>Meaning of the symbols in the list:</p> <ul> <li> Highly recommended</li> <li> LLaMA model-related or Mistral-related work</li> <li> Code available on GitHub</li> <li> models or datasets available on Hugging Face</li> </ul>"},{"location":"readinglist/#survey-papers","title":"Survey Papers","text":"<ul> <li> <p>      E. Yang et al., \u201cModel Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities.\u201d arXiv, Aug. 14, 2024.</p> Quote <p></p> </li> <li> <p> Yadav et al. A Survey on Model MoErging: Recycling and Routing Among Specialized Experts for Collaborative Learning arXiv:2408.07057</p> </li> <li>      W. Li, Y. Peng, M. Zhang, L. Ding, H. Hu, and L. Shen, \u201cDeep Model Fusion: A Survey.\u201d arXiv, Sep. 27, 2023. doi: 10.48550/arXiv.2309.15698.</li> <li>      H. Zheng et al., \u201cLearn From Model Beyond Fine-Tuning: A Survey.\u201d arXiv, Oct. 12, 2023.</li> </ul>"},{"location":"readinglist/#findings-on-model-fusion","title":"Findings on Model Fusion","text":"<ul> <li> Aakanksha et al. Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning arXiv:2410.10801</li> <li> Yadav et al. What Matters for Model Merging at Scale? arXiv:2410.03617</li> </ul>"},{"location":"readinglist/#model-ensemble","title":"Model Ensemble","text":"<ul> <li>Liu T Y, Soatto S. Tangent Model Composition for Ensembling and Continual Fine-tuning. arXiv, 2023.</li> <li> <p> Wan et al. Knowledge Fusion of Large Language Models arXiv:2401.10491</p> Quote <p></p> </li> <li> <p> Wan F, Yang Z, Zhong L, et al. FuseChat: Knowledge Fusion of Chat Models. arXiv, 2024.</p> Quote <p></p> </li> </ul>"},{"location":"readinglist/#model-merging","title":"Model Merging","text":""},{"location":"readinglist/#mode-connectivity","title":"Mode Connectivity","text":"<p>Mode connectivity is such an important concept in model merging that it deserves its own page.</p>"},{"location":"readinglist/#weight-interpolation","title":"Weight Interpolation","text":"<ul> <li> <p>Osowiechi et al. WATT: Weight Average Test-Time Adaptation of CLIP arXiv:2406.13875</p> Quote <p></p> </li> <li> <p>Jiang et al. ForkMerge: Mitigating Negative Transfer in Auxiliary-Task Learning</p> Quote <p></p> </li> <li> <p>Chronopoulou et al. Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot Summarization arXiv:2311.09344</p> Quote <p></p> </li> <li> <p>      L. Yu, B. Yu, H. Yu, F. Huang, and Y. Li, \u201cLanguage Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch,\u201d Nov. 06, 2023, arXiv: arXiv:2311.03099. Available: http://arxiv.org/abs/2311.03099</p> </li> <li>       E. Yang et al., \u201cAdaMerging: Adaptive Model Merging for Multi-Task Learning,\u201d ICLR 2024, arXiv: arXiv:2310.02575. doi: 10.48550/arXiv.2310.02575.</li> <li> P. Yadav, D. Tam, L. Choshen, C. Raffel, and M. Bansal, \u201cResolving Interference When Merging Models,\u201d Jun. 02, 2023, arXiv: arXiv:2306.01708. Available: http://arxiv.org/abs/2306.01708</li> <li> Guillermo Ortiz-Jimenez, Alessandro Favero, and Pascal Frossard, \u201cTask Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models,\u201d May 30, 2023, arXiv: arXiv:2305.12827. doi: 10.48550/arXiv.2305.12827.</li> <li> G. Ilharco et al., \u201cEditing Models with Task Arithmetic,\u201d Mar. 31, 2023, arXiv: arXiv:2212.04089. doi: 10.48550/arXiv.2212.04089.</li> <li> <p>Tang et al. Towards Efficient Pareto Set Approximation via Mixture of Experts Based Model Fusion</p> Quote <p></p> </li> <li> <p>Rame et al. Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards arXiv:2306.04488</p> </li> <li> <p>Huang et al. LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition arXiv:2307.13269</p> Quote <p></p> </li> <li> <p>Wu et al. Pi-Tuning: Transferring Multimodal Foundation Models with Optimal Multi-task Interpolation</p> Quote <p></p> </li> <li> <p>Chronopoulou et al. AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models arXiv:2302.07027</p> Quote <p></p> </li> <li> <p>Zimmer et al. Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging arXiv:2306.16788</p> Quote <p></p> </li> <li> <p> Wortsman et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time arXiv:2203.05482</p> </li> </ul>"},{"location":"readinglist/#alignment-based-methods","title":"Alignment-based Methods","text":"<ul> <li> <p>Kinderman et al. Foldable SuperNets: Scalable Merging of Transformers with Different Initializations and Tasks arXiv:2410.01483</p> Quote <p></p> </li> <li> <p>S. K. Ainsworth, J. Hayase, and S. Srinivasa, \u201cGit Re-Basin: Merging Models modulo Permutation Symmetries,\u201d ICLR 2023. Available: http://arxiv.org/abs/2209.04836</p> </li> <li>George Stoica, Daniel Bolya, Jakob Bjorner, Taylor Hearn, and Judy Hoffman, \u201cZipIt! Merging Models from Different Tasks without Training,\u201d May 04, 2023, arXiv: arXiv:2305.03053. Available: http://arxiv.org/abs/2305.03053</li> </ul>"},{"location":"readinglist/#subspace-based-methods","title":"Subspace-based Methods","text":"<ul> <li>Tang A, Shen L, Luo Y, et al. Concrete subspace learning based interference elimination for multi-task model fusion. arXiv preprint arXiv:2312.06173, 2023.</li> <li>       X. Yi, S. Zheng, L. Wang, X. Wang, and L. He, \u201cA safety realignment framework via subspace-oriented model fusion for large language models.\u201d arXiv, May 14, 2024. doi: 10.48550/arXiv.2405.09055.</li> <li> Wang K, Dimitriadis N, Ortiz-Jimenez G, et al. Localizing Task Information for Improved Model Merging and Compression. arXiv preprint arXiv:2405.07813, 2024.</li> </ul>"},{"location":"readinglist/#online-model-merging","title":"Online Model Merging","text":"<ul> <li> <p> Alexandrov el al. Mitigating Catastrophic Forgetting in Language Transfer via Model Merging arXiv:2407.08699</p> Quote <p> </p> </li> <li> <p> Lu et al. Online Merging Optimizers for Boosting Rewards and Mitigating Tax in Alignment arXiv:2405.17931</p> </li> <li>Izmailov et al. Averaging Weights Leads to Wider Optima and Better Generalization</li> <li>Kaddour et al. Stop Wasting My Time! Saving Days of ImageNet and BERT Training with Latest Weight Averaging arXiv:2209.14981</li> <li>Zhang et al. Lookahead Optimizer: k steps forward, 1 step back http://arxiv.org/abs/1907.08610</li> </ul>"},{"location":"readinglist/#model-mixingupscalingexpansion","title":"Model Mixing/Upscaling/Expansion","text":"<ul> <li> <p> Samragh et al. Scaling Smart: Accelerating Large Language Model Pre-training with Small Model Initialization arXiv:2409.12903</p> Quote <p></p> </li> <li> <p>Zhao et al. Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to Extremes Through Rank-Wise Clustering arXiv:2409.16167</p> Quote <p></p> </li> <li> <p>Tang et al. SMILE: Zero-Shot Sparse Mixture of Low-Rank Experts Construction From Pre-Trained Foundation Models arXiv:2408.10174</p> Quote <p></p> </li> <li> <p>      C. Chen et al., \u201cModel Composition for Multimodal Large Language Models.\u201d arXiv, Feb. 20, 2024. doi: 10.48550/arXiv.2402.12750.</p> </li> <li>A. Tang, L. Shen, Y. Luo, N. Yin, L. Zhang, and D. Tao, \u201cMerging Multi-Task Models via Weight-Ensembling Mixture of Experts,\u201d Feb. 01, 2024, arXiv: arXiv:2402.00433. doi: 10.48550/arXiv.2402.00433.</li> <li> <p>       Zhenyi Lu et al., \"Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging\" 10.48550/arXiv.2406.15479</p> Quote <p></p> </li> <li> <p>  Tang A, Shen L, Luo Y, et al. SMILE: Zero-Shot Sparse Mixture of Low-Rank Experts Construction From Pre-Trained Foundation Models. arXiv, 2024.</p> </li> <li> <p> Kim et al. SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling arXiv:2312.15166</p> Quote <p></p> </li> <li> <p>Komatsuzaki et al. Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints arXiv:2212.05055</p> Quote <p></p> </li> </ul>"},{"location":"readinglist/#benchmarks","title":"Benchmarks","text":"<ul> <li>Tam et al. Realistic Evaluation of Model Merging for Compositional Generalization arXiv:2409.18314</li> <li> Tang et al. FusionBench: A Comprehensive Benchmark of Deep Model Fusion.</li> </ul>"},{"location":"readinglist/#libraries-and-tools","title":"Libraries and Tools","text":""},{"location":"readinglist/#fine-tuning-preparing-models-for-fusion","title":"Fine-tuning, Preparing models for fusion","text":"<ul> <li>     PyTorch Classification: A PyTorch library for training/fine-tuning models (CNN, ViT, CLIP) on image classification tasks</li> <li>      LLaMA Factory: A PyTorch library for fine-tuning LLMs</li> </ul>"},{"location":"readinglist/#model-fusion","title":"Model Fusion","text":"<ul> <li>      FusionBench: A Comprehensive Benchmark of Deep Model Fusion.</li> <li>       MergeKit: A PyTorch library for merging large language models.</li> </ul>"},{"location":"readinglist/#version-control","title":"Version Control","text":"<ul> <li>Kandpal et al. Git-Theta: A Git Extension for Collaborative Development of Machine Learning Models arXiv:2306.04529</li> </ul>"},{"location":"readinglist/#other-applications-of-model-fusion","title":"Other Applications of Model Fusion","text":""},{"location":"readinglist/#applications-in-reinforcement-learning-rl","title":"Applications in Reinforcement Learning (RL)","text":"<ul> <li>(Survey Paper) Song Y, Suganthan P N, Pedrycz W, et al. Ensemble reinforcement learning: A survey. Applied Soft Computing, 2023.</li> <li> Lee K, Laskin M, Srinivas A, et al. \u201cSunrise: A simple unified framework for ensemble learning in deep reinforcement learning\", ICML, 2021.</li> <li>Ren J, Li Y, Ding Z, et al. \u201cProbabilistic mixture-of-experts for efficient deep reinforcement learning\". arXiv:2104.09122, 2021.</li> <li> Celik O, Taranovic A, Neumann G. \u201cAcquiring Diverse Skills using Curriculum Reinforcement Learning with Mixture of Experts\". arXiv preprint arXiv:2403.06966, 2024.</li> </ul>"},{"location":"readinglist/mode_connectivity/","title":"Mode-Connectivity","text":"<p>This is a collection of AWESOME things about recent researches about Mode Connectivity, mainly about the findings of Geometric Properties of learned neural networks.  Mode connectivity is an important property of the loss landscape of deep neural networks and is crucial for understanding weight interpolation-based model merging methods. For the detailed methods side, we include papers which try to rebasin the different learned models and how to directly induce a better model (generalization) from one-shot training. For the application side, downstream taks such as federated learning, continual learning and sparse neural network which may potentially benefit from the advances of the model connectivity research. Waited to be researched...</p>"},{"location":"readinglist/mode_connectivity/#geometric-properties","title":"Geometric Properties","text":""},{"location":"readinglist/mode_connectivity/#the-lmc","title":"The LMC","text":"<ul> <li>Topology and Geometry of Half-Rectified Network Optimization.</li> </ul> <p>C. Daniel Freeman, Joan Bruna [ICLR17]</p>"},{"location":"readinglist/mode_connectivity/#the-nonlienar-mc","title":"The Nonlienar MC","text":"<ul> <li>Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs. </li> </ul> <p>Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, Andrew Gordon Wilson [Neurips18][codes]</p> <ul> <li>Essentially No Barriers in Neural Network Energy Landscape. </li> </ul> <p>Felix Draxler, Kambis Veschgini, Manfred Salmhofer, Fred A. Hamprecht [ICML18][codes]</p> <ul> <li> Loss Surface Simplexes for Mode Connecting Volumes and Fast Ensembling.</li> </ul> <p>Gregory W. Benton, Wesley J. Maddox, Sanae Lotfi, Andrew Gordon Wilson [ICML21][codes]</p>"},{"location":"readinglist/mode_connectivity/#findings","title":"Findings","text":"<ul> <li>Geometry of the Loss Landscape in Overparameterized Neural Networks: Symmetries and Invariances.</li> </ul> <p>Berfin \u015eim\u015fek, Fran\u00e7ois Ged, Arthur Jacot, Francesco Spadaro, Cl\u00e9ment Hongler, Wulfram Gerstner, Johanni Brea [ICML21][codes]</p> <ul> <li>(Initialisations) Random initialisations performing above chance and how to find them. </li> </ul> <p>Frederik Benzing, Simon Schug, Robert Meier, Johannes von Oswald, Yassir Akram, Nicolas Zucchet, Laurence Aitchison, Angelika Steger [NeurIPS22 OPT][codes]</p> <ul> <li>Linear mode connectivity and the lottery ticket hypothesis. </li> </ul> <p>Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, Michael Carbin [ICML20][codes]</p> <ul> <li>(Functional behaviors of end points) On Convexity and Linear Mode Connectivity in Neural Networks. </li> </ul> <p>David Yunis, Kumar Kshitij Patel, Pedro Henrique Pamplona Savarese, Gal Vardi, Jonathan Frankle, Matthew Walter, Karen Livescu, Michael Maire [NeurIPS22 OPT]</p> <ul> <li>Large Scale Structure of Neural Network Loss Landscapes.</li> </ul> <p>Stanislav Fort, Stanislaw Jastrzebski [NeurIPS19]</p> <ul> <li>Plateau in Monotonic Linear Interpolation -- A \"Biased\" View of Loss Landscape for Deep Networks. </li> </ul> <p>Xiang Wang, Annie N. Wang, Mo Zhou, Rong Ge [ICLR23]</p> <ul> <li>Analyzing Monotonic Linear Interpolation in Neural Network Loss Landscapes. </li> </ul> <p>James Lucas, Juhan Bae, Michael R. Zhang, Stanislav Fort, Richard Zemel, Roger Grosse [ICML21][codes]</p>"},{"location":"readinglist/mode_connectivity/#theory","title":"Theory","text":"<ul> <li>Explaining landscape connectivity of low-cost solutions for multilayer nets.</li> </ul> <p>Rohith Kuditipudi, Xiang Wang, Holden Lee, Yi Zhang, Zhiyuan Li, Wei Hu, Sanjeev Arora, Rong Ge [NeurIPS19]</p>"},{"location":"readinglist/mode_connectivity/#methods-for-rebasin","title":"Methods for rebasin","text":"<ul> <li>(Width, Depth)(Simulated Annealing) The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks.</li> </ul> <p>Rahim Entezari, Hanie Sedghi, Olga Saukh, Behnam Neyshabur [ICLR22][codes]</p> <ul> <li>Optimizing mode connectivity via neuron alignment.</li> </ul> <p>N. Joseph Tatro, Pin-Yu Chen, Payel Das, Igor Melnyk, Prasanna Sattigeri, Rongjie Lai [NeurIPS19][codes]</p> <ul> <li>\ud83d\udd25  (Three methods) Git Re-Basin: Merging Models modulo Permutation Symmetries. </li> </ul> <p>Samuel K. Ainsworth, Jonathan Hayase, Siddhartha Srinivasa [ICLR23][codes][pytorch]</p> <ul> <li>Re-basin via implicit Sinkhorn differentiation.</li> </ul> <p>Fidel A. Guerrero Pe\u00f1a, Heitor Rapela Medeiros, Thomas Dubail, Masih Aminbeidokhti, Eric Granger, Marco Pedersoli [paper22]</p> <ul> <li>Linear Mode Connectivity of Deep Neural Networks via Permutation Invariance and Renormalization.</li> </ul> <p>Rahim Entezari, Hanie Sedghi, Olga Saukh, Behnam Neyshabur [ICLR23][codes]</p>"},{"location":"readinglist/mode_connectivity/#model-merging","title":"Model merging","text":"<ul> <li> [Stochastic Weight Averaging] Averaging Weights Leads to Wider Optima and Better Generalization. </li> </ul> <p>Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, Andrew Gordon Wilson [UAI18][codes]</p> <ul> <li>Subspace Inference for Bayesian Deep Learning.</li> </ul> <p>Pavel Izmailov, Wesley J. Maddox, Polina Kirichenko, Timur Garipov, Dmitry Vetrov, Andrew Gordon Wilson. [UAI19][codes]</p> <ul> <li>Bayesian Nonparametric Federated Learning of Neural Networks.</li> </ul> <p>Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Trong Nghia Hoang, and Yasaman Khazaeni. [ICML19][codes]</p> <ul> <li>Model fusion via optimal transport.</li> </ul> <p>Singh, Sidak Pal and Jaggi, Martin [NeurIPS20][codes]</p> <ul> <li>[Averaging merge] Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. </li> </ul> <p>Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, Ludwig Schmidt [ICML22][codes]</p> <ul> <li>lo-fi: distributed fine-tuning without communication. </li> </ul> <p>Mitchell Wortsman, Suchin Gururangan, Shen Li, Ali Farhadi, Ludwig Schmidt, Micheal Rabbat, Ari S. Morcos [TMLR23]</p> <ul> <li> Learning Neural Network Subspaces.</li> </ul> <p>Mitchell Wortsman, Maxwell Horton, Carlos Guestrin, Ali Farhadi, Mohammad Rastegari [ICML21][codes]</p> <ul> <li>Robust fine-tuning of zero-shot models. </li> </ul> <p>Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo-Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, Ludwig Schmidt [CVPR22][codes]</p> <ul> <li>[Fisher merge] Merging Models with Fisher-Weighted Averaging.</li> </ul> <p>Michael Matena, Colin Raffel [NeurIPS22][codes]</p> <ul> <li>[Regression Mean merge] Dataless Knowledge Fusion by Merging Weights of Language Models. </li> </ul> <p>Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, Pengxiang Cheng [ICLR23]</p> <ul> <li>Wasserstein Barycenter-based Model Fusion and Linear Mode Connectivity of Neural Networks. </li> </ul> <p>Aditya Kumar Akash, Sixu Li, Nicol\u00e1s Garc\u00eda Trillos [paper23][codes]</p> <ul> <li>PopulAtion Parameter Averaging (PAPA).</li> </ul> <p>Alexia Jolicoeur-Martineau, Emy Gervais, Kilian Fatras, Yan Zhang, Simon Lacoste-Julien [paper23]</p> <ul> <li>ZipIt! Merging Models from Different Tasks without Training.</li> </ul> <p>George Stoica, Daniel Bolya, Jakob Bjorner, Taylor Hearn, and Judy Hoffman [paper23]</p>"},{"location":"readinglist/mode_connectivity/#pretrained-model-connectivity","title":"Pretrained model connectivity","text":"<ul> <li>What is being transferred in transfer learning? </li> </ul> <p>Behnam Neyshabur, Hanie Sedghi, Chiyuan Zhang [NeurIPS20][codes]</p> <ul> <li>Exploring Mode Connectivity for Pre-trained Language Models. </li> </ul> <p>Yujia Qin, Cheng Qian, Jing Yi, Weize Chen, Yankai Lin, Xu Han, Zhiyuan Liu, Maosong Sun, Jie Zhou [EMNLP22][codes]</p> <ul> <li>Knowledge is a Region in Weight Space for Fine-tuned Language Models. </li> </ul> <p>Almog Gueta, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, Leshem Choshen [paper23]</p>"},{"location":"readinglist/mode_connectivity/#equivariant-network-design","title":"Equivariant Network Design","text":"<ul> <li>\ud83d\udc40 Equivariant Architectures for Learning in Deep Weight Spaces. </li> </ul> <p>Aviv Navon, Aviv Shamsian, Idan Achituve, Ethan Fetaya, Gal Chechik, Haggai Maron [paper23][codes]</p> <ul> <li>\ud83d\udc40 Permutation Equivariant Neural Functionals. </li> </ul> <p>Allan Zhou, Kaien Yang, Kaylee Burns, Yiding Jiang, Samuel Sokota, J. Zico Kolter, Chelsea Finn [paper23]</p>"},{"location":"readinglist/mode_connectivity/#related-paper","title":"Related paper","text":"<ul> <li>Entropy-SGD optimizes the prior of a PAC-Bayes bound: Generalization properties of Entropy-SGD and data-dependent priors. </li> </ul> <p>Gintare Karolina Dziugaite, Daniel Roy [ICML18]</p> <ul> <li>Sharpness-Aware Minimization for Efficiently Improving Generalization. </li> </ul> <p>Pierre Foret, Ariel Kleiner, Hossein Mobahi, Behnam Neyshabur [ICLR21][codes]</p> <ul> <li>Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization.</li> </ul> <p>Alexandre Ram\u00e9, Kartik Ahuja, Jianyu Zhang, Matthieu Cord, L\u00e9on Bottou, David Lopez-Paz [paper23][codes]</p>"},{"location":"readinglist/mode_connectivity/#applications","title":"Applications","text":"<ul> <li>(FL) Connecting Low-Loss Subspace for Personalized Federated Learning. </li> </ul> <p>Seok-Ju Hahn, Minwoo Jeong, Junghye Lee [KDD22][codes]</p> <ul> <li>Linear Mode Connectivity in Multitask and Continual Learning. </li> </ul> <p>Seyed Iman Mirzadeh, Mehrdad Farajtabar, Dilan Gorur, Razvan Pascanu, Hassan Ghasemzadeh [ICLR21][codes]</p> <ul> <li>All Roads Lead to Rome? On Invariance of BERT Representations. </li> </ul> <p>Yuxin Ren, Qipeng Guo, Zhijing Jin, Shauli Ravfogel, Mrinmaya Sachan, Ryan Cotterell, Bernhard Sch\u00f6lkopf [TACL23]</p> <ul> <li>(Meta Learning) Subspace Learning for Effective Meta-Learning.</li> </ul> <p>Weisen Jiang, James Kwok, Yu Zhang [ICML22][codes]</p> <ul> <li>(Incremental Learning) Towards better plasticity-stability trade-off in incremental learning: a simple linear connector.</li> </ul> <p>Guoliang Lin, Hanlu Chu, Hanjiang Lai [CVPR22][codes]</p> <ul> <li> (Sparsity) Unmasking the Lottery Ticket Hypothesis: What's Encoded in a Winning Ticket's Mask? </li> </ul> <p>Mansheej Paul, Feng Chen, Brett W. Larsen, Jonathan Frankle, Surya Ganguli, Gintare Karolina Dziugaite [ICLR23]</p> <ul> <li> Improving Ensemble Distillation With Weight Averaging and Diversifying Perturbation.</li> </ul> <p>Giung Nam, Hyungi Lee, Byeongho Heo, Juho Lee [ICML22][codes]</p> <ul> <li>LCS: Learning Compressible Subspaces for Efficient, Adaptive, Real-Time Network Compression at Inference Time</li> </ul> <p>Elvis Nunez, Maxwell Horton, Anish Prabhu, Anurag Ranjan, Ali Farhadi, Mohammad Rastegari[WACV23][codes]</p> <ul> <li>(OOD) Diverse Weight Averaging for Out-of-Distribution Generalization</li> </ul> <p>Alexandre Ram\u00e9, Matthieu Kirchmeyer, Thibaud Rahier, Alain Rakotomamonjy, Patrick Gallinari, Matthieu Cord[NeurIPS22][codes]</p> <ul> <li>Linear Connectivity Reveals Generalization Strategies. </li> </ul> <p>Jeevesh Juneja, Rachit Bansal, Kyunghyun Cho, Jo\u00e3o Sedoc, Naomi Saphra [ICLR23][codes]</p>"},{"location":"readinglist/mode_connectivity/#tools","title":"Tools","text":"<ul> <li>Loss landscapes. For loss landscape visualization and analysis.     [pypi]</li> </ul>"},{"location":"taskpool/","title":"Introduction to Taskpool Module","text":"<p>A taskpool is a collection of tasks that can be used to evaluate the performance of merged models. Each task in the taskpool is defined by a dataset and a metric.</p> <p>A taskpool is specified by a <code>yaml</code> configuration file, which often contains the following fields:</p> <ul> <li><code>type</code>: The type of the taskpool.</li> <li><code>dataset_type</code>: The type of the dataset used in the tasks.</li> <li><code>tasks</code>: A list of tasks, each task is dict with the following fields:<ul> <li><code>name</code>: The name of the task.</li> <li><code>dataset</code>: The dataset used for the task.</li> <li><code>metric</code>: The metric used to evaluate the performance of the model on the task.</li> </ul> </li> </ul>"},{"location":"taskpool/#references","title":"References","text":""},{"location":"taskpool/#fusion_bench.taskpool.BaseTaskPool","title":"<code>BaseTaskPool</code>","text":"<p>               Bases: <code>BaseYAMLSerializableModel</code></p> Source code in <code>fusion_bench/taskpool/base_pool.py</code> <pre><code>class BaseTaskPool(BaseYAMLSerializableModel):\n    _program = None\n\n    @abstractmethod\n    def evaluate(self, model, *args, **kwargs):\n        \"\"\"\n        Evaluate the model on all tasks in the task pool, and return a report.\n\n        Take image classification as an example, the report will look like:\n\n        ```python\n        {\n            \"mnist\": {\n                \"accuracy\": 0.8,\n                \"loss\": 0.2,\n            },\n            &lt;task_name&gt;: {\n                &lt;metric_name&gt;: &lt;metric_value&gt;,\n                ...\n            },\n        }\n        ```\n\n        Args:\n            model: The model to evaluate.\n\n        Returns:\n            report (dict): A dictionary containing the results of the evaluation for each task.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"taskpool/#fusion_bench.taskpool.BaseTaskPool.evaluate","title":"<code>evaluate(model, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Evaluate the model on all tasks in the task pool, and return a report.</p> <p>Take image classification as an example, the report will look like:</p> <pre><code>{\n    \"mnist\": {\n        \"accuracy\": 0.8,\n        \"loss\": 0.2,\n    },\n    &lt;task_name&gt;: {\n        &lt;metric_name&gt;: &lt;metric_value&gt;,\n        ...\n    },\n}\n</code></pre> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>report</code> (              <code>dict</code> )          \u2013            <p>A dictionary containing the results of the evaluation for each task.</p> </li> </ul> Source code in <code>fusion_bench/taskpool/base_pool.py</code> <pre><code>@abstractmethod\ndef evaluate(self, model, *args, **kwargs):\n    \"\"\"\n    Evaluate the model on all tasks in the task pool, and return a report.\n\n    Take image classification as an example, the report will look like:\n\n    ```python\n    {\n        \"mnist\": {\n            \"accuracy\": 0.8,\n            \"loss\": 0.2,\n        },\n        &lt;task_name&gt;: {\n            &lt;metric_name&gt;: &lt;metric_value&gt;,\n            ...\n        },\n    }\n    ```\n\n    Args:\n        model: The model to evaluate.\n\n    Returns:\n        report (dict): A dictionary containing the results of the evaluation for each task.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"taskpool/#fusion_bench.taskpool.BaseTaskPool.evaluate(model)","title":"<code>model</code>","text":"\u2013            <p>The model to evaluate.</p>"},{"location":"taskpool/LlamaTestGenerationTaskPool/","title":"LlamaTestGenerationTaskPool","text":"<p>The <code>LlamaTestGenerationTaskPool</code> class is used to evaluate a language model on a set of prompts. It can also be used in an interactive mode for debugging purposes.</p>"},{"location":"taskpool/LlamaTestGenerationTaskPool/#references","title":"References","text":""},{"location":"taskpool/LlamaTestGenerationTaskPool/#fusion_bench.taskpool.llama.test_generation","title":"<code>test_generation</code>","text":""},{"location":"taskpool/LlamaTestGenerationTaskPool/#fusion_bench.taskpool.llama.test_generation.LlamaTestGenerationTaskPool","title":"<code>LlamaTestGenerationTaskPool</code>","text":"<p>               Bases: <code>BaseTaskPool</code></p> <p>This task pool is used to evaluate a language model on a set of prompts. For the purpose of debugging, it can also be used in an interactive mode.</p> Source code in <code>fusion_bench/taskpool/llama/test_generation.py</code> <pre><code>class LlamaTestGenerationTaskPool(BaseTaskPool):\n    \"\"\"\n    This task pool is used to evaluate a language model on a set of prompts.\n    For the purpose of debugging, it can also be used in an interactive mode.\n    \"\"\"\n\n    def __init__(\n        self,\n        test_prompts: List[str],\n        max_length: int = 1024,\n        temperature: float = 0.01,\n        top_p: float = 0.9,\n        iterative_mode: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Args:\n            test_prompts (List[str]): A list of prompts to be used for testing the model.\n            max_length (int, optional): The maximum length of the generated text. Defaults to 1024.\n            temperature (float, optional): The sampling temperature for text generation. Defaults to 0.01.\n            top_p (float, optional): The cumulative probability for nucleus sampling. Defaults to 0.9.\n            iterative_mode (bool, optional): If True, enables interactive mode for debugging. Defaults to False.\n        \"\"\"\n        self.test_prompts = test_prompts\n        self.max_length = max_length\n        self.temperature = temperature\n        self.top_p = top_p\n        self.iterative_mode = iterative_mode\n        super().__init__(**kwargs)\n\n    def evaluate(\n        self,\n        model: Union[\"LlamaForCausalLM\", Any],\n        tokenizer: Optional[\"PreTrainedTokenizer\"] = None,\n    ):\n        if tokenizer is None:\n            if self._program is None:\n                log.error(\n                    \"`_program` is not set. This is probably happening when you are not runing the program via `fusion_bench` CLI.\"\n                    \"Please pass `tokenizer` to this function.\"\n                )\n            modelpool: \"CausalLMPool\" = self._program.modelpool\n            tokenizer = modelpool.load_tokenizer()\n\n        report = get_model_summary(model)\n        if self.test_prompts is not None:\n            for prompt_idx, prompt in enumerate(self.test_prompts):\n                print(f\"=== Generating text {prompt_idx+1}/{len(self.test_prompts)}\")\n                report[f\"conversation_{prompt_idx+1}\"] = self._generate_text(\n                    model, tokenizer, prompt\n                )\n\n        if self.iterative_mode:\n            for prompt_idx in itertools.count():\n                # Prompt for input\n                # print usage instructions\n                print(\"Enter a prompt to generate text. Type 'exit' to exit the loop.\")\n                prompt = input(\n                    f\"Enter a prompt, or type 'exit' to quit ({prompt_idx+1}): \"\n                )\n                if prompt == \"exit\":\n                    break\n                report[f\"iterative_conversation_{prompt_idx+1}\"] = self._generate_text(\n                    model, tokenizer, prompt\n                )\n\n        return report\n\n    def _generate_text(\n        self, model: \"LlamaForCausalLM\", tokenizer: \"PreTrainedTokenizer\", prompt: str\n    ) -&gt; dict:\n        \"\"\"\n        Generate text using the provided model and tokenizer for a given prompt.\n\n        This method generates text based on the given prompt using the specified model and tokenizer.\n        It prints the prompt and the generated response, and returns a dictionary containing the prompt,\n        response, wall time, number of characters, and number of tokens.\n\n        Args:\n            model: The language model to be used for text generation.\n            tokenizer: The tokenizer to be used for encoding and decoding text.\n            prompt (str): The input prompt for text generation.\n\n        Returns:\n            dict: A dictionary containing the following keys:\n                - \"prompt\" (str): The input prompt.\n                - \"response\" (str): The generated response.\n                - \"wall_time\" (float): The time taken to generate the response.\n                - \"num_chars\" (int): The number of characters in the generated response.\n                - \"num_tokens\" (int): The number of tokens in the generated response.\n        \"\"\"\n        print(prompt)\n        start_time = time.time()\n        outputs = generate_text(\n            model,\n            tokenizer=tokenizer,\n            prompt=prompt,\n            max_length=self.max_length,\n            temperature=self.temperature,\n            top_p=self.top_p,\n        )\n        print_bordered(\n            outputs[\"response\"], title=\"Generated Text\", code_style=\"markdown\"\n        )\n        print(\"\\n\")\n        return {\n            \"prompt\": prompt,\n            \"response\": outputs[\"response\"],\n            \"wall_time\": time.time() - start_time,\n            \"num_chars\": len(outputs[\"response\"]),\n            \"num_tokens\": outputs[\"num_tokens\"],\n        }\n</code></pre>"},{"location":"taskpool/LlamaTestGenerationTaskPool/#fusion_bench.taskpool.llama.test_generation.LlamaTestGenerationTaskPool.__init__","title":"<code>__init__(test_prompts, max_length=1024, temperature=0.01, top_p=0.9, iterative_mode=False, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>test_prompts</code> \u00b6              (<code>List[str]</code>)           \u2013            <p>A list of prompts to be used for testing the model.</p> </li> <li> <code>max_length</code> \u00b6              (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The maximum length of the generated text. Defaults to 1024.</p> </li> <li> <code>temperature</code> \u00b6              (<code>float</code>, default:                   <code>0.01</code> )           \u2013            <p>The sampling temperature for text generation. Defaults to 0.01.</p> </li> <li> <code>top_p</code> \u00b6              (<code>float</code>, default:                   <code>0.9</code> )           \u2013            <p>The cumulative probability for nucleus sampling. Defaults to 0.9.</p> </li> <li> <code>iterative_mode</code> \u00b6              (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, enables interactive mode for debugging. Defaults to False.</p> </li> </ul> Source code in <code>fusion_bench/taskpool/llama/test_generation.py</code> <pre><code>def __init__(\n    self,\n    test_prompts: List[str],\n    max_length: int = 1024,\n    temperature: float = 0.01,\n    top_p: float = 0.9,\n    iterative_mode: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    Args:\n        test_prompts (List[str]): A list of prompts to be used for testing the model.\n        max_length (int, optional): The maximum length of the generated text. Defaults to 1024.\n        temperature (float, optional): The sampling temperature for text generation. Defaults to 0.01.\n        top_p (float, optional): The cumulative probability for nucleus sampling. Defaults to 0.9.\n        iterative_mode (bool, optional): If True, enables interactive mode for debugging. Defaults to False.\n    \"\"\"\n    self.test_prompts = test_prompts\n    self.max_length = max_length\n    self.temperature = temperature\n    self.top_p = top_p\n    self.iterative_mode = iterative_mode\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"taskpool/LlamaTestGenerationTaskPool/#fusion_bench.taskpool.llama.test_generation.generate_text","title":"<code>generate_text(model, tokenizer, prompt, max_length=1024, temperature=0.01, top_p=0.9, device=None)</code>","text":"<p>Generate text using the loaded model.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>str</code>          \u2013            <p>Generated text</p> </li> </ul> Source code in <code>fusion_bench/taskpool/llama/test_generation.py</code> <pre><code>def generate_text(\n    model: \"LlamaForCausalLM\",\n    tokenizer: \"PreTrainedTokenizer\",\n    prompt: str,\n    max_length: int = 1024,\n    temperature: float = 0.01,\n    top_p=0.9,\n    device: torch.device = None,\n):\n    \"\"\"\n    Generate text using the loaded model.\n\n    Args:\n        model: The loaded language model\n        tokenizer: The loaded tokenizer\n        prompt (str): Input prompt text\n        max_length (int): Maximum length of generated sequence\n        temperature (float): Controls randomness (higher = more random)\n        top_p (float): Nucleus sampling parameter\n\n    Returns:\n        str: Generated text\n    \"\"\"\n    if device is None:\n        device = get_device(model)\n\n    # Encode the prompt\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n    # Move to GPU if available\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    # Generate\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_length=max_length,\n            temperature=temperature,\n            top_p=top_p,\n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n            do_sample=True,\n        )\n\n    # Decode and return the generated text\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n    response = generated_text[len(prompt) :]\n    return {\n        \"generated_text\": generated_text,\n        \"response\": response,\n        \"num_tokens\": len(outputs[0]) - len(inputs[\"input_ids\"][0]),\n    }\n</code></pre>"},{"location":"taskpool/LlamaTestGenerationTaskPool/#fusion_bench.taskpool.llama.test_generation.generate_text(model)","title":"<code>model</code>","text":"(<code>LlamaForCausalLM</code>)           \u2013            <p>The loaded language model</p>"},{"location":"taskpool/LlamaTestGenerationTaskPool/#fusion_bench.taskpool.llama.test_generation.generate_text(tokenizer)","title":"<code>tokenizer</code>","text":"(<code>PreTrainedTokenizer</code>)           \u2013            <p>The loaded tokenizer</p>"},{"location":"taskpool/LlamaTestGenerationTaskPool/#fusion_bench.taskpool.llama.test_generation.generate_text(prompt)","title":"<code>prompt</code>","text":"(<code>str</code>)           \u2013            <p>Input prompt text</p>"},{"location":"taskpool/LlamaTestGenerationTaskPool/#fusion_bench.taskpool.llama.test_generation.generate_text(max_length)","title":"<code>max_length</code>","text":"(<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>Maximum length of generated sequence</p>"},{"location":"taskpool/LlamaTestGenerationTaskPool/#fusion_bench.taskpool.llama.test_generation.generate_text(temperature)","title":"<code>temperature</code>","text":"(<code>float</code>, default:                   <code>0.01</code> )           \u2013            <p>Controls randomness (higher = more random)</p>"},{"location":"taskpool/LlamaTestGenerationTaskPool/#fusion_bench.taskpool.llama.test_generation.generate_text(top_p)","title":"<code>top_p</code>","text":"(<code>float</code>, default:                   <code>0.9</code> )           \u2013            <p>Nucleus sampling parameter</p>"},{"location":"taskpool/clip_vit_classification/","title":"Image Classification Tasks for CLIP Models","text":""},{"location":"taskpool/clip_vit_classification/#clipvisionmodeltaskpool","title":"CLIPVisionModelTaskPool","text":"<p>The <code>CLIPVisionModelTaskPool</code> class is used to define image classification tasks for CLIP models. It provides methods to evaluate the performance of a given model on multiple datasets.</p>"},{"location":"taskpool/clip_vit_classification/#attributes","title":"Attributes","text":"<ul> <li><code>test_datasets</code>: A dictionary containing the test datasets.</li> <li><code>processor</code>: The processor used for preprocessing the input data. This is used to set up the classifier.</li> <li><code>data_processor</code>: The data processor used for processing the input data.</li> <li><code>clip_model</code>: The CLIP model used for evaluation.</li> <li><code>dataloader_kwargs</code>: Keyword arguments for the data loader.</li> <li><code>layer_wise_feature_save_path</code>: Path to save the layer-wise features.</li> <li><code>layer_wise_feature_first_token_only</code>: Boolean indicating whether to save only the first token of the features.</li> <li><code>layer_wise_feature_max_num</code>: Maximum number of features to save.</li> <li><code>fast_dev_run</code>: Boolean indicating whether to run in fast development mode.</li> </ul>"},{"location":"taskpool/clip_vit_classification/#methods","title":"Methods","text":"<ul> <li><code>setup()</code>: Sets up the processor, data processor, CLIP model, test datasets, and data loaders.</li> <li><code>evaluate(model)</code>: Evaluates the given model on the image classification task.</li> <li><code>on_task_evaluation_begin(classifier, task_name)</code>: Called at the beginning of task evaluation to set up hooks for saving layer-wise features.</li> <li><code>on_task_evaluation_end()</code>: Called at the end of task evaluation to save features and remove hooks.</li> </ul>"},{"location":"taskpool/clip_vit_classification/#configuration","title":"Configuration","text":"<p>The <code>CLIPVisionModelTaskPool</code> class can be configured using a YAML file. Here is an example configuration:</p> <pre><code>test_datasets:\n  dataset1: ...\n  dataset2: ...\nprocessor:\n  _target_: transformers.CLIPProcessor.from_pretrained\n  pretrained_model_name_or_path: openai/clip-vit-base-patch32\ndata_processor:\n  _target_: transformers.CLIPProcessor.from_pretrained\n  pretrained_model_name_or_path: openai/clip-vit-base-patch32\nclip_model:\n  _target_: transformers.CLIPModel.from_pretrained\n  pretrained_model_name_or_path: openai/clip-vit-base-patch32\ndataloader_kwargs:\n  batch_size: 32\n  num_workers: 4\nlayer_wise_feature_save_path: path/to/save/features\nlayer_wise_feature_first_token_only: true\nlayer_wise_feature_max_num: 1000\nfast_dev_run: false\n</code></pre>"},{"location":"taskpool/clip_vit_classification/#references","title":"References","text":""},{"location":"taskpool/clip_vit_classification/#fusion_bench.taskpool.CLIPVisionModelTaskPool","title":"<code>CLIPVisionModelTaskPool</code>","text":"<p>               Bases: <code>BaseTaskPool</code>, <code>LightningFabricMixin</code></p> <p>This class is used to define the image classification task for CLIP models.</p> <p>Attributes:</p> <ul> <li> <code>test_datasets</code>               (<code>Union[DictConfig, Dict[str, Dataset]]</code>)           \u2013            <p>The test datasets to evaluate the model on.</p> </li> <li> <code>processor</code>               (<code>Union[DictConfig, CLIPProcessor]</code>)           \u2013            <p>The processor used for preprocessing the input data.</p> </li> <li> <code>data_processor</code>               (<code>Union[DictConfig, CLIPProcessor]</code>)           \u2013            <p>The data processor used for processing the input data.</p> </li> <li> <code>clip_model</code>               (<code>Union[DictConfig, CLIPModel]</code>)           \u2013            <p>The CLIP model used for evaluation.</p> </li> <li> <code>dataloader_kwargs</code>               (<code>DictConfig</code>)           \u2013            <p>Keyword arguments for the data loader.</p> </li> <li> <code>layer_wise_feature_save_path</code>               (<code>Optional[str]</code>)           \u2013            <p>Path to save the layer-wise features.</p> </li> <li> <code>layer_wise_feature_first_token_only</code>               (<code>bool</code>)           \u2013            <p>Boolean indicating whether to save only the first token of the features.</p> </li> <li> <code>layer_wise_feature_max_num</code>               (<code>Optional[int]</code>)           \u2013            <p>Maximum number of features to save.</p> </li> <li> <code>fast_dev_run</code>               (<code>bool</code>)           \u2013            <p>Boolean indicating whether to run in fast development mode.</p> </li> </ul> Source code in <code>fusion_bench/taskpool/clip_vision/taskpool.py</code> <pre><code>class CLIPVisionModelTaskPool(\n    BaseTaskPool,\n    LightningFabricMixin,\n):\n    \"\"\"\n    This class is used to define the image classification task for CLIP models.\n\n    Attributes:\n        test_datasets (Union[DictConfig, Dict[str, Dataset]]): The test datasets to evaluate the model on.\n        processor (Union[DictConfig, CLIPProcessor]): The processor used for preprocessing the input data.\n        data_processor (Union[DictConfig, CLIPProcessor]): The data processor used for processing the input data.\n        clip_model (Union[DictConfig, CLIPModel]): The CLIP model used for evaluation.\n        dataloader_kwargs (DictConfig): Keyword arguments for the data loader.\n        layer_wise_feature_save_path (Optional[str]): Path to save the layer-wise features.\n        layer_wise_feature_first_token_only (bool): Boolean indicating whether to save only the first token of the features.\n        layer_wise_feature_max_num (Optional[int]): Maximum number of features to save.\n        fast_dev_run (bool): Boolean indicating whether to run in fast development mode.\n    \"\"\"\n\n    _is_setup = False\n\n    # hooks and handles for saving layer-wise features\n    _layer_wise_feature_save_hooks: Dict[int, LayerWiseFeatureSaver] = {}\n    _layer_wise_feature_save_hook_handles: Dict[int, RemovableHandle] = {}\n\n    _config_mapping = BaseTaskPool._config_mapping | {\n        \"_test_datasets\": \"test_datasets\",\n        \"_processor\": \"processor\",\n        \"_data_processor\": \"data_processor\",\n        \"_clip_model\": \"clip_model\",\n        \"_dataloader_kwargs\": \"dataloader_kwargs\",\n        \"_layer_wise_feature_save_path\": \"layer_wise_feature_save_path\",\n        \"fast_dev_run\": \"fast_dev_run\",\n    }\n\n    def __init__(\n        self,\n        test_datasets: Union[DictConfig, Dict[str, Dataset]],\n        *,\n        processor: Union[DictConfig, CLIPProcessor],\n        data_processor: Union[DictConfig, CLIPProcessor],\n        clip_model: Union[DictConfig, CLIPModel],\n        dataloader_kwargs: DictConfig = None,\n        layer_wise_feature_save_path: Optional[str] = None,\n        layer_wise_feature_first_token_only: bool = True,\n        layer_wise_feature_max_num: Optional[int] = None,\n        fast_dev_run: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the CLIPVisionModelTaskPool.\n        \"\"\"\n        self._test_datasets = test_datasets\n        self._processor = processor\n        self._data_processor = data_processor\n        self._clip_model = clip_model\n        self._dataloader_kwargs = dataloader_kwargs or {}\n\n        # layer-wise feature saving\n        self._layer_wise_feature_save_path = layer_wise_feature_save_path\n        self.layer_wise_feature_save_path = (\n            Path(layer_wise_feature_save_path)\n            if layer_wise_feature_save_path is not None\n            else None\n        )\n        self.layer_wise_feature_first_token_only = layer_wise_feature_first_token_only\n        self.layer_wise_feature_max_num = layer_wise_feature_max_num\n\n        self.fast_dev_run = fast_dev_run\n        super().__init__(**kwargs)\n\n    def setup(self):\n        \"\"\"\n        Set up the processor, data processor, CLIP model, test datasets, and data loaders.\n        \"\"\"\n        # setup processor and clip model\n        self.processor = (\n            instantiate(self._processor)\n            if isinstance(self._processor, DictConfig)\n            else self._processor\n        )\n        self.data_processor = (\n            instantiate(self._data_processor)\n            if isinstance(self._data_processor, DictConfig)\n            else self._data_processor\n        )\n        self.clip_model = (\n            instantiate(self._clip_model)\n            if isinstance(self._clip_model, DictConfig)\n            else self._clip_model\n        )\n        self.clip_model = self.fabric.to_device(self.clip_model)\n        self.clip_model.requires_grad_(False)\n        self.clip_model.eval()\n\n        # Load the test datasets\n        self.test_datasets = {\n            name: instantiate(dataset) if isinstance(dataset, DictConfig) else dataset\n            for name, dataset in self._test_datasets.items()\n        }\n        self.test_datasets = {\n            name: CLIPDataset(dataset, self.data_processor)\n            for name, dataset in self.test_datasets.items()\n        }\n        # Setup the dataloaders\n        self.test_dataloaders = {\n            name: DataLoader(\n                dataset,\n                **self._dataloader_kwargs,\n                collate_fn=(\n                    raw_image_collate_fn if self.data_processor is None else None\n                ),\n            )\n            for name, dataset in self.test_datasets.items()\n        }\n        self.test_dataloaders = {\n            name: self.fabric.setup_dataloaders(dataloader)\n            for name, dataloader in self.test_dataloaders.items()\n        }\n\n        self._is_setup = True\n\n    @torch.no_grad()\n    def _evaluate(\n        self,\n        classifier: HFCLIPClassifier,\n        test_loader: DataLoader,\n        num_classes: int,\n        task_name: str = None,\n    ):\n        \"\"\"\n        Evaluate the classifier on the test dataset (single-task evaluation).\n\n        Args:\n            classifier (HFCLIPClassifier): The classifier to evaluate.\n            test_loader (DataLoader): The data loader for the test dataset.\n            num_classes (int): The number of classes in the classification task.\n            task_name (str): The name of the task.\n\n        Returns:\n            Dict[str, float]: A dictionary containing the accuracy and loss of the classifier on the test dataset.\n        \"\"\"\n        accuracy: MulticlassAccuracy = Accuracy(\n            task=\"multiclass\", num_classes=num_classes\n        )\n        classifier.eval()\n        loss_metric = MeanMetric()\n        # if fast_dev_run is set, we only evaluate on a batch of the data\n        if self.fast_dev_run:\n            log.info(\"Running under fast_dev_run mode, evaluating on a single batch.\")\n            test_loader = itertools.islice(test_loader, 1)\n        else:\n            test_loader = test_loader\n\n        pbar = tqdm(\n            test_loader,\n            desc=f\"Evaluating {task_name}\",\n            leave=False,\n            dynamic_ncols=True,\n        )\n        for batch in pbar:\n            inputs, targets = batch\n            outputs = classifier(\n                inputs,\n                return_image_embeds=True,\n                return_dict=True,\n                task_name=task_name,\n            )\n            logits: Tensor = outputs[\"logits\"]\n\n            loss = F.cross_entropy(logits, targets)\n            loss_metric.update(loss.detach().cpu())\n            acc = accuracy(logits.detach().cpu(), targets.detach().cpu())\n            pbar.set_postfix(\n                {\n                    \"accuracy\": accuracy.compute().item(),\n                    \"loss\": loss_metric.compute().item(),\n                }\n            )\n\n        acc = accuracy.compute().item()\n        loss = loss_metric.compute().item()\n        results = {\"accuracy\": acc, \"loss\": loss}\n        return results\n\n    def evaluate(\n        self,\n        model: Union[CLIPVisionModel, CLIPVisionTransformer],\n        name=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Evaluate the model on the image classification task.\n\n        Args:\n            model (Union[CLIPVisionModel, CLIPVisionTransformer]): The model to evaluate.\n            name (Optional[str]): The name of the model. This will be logged into the report if not None.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing the evaluation results for each task.\n        \"\"\"\n        if not self._is_setup:\n            self.setup()\n\n        report = {}\n        # CLIPVisionModel works the same with CLIPVisonTransformer, so we can use it directly\n        if hasattr(model, \"is_surgery_model\") and model.is_surgery_model:\n            log.info(\"running evaluation on a surgery model.\")\n            model: \"SurgeryModelWrapper\" = model\n            self.clip_model.vision_model = model\n        else:\n            # replace the vision encoder with the model\n            self.clip_model.vision_model = model\n        classifier = HFCLIPClassifier(\n            self.clip_model,\n            processor=self.processor,\n        )\n        classifier = cast(HFCLIPClassifier, self.fabric.to_device(classifier))\n        # collect basic model information\n        training_params, all_params = count_parameters(model)\n        report[\"model_info\"] = {\n            \"trainable_params\": training_params,\n            \"all_params\": all_params,\n            \"trainable_percentage\": training_params / all_params,\n        }\n        if name is not None:\n            report[\"model_info\"][\"name\"] = name\n\n        # evaluate on each task\n        pbar = tqdm(\n            self.test_dataloaders.items(),\n            desc=\"Evaluating tasks\",\n            total=len(self.test_dataloaders),\n        )\n        for task_name, test_dataloader in pbar:\n            classnames, templates = get_classnames_and_templates(task_name)\n            self.on_task_evaluation_begin(classifier, task_name)\n            classifier.set_classification_task(classnames, templates)\n            result = self._evaluate(\n                classifier,\n                test_dataloader,\n                num_classes=len(classnames),\n                task_name=task_name,\n            )\n            report[task_name] = result\n            self.on_task_evaluation_end()\n\n        # calculate the average accuracy and loss\n        if \"average\" not in report:\n            report[\"average\"] = {}\n            accuracies = [\n                value[\"accuracy\"]\n                for key, value in report.items()\n                if \"accuracy\" in value\n            ]\n            if len(accuracies) &gt; 0:\n                average_accuracy = sum(accuracies) / len(accuracies)\n                report[\"average\"][\"accuracy\"] = average_accuracy\n            losses = [value[\"loss\"] for key, value in report.items() if \"loss\" in value]\n            if len(losses) &gt; 0:\n                average_loss = sum(losses) / len(losses)\n                report[\"average\"][\"loss\"] = average_loss\n\n        log.info(f\"Evaluation Result: {report}\")\n        if self.fabric.is_global_zero and len(self.fabric._loggers) &gt; 0:\n            with open(os.path.join(self.log_dir, \"report.json\"), \"w\") as fp:\n                json.dump(report, fp)\n        return report\n\n    def on_task_evaluation_begin(self, classifier: HFCLIPClassifier, task_name: str):\n        \"\"\"\n        Called at the beginning of task evaluation to set up hooks for saving layer-wise features.\n\n        Args:\n            classifier (HFCLIPClassifier): The classifier being evaluated.\n            task_name (str): The name of the task being evaluated.\n        \"\"\"\n        if self.layer_wise_feature_save_path is not None:\n            # setup hooks for saving layer-wise features\n            assert isinstance(\n                classifier.clip_model.vision_model,\n                (CLIPVisionTransformer, CLIPVisionModel),\n            ), \"Vision model is expected to be a CLIPVisionTransformer\"\n            vision_model = classifier.clip_model.vision_model\n            if isinstance(vision_model, CLIPVisionModel):\n                vision_model = vision_model.vision_model\n                # assign forward hooks for each layer\n            for i, layer in enumerate(vision_model.encoder.layers):\n                self._layer_wise_feature_save_hooks[i] = LayerWiseFeatureSaver(\n                    self.layer_wise_feature_save_path / task_name / f\"layer_{i}.pth\",\n                    first_token_only=self.layer_wise_feature_first_token_only,\n                    max_num=self.layer_wise_feature_max_num,\n                )\n                self._layer_wise_feature_save_hook_handles[i] = (\n                    layer.register_forward_hook(self._layer_wise_feature_save_hooks[i])\n                )\n\n    def on_task_evaluation_end(self):\n        \"\"\"\n        Called at the end of task evaluation to save features and remove hooks.\n        \"\"\"\n        if self.layer_wise_feature_save_path is not None:\n            # save features and remove hooks after evaluation\n            for i, hook in self._layer_wise_feature_save_hooks.items():\n                hook.save_features()\n                self._layer_wise_feature_save_hook_handles[i].remove()\n</code></pre>"},{"location":"taskpool/clip_vit_classification/#fusion_bench.taskpool.CLIPVisionModelTaskPool.__init__","title":"<code>__init__(test_datasets, *, processor, data_processor, clip_model, dataloader_kwargs=None, layer_wise_feature_save_path=None, layer_wise_feature_first_token_only=True, layer_wise_feature_max_num=None, fast_dev_run=False, **kwargs)</code>","text":"<p>Initialize the CLIPVisionModelTaskPool.</p> Source code in <code>fusion_bench/taskpool/clip_vision/taskpool.py</code> <pre><code>def __init__(\n    self,\n    test_datasets: Union[DictConfig, Dict[str, Dataset]],\n    *,\n    processor: Union[DictConfig, CLIPProcessor],\n    data_processor: Union[DictConfig, CLIPProcessor],\n    clip_model: Union[DictConfig, CLIPModel],\n    dataloader_kwargs: DictConfig = None,\n    layer_wise_feature_save_path: Optional[str] = None,\n    layer_wise_feature_first_token_only: bool = True,\n    layer_wise_feature_max_num: Optional[int] = None,\n    fast_dev_run: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    Initialize the CLIPVisionModelTaskPool.\n    \"\"\"\n    self._test_datasets = test_datasets\n    self._processor = processor\n    self._data_processor = data_processor\n    self._clip_model = clip_model\n    self._dataloader_kwargs = dataloader_kwargs or {}\n\n    # layer-wise feature saving\n    self._layer_wise_feature_save_path = layer_wise_feature_save_path\n    self.layer_wise_feature_save_path = (\n        Path(layer_wise_feature_save_path)\n        if layer_wise_feature_save_path is not None\n        else None\n    )\n    self.layer_wise_feature_first_token_only = layer_wise_feature_first_token_only\n    self.layer_wise_feature_max_num = layer_wise_feature_max_num\n\n    self.fast_dev_run = fast_dev_run\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"taskpool/clip_vit_classification/#fusion_bench.taskpool.CLIPVisionModelTaskPool.evaluate","title":"<code>evaluate(model, name=None, **kwargs)</code>","text":"<p>Evaluate the model on the image classification task.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li>           \u2013            <p>Dict[str, Any]: A dictionary containing the evaluation results for each task.</p> </li> </ul> Source code in <code>fusion_bench/taskpool/clip_vision/taskpool.py</code> <pre><code>def evaluate(\n    self,\n    model: Union[CLIPVisionModel, CLIPVisionTransformer],\n    name=None,\n    **kwargs,\n):\n    \"\"\"\n    Evaluate the model on the image classification task.\n\n    Args:\n        model (Union[CLIPVisionModel, CLIPVisionTransformer]): The model to evaluate.\n        name (Optional[str]): The name of the model. This will be logged into the report if not None.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the evaluation results for each task.\n    \"\"\"\n    if not self._is_setup:\n        self.setup()\n\n    report = {}\n    # CLIPVisionModel works the same with CLIPVisonTransformer, so we can use it directly\n    if hasattr(model, \"is_surgery_model\") and model.is_surgery_model:\n        log.info(\"running evaluation on a surgery model.\")\n        model: \"SurgeryModelWrapper\" = model\n        self.clip_model.vision_model = model\n    else:\n        # replace the vision encoder with the model\n        self.clip_model.vision_model = model\n    classifier = HFCLIPClassifier(\n        self.clip_model,\n        processor=self.processor,\n    )\n    classifier = cast(HFCLIPClassifier, self.fabric.to_device(classifier))\n    # collect basic model information\n    training_params, all_params = count_parameters(model)\n    report[\"model_info\"] = {\n        \"trainable_params\": training_params,\n        \"all_params\": all_params,\n        \"trainable_percentage\": training_params / all_params,\n    }\n    if name is not None:\n        report[\"model_info\"][\"name\"] = name\n\n    # evaluate on each task\n    pbar = tqdm(\n        self.test_dataloaders.items(),\n        desc=\"Evaluating tasks\",\n        total=len(self.test_dataloaders),\n    )\n    for task_name, test_dataloader in pbar:\n        classnames, templates = get_classnames_and_templates(task_name)\n        self.on_task_evaluation_begin(classifier, task_name)\n        classifier.set_classification_task(classnames, templates)\n        result = self._evaluate(\n            classifier,\n            test_dataloader,\n            num_classes=len(classnames),\n            task_name=task_name,\n        )\n        report[task_name] = result\n        self.on_task_evaluation_end()\n\n    # calculate the average accuracy and loss\n    if \"average\" not in report:\n        report[\"average\"] = {}\n        accuracies = [\n            value[\"accuracy\"]\n            for key, value in report.items()\n            if \"accuracy\" in value\n        ]\n        if len(accuracies) &gt; 0:\n            average_accuracy = sum(accuracies) / len(accuracies)\n            report[\"average\"][\"accuracy\"] = average_accuracy\n        losses = [value[\"loss\"] for key, value in report.items() if \"loss\" in value]\n        if len(losses) &gt; 0:\n            average_loss = sum(losses) / len(losses)\n            report[\"average\"][\"loss\"] = average_loss\n\n    log.info(f\"Evaluation Result: {report}\")\n    if self.fabric.is_global_zero and len(self.fabric._loggers) &gt; 0:\n        with open(os.path.join(self.log_dir, \"report.json\"), \"w\") as fp:\n            json.dump(report, fp)\n    return report\n</code></pre>"},{"location":"taskpool/clip_vit_classification/#fusion_bench.taskpool.CLIPVisionModelTaskPool.evaluate(model)","title":"<code>model</code>","text":"(<code>Union[CLIPVisionModel, CLIPVisionTransformer]</code>)           \u2013            <p>The model to evaluate.</p>"},{"location":"taskpool/clip_vit_classification/#fusion_bench.taskpool.CLIPVisionModelTaskPool.evaluate(name)","title":"<code>name</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The name of the model. This will be logged into the report if not None.</p>"},{"location":"taskpool/clip_vit_classification/#fusion_bench.taskpool.CLIPVisionModelTaskPool.on_task_evaluation_begin","title":"<code>on_task_evaluation_begin(classifier, task_name)</code>","text":"<p>Called at the beginning of task evaluation to set up hooks for saving layer-wise features.</p> <p>Parameters:</p> Source code in <code>fusion_bench/taskpool/clip_vision/taskpool.py</code> <pre><code>def on_task_evaluation_begin(self, classifier: HFCLIPClassifier, task_name: str):\n    \"\"\"\n    Called at the beginning of task evaluation to set up hooks for saving layer-wise features.\n\n    Args:\n        classifier (HFCLIPClassifier): The classifier being evaluated.\n        task_name (str): The name of the task being evaluated.\n    \"\"\"\n    if self.layer_wise_feature_save_path is not None:\n        # setup hooks for saving layer-wise features\n        assert isinstance(\n            classifier.clip_model.vision_model,\n            (CLIPVisionTransformer, CLIPVisionModel),\n        ), \"Vision model is expected to be a CLIPVisionTransformer\"\n        vision_model = classifier.clip_model.vision_model\n        if isinstance(vision_model, CLIPVisionModel):\n            vision_model = vision_model.vision_model\n            # assign forward hooks for each layer\n        for i, layer in enumerate(vision_model.encoder.layers):\n            self._layer_wise_feature_save_hooks[i] = LayerWiseFeatureSaver(\n                self.layer_wise_feature_save_path / task_name / f\"layer_{i}.pth\",\n                first_token_only=self.layer_wise_feature_first_token_only,\n                max_num=self.layer_wise_feature_max_num,\n            )\n            self._layer_wise_feature_save_hook_handles[i] = (\n                layer.register_forward_hook(self._layer_wise_feature_save_hooks[i])\n            )\n</code></pre>"},{"location":"taskpool/clip_vit_classification/#fusion_bench.taskpool.CLIPVisionModelTaskPool.on_task_evaluation_begin(classifier)","title":"<code>classifier</code>","text":"(<code>HFCLIPClassifier</code>)           \u2013            <p>The classifier being evaluated.</p>"},{"location":"taskpool/clip_vit_classification/#fusion_bench.taskpool.CLIPVisionModelTaskPool.on_task_evaluation_begin(task_name)","title":"<code>task_name</code>","text":"(<code>str</code>)           \u2013            <p>The name of the task being evaluated.</p>"},{"location":"taskpool/clip_vit_classification/#fusion_bench.taskpool.CLIPVisionModelTaskPool.on_task_evaluation_end","title":"<code>on_task_evaluation_end()</code>","text":"<p>Called at the end of task evaluation to save features and remove hooks.</p> Source code in <code>fusion_bench/taskpool/clip_vision/taskpool.py</code> <pre><code>def on_task_evaluation_end(self):\n    \"\"\"\n    Called at the end of task evaluation to save features and remove hooks.\n    \"\"\"\n    if self.layer_wise_feature_save_path is not None:\n        # save features and remove hooks after evaluation\n        for i, hook in self._layer_wise_feature_save_hooks.items():\n            hook.save_features()\n            self._layer_wise_feature_save_hook_handles[i].remove()\n</code></pre>"},{"location":"taskpool/clip_vit_classification/#fusion_bench.taskpool.CLIPVisionModelTaskPool.setup","title":"<code>setup()</code>","text":"<p>Set up the processor, data processor, CLIP model, test datasets, and data loaders.</p> Source code in <code>fusion_bench/taskpool/clip_vision/taskpool.py</code> <pre><code>def setup(self):\n    \"\"\"\n    Set up the processor, data processor, CLIP model, test datasets, and data loaders.\n    \"\"\"\n    # setup processor and clip model\n    self.processor = (\n        instantiate(self._processor)\n        if isinstance(self._processor, DictConfig)\n        else self._processor\n    )\n    self.data_processor = (\n        instantiate(self._data_processor)\n        if isinstance(self._data_processor, DictConfig)\n        else self._data_processor\n    )\n    self.clip_model = (\n        instantiate(self._clip_model)\n        if isinstance(self._clip_model, DictConfig)\n        else self._clip_model\n    )\n    self.clip_model = self.fabric.to_device(self.clip_model)\n    self.clip_model.requires_grad_(False)\n    self.clip_model.eval()\n\n    # Load the test datasets\n    self.test_datasets = {\n        name: instantiate(dataset) if isinstance(dataset, DictConfig) else dataset\n        for name, dataset in self._test_datasets.items()\n    }\n    self.test_datasets = {\n        name: CLIPDataset(dataset, self.data_processor)\n        for name, dataset in self.test_datasets.items()\n    }\n    # Setup the dataloaders\n    self.test_dataloaders = {\n        name: DataLoader(\n            dataset,\n            **self._dataloader_kwargs,\n            collate_fn=(\n                raw_image_collate_fn if self.data_processor is None else None\n            ),\n        )\n        for name, dataset in self.test_datasets.items()\n    }\n    self.test_dataloaders = {\n        name: self.fabric.setup_dataloaders(dataloader)\n        for name, dataloader in self.test_dataloaders.items()\n    }\n\n    self._is_setup = True\n</code></pre>"},{"location":"taskpool/dummy/","title":"Dummy TaskPool","text":"<p>The <code>DummyTaskPool</code> is used for debugging purposes.  It inherits from the base <code>TaskPool</code> class.</p>"},{"location":"taskpool/dummy/#reference","title":"Reference","text":""},{"location":"taskpool/dummy/#fusion_bench.taskpool.dummy.DummyTaskPool","title":"<code>DummyTaskPool</code>","text":"<p>               Bases: <code>BaseTaskPool</code></p> <p>This is a dummy task pool used for debugging purposes. It inherits from the base TaskPool class.</p> Source code in <code>fusion_bench/taskpool/dummy.py</code> <pre><code>class DummyTaskPool(BaseTaskPool):\n    \"\"\"\n    This is a dummy task pool used for debugging purposes. It inherits from the base TaskPool class.\n    \"\"\"\n\n    def __init__(self, model_save_path: Optional[str] = None):\n        super().__init__()\n        self.model_save_path = model_save_path\n\n    def evaluate(self, model):\n        \"\"\"\n        Evaluate the given model.\n        This method does nothing but print the parameters of the model in a human-readable format.\n\n        Args:\n            model: The model to evaluate.\n        \"\"\"\n        print_parameters(model, is_human_readable=True)\n\n        if self.model_save_path is not None:\n            with timeit_context(f\"Saving the model to {self.model_save_path}\"):\n                separate_save(model, self.model_save_path)\n\n        return get_model_summary(model)\n</code></pre>"},{"location":"taskpool/dummy/#fusion_bench.taskpool.dummy.DummyTaskPool.evaluate","title":"<code>evaluate(model)</code>","text":"<p>Evaluate the given model. This method does nothing but print the parameters of the model in a human-readable format.</p> <p>Parameters:</p> Source code in <code>fusion_bench/taskpool/dummy.py</code> <pre><code>def evaluate(self, model):\n    \"\"\"\n    Evaluate the given model.\n    This method does nothing but print the parameters of the model in a human-readable format.\n\n    Args:\n        model: The model to evaluate.\n    \"\"\"\n    print_parameters(model, is_human_readable=True)\n\n    if self.model_save_path is not None:\n        with timeit_context(f\"Saving the model to {self.model_save_path}\"):\n            separate_save(model, self.model_save_path)\n\n    return get_model_summary(model)\n</code></pre>"},{"location":"taskpool/dummy/#fusion_bench.taskpool.dummy.DummyTaskPool.evaluate(model)","title":"<code>model</code>","text":"\u2013            <p>The model to evaluate.</p>"},{"location":"taskpool/flan-t5_generation/","title":"Flan-T5 Models for Text Generation Tasks","text":"<p>This task pool provides a set of text generation tasks from the GLUE benchmark for the Flan-T5 model.  Each task is associated with a dataset.  We report the exact match accuracy metric for CoLA, MNLI, MRPC, QNLI, QQP, RTE, and SST2, and spearman's rho for STSB.</p>"},{"location":"taskpool/flan-t5_generation/#references","title":"References","text":""},{"location":"taskpool/flan-t5_generation/#fusion_bench.compat.taskpool.flan_t5_glue_text_generation","title":"<code>flan_t5_glue_text_generation</code>","text":""},{"location":"taskpool/flan-t5_generation/#fusion_bench.compat.taskpool.flan_t5_glue_text_generation.FlanT5GLUETextGenerationTaskPool","title":"<code>FlanT5GLUETextGenerationTaskPool</code>","text":"<p>               Bases: <code>LightningFabricMixin</code>, <code>TaskPool</code></p> <p>A task pool for FlanT5 GLUE text generation tasks. This class manages the tasks and provides methods for loading and evaluating tasks.</p> Source code in <code>fusion_bench/compat/taskpool/flan_t5_glue_text_generation.py</code> <pre><code>class FlanT5GLUETextGenerationTaskPool(LightningFabricMixin, TaskPool):\n    \"\"\"\n    A task pool for FlanT5 GLUE text generation tasks.\n    This class manages the tasks and provides methods for loading and evaluating tasks.\n    \"\"\"\n\n    _tokenizer_instance = None\n\n    @property\n    def tokenizer(self):\n        \"\"\"\n        Returns the tokenizer. If it's not already initialized, it initializes it using the config's tokenizer.\n        \"\"\"\n        if self._tokenizer_instance is None:\n            self._tokenizer_instance = AutoTokenizer.from_pretrained(\n                self.config.tokenizer\n            )\n        return self._tokenizer_instance\n\n    def load_task(self, task_name_or_config: str | DictConfig):\n        \"\"\"\n        Loads a task given a task name or config. If the task name is in `CLASSIFICATION_TASKS`, it creates a `FlanT5GLUETextGenerationClassificationTask`.\n        If the task name is in `REGRESSION_TASKS`, it creates a `FlanT5GLUETextGenerationRegressionTask`. Otherwise, it raises a `ValueError`.\n        \"\"\"\n        if isinstance(task_name_or_config, str):\n            task_config = self.get_task_config(task_name_or_config)\n        else:\n            task_config = task_name_or_config\n\n        if task_config.name in CLASSIFICATION_TASKS:\n            task = FlanT5GLUETextGenerationClassificationTask(task_config)\n            task._taskpool = self\n            return task\n        elif task_config.name in REGRESSION_TASKS:\n            task = FlanT5GLUETextGenerationRegressionTask(task_config)\n            task._taskpool = self\n            return task\n        else:\n            raise ValueError(f\"Unknown task {task_config.name}\")\n\n    def evaluate(self, model: T5ForConditionalGeneration):\n        \"\"\"\n        Evaluate the model on the FlanT5 GLUE text generation tasks.\n\n        Args:\n            model (T5ForConditionalGeneration): The model to evaluate.\n\n        Returns:\n            dict: A dictionary containing the evaluation results for each task.\n        \"\"\"\n        if not isinstance(model, T5ForConditionalGeneration):\n            log.warning(\n                f\"Model is not an instance of T5ForConditionalGeneration, but {type(model)}\"\n            )\n        report = {}\n        training_params, all_params = count_parameters(model)\n        report[\"model_info\"] = {\n            \"trainable_params\": training_params,\n            \"all_params\": all_params,\n            \"trainable_percentage\": training_params / all_params,\n        }\n        model = self.fabric.setup(model)\n        report.update(super().evaluate(model))\n        log.info(f\"evaluation report: {report}\")\n        return report\n</code></pre>"},{"location":"taskpool/flan-t5_generation/#fusion_bench.compat.taskpool.flan_t5_glue_text_generation.FlanT5GLUETextGenerationTaskPool.tokenizer","title":"<code>tokenizer</code>  <code>property</code>","text":"<p>Returns the tokenizer. If it's not already initialized, it initializes it using the config's tokenizer.</p>"},{"location":"taskpool/flan-t5_generation/#fusion_bench.compat.taskpool.flan_t5_glue_text_generation.FlanT5GLUETextGenerationTaskPool.evaluate","title":"<code>evaluate(model)</code>","text":"<p>Evaluate the model on the FlanT5 GLUE text generation tasks.</p> <p>Parameters:</p> <ul> <li> <code>model</code> \u00b6              (<code>T5ForConditionalGeneration</code>)           \u2013            <p>The model to evaluate.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code>          \u2013            <p>A dictionary containing the evaluation results for each task.</p> </li> </ul> Source code in <code>fusion_bench/compat/taskpool/flan_t5_glue_text_generation.py</code> <pre><code>def evaluate(self, model: T5ForConditionalGeneration):\n    \"\"\"\n    Evaluate the model on the FlanT5 GLUE text generation tasks.\n\n    Args:\n        model (T5ForConditionalGeneration): The model to evaluate.\n\n    Returns:\n        dict: A dictionary containing the evaluation results for each task.\n    \"\"\"\n    if not isinstance(model, T5ForConditionalGeneration):\n        log.warning(\n            f\"Model is not an instance of T5ForConditionalGeneration, but {type(model)}\"\n        )\n    report = {}\n    training_params, all_params = count_parameters(model)\n    report[\"model_info\"] = {\n        \"trainable_params\": training_params,\n        \"all_params\": all_params,\n        \"trainable_percentage\": training_params / all_params,\n    }\n    model = self.fabric.setup(model)\n    report.update(super().evaluate(model))\n    log.info(f\"evaluation report: {report}\")\n    return report\n</code></pre>"},{"location":"taskpool/flan-t5_generation/#fusion_bench.compat.taskpool.flan_t5_glue_text_generation.FlanT5GLUETextGenerationTaskPool.load_task","title":"<code>load_task(task_name_or_config)</code>","text":"<p>Loads a task given a task name or config. If the task name is in <code>CLASSIFICATION_TASKS</code>, it creates a <code>FlanT5GLUETextGenerationClassificationTask</code>. If the task name is in <code>REGRESSION_TASKS</code>, it creates a <code>FlanT5GLUETextGenerationRegressionTask</code>. Otherwise, it raises a <code>ValueError</code>.</p> Source code in <code>fusion_bench/compat/taskpool/flan_t5_glue_text_generation.py</code> <pre><code>def load_task(self, task_name_or_config: str | DictConfig):\n    \"\"\"\n    Loads a task given a task name or config. If the task name is in `CLASSIFICATION_TASKS`, it creates a `FlanT5GLUETextGenerationClassificationTask`.\n    If the task name is in `REGRESSION_TASKS`, it creates a `FlanT5GLUETextGenerationRegressionTask`. Otherwise, it raises a `ValueError`.\n    \"\"\"\n    if isinstance(task_name_or_config, str):\n        task_config = self.get_task_config(task_name_or_config)\n    else:\n        task_config = task_name_or_config\n\n    if task_config.name in CLASSIFICATION_TASKS:\n        task = FlanT5GLUETextGenerationClassificationTask(task_config)\n        task._taskpool = self\n        return task\n    elif task_config.name in REGRESSION_TASKS:\n        task = FlanT5GLUETextGenerationRegressionTask(task_config)\n        task._taskpool = self\n        return task\n    else:\n        raise ValueError(f\"Unknown task {task_config.name}\")\n</code></pre>"},{"location":"taskpool/gpt2_classification/","title":"GPT-2 Sequence Classification Tasks","text":"<p>This task pool provides a set of sequence classification tasks from the GLUE benchmark for the GPT-2 model.  Each task is associated with a dataset and the accuracy metric. The tasks are: CoLA, MNLI, MRPC, QNLI, QQP, RTE, and SST2.</p>"},{"location":"taskpool/gpt2_classification/#references","title":"References","text":""},{"location":"taskpool/gpt2_classification/#fusion_bench.taskpool.gpt2_text_classification","title":"<code>gpt2_text_classification</code>","text":""},{"location":"taskpool/gpt2_classification/#fusion_bench.taskpool.gpt2_text_classification.GPT2TextClassificationTaskPool","title":"<code>GPT2TextClassificationTaskPool</code>","text":"<p>               Bases: <code>BaseTaskPool</code>, <code>LightningFabricMixin</code></p> <p>A task pool for GPT2 text classification tasks. This class manages the tasks and provides methods for loading test dataset and evaluation.</p> Source code in <code>fusion_bench/taskpool/gpt2_text_classification.py</code> <pre><code>class GPT2TextClassificationTaskPool(BaseTaskPool, LightningFabricMixin):\n    \"\"\"\n    A task pool for GPT2 text classification tasks.\n    This class manages the tasks and provides methods for loading test dataset and evaluation.\n    \"\"\"\n\n    _config_mapping = BaseTaskPool._config_mapping | {\n        \"_test_datasets\": \"test_datasets\",\n        \"_tokenizer\": \"tokenizer\",\n        \"dataloader_kwargs\": \"dataloader_kwargs\",\n        \"fast_dev_run\": \"fast_dev_run\",\n    }\n\n    def __init__(\n        self,\n        test_datasets: DictConfig,\n        tokenizer: DictConfig,\n        dataloader_kwargs: DictConfig,\n        fast_dev_run: bool,\n        **kwargs,\n    ):\n        self._test_datasets = test_datasets\n        self._tokenizer = tokenizer\n        self.dataloader_kwargs = dataloader_kwargs\n        self.fast_dev_run = fast_dev_run\n        super().__init__(**kwargs)\n\n        self.setup()\n\n    def setup(self):\n        global tokenizer\n        self.tokenizer = tokenizer = instantiate(self._tokenizer)\n\n    def get_classifier(\n        self, task_name: str, model: GPT2Model\n    ) -&gt; GPT2ForSequenceClassification:\n        modelpool = self._program.modelpool\n        classifier = modelpool.load_classifier(task_name)\n        classifier.transformer = deepcopy(model)\n        return classifier\n\n    @torch.no_grad()\n    def evaluate_single_task(\n        self,\n        task_name: str,\n        model: GPT2Model,\n        test_loader: DataLoader,\n    ):\n        loss_metric = MeanMetric()\n        # load classifier and replace the backbone with the passed model\n        model: GPT2ForSequenceClassification = self.get_classifier(task_name, model)\n        accuracy = Accuracy(\"multiclass\", num_classes=model.num_labels)\n        model = self.fabric.setup(model)\n\n        if self.config.get(\"fast_dev_run\", False):\n            log.info(\"Running under fast_dev_run mode, evaluating on a single batch.\")\n            test_loader = itertools.islice(test_loader, 1)\n        else:\n            test_loader = test_loader\n\n        for batch in (\n            pbar := tqdm(\n                test_loader, desc=\"Evaluating\", leave=False, dynamic_ncols=True\n            )\n        ):\n            input_ids = batch[\"input_ids\"]\n            attention_mask = batch[\"attention_mask\"]\n            labels = batch[\"labels\"]\n\n            outputs = model(input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            loss = F.cross_entropy(logits, labels)\n\n            accuracy(logits.detach().cpu(), labels.detach().cpu())\n            loss_metric.update(loss.detach().cpu())\n            pbar.set_postfix(\n                {\n                    \"accuracy\": accuracy.compute().item(),\n                    \"loss\": loss_metric.compute().item(),\n                }\n            )\n\n        acc = accuracy.compute().item()\n        loss = loss_metric.compute().item()\n        results = {\"accuracy\": acc, \"loss\": loss}\n        log.info(f\"Results for task {task_name}: {results}\")\n        return results\n\n    def get_test_dataloader(self, task_name: str):\n        dataset = instantiate(self._test_datasets[task_name])\n        dataloader_kwargs = {\n            \"shuffle\": False,\n        }\n        dataloader_kwargs.update(self.dataloader_kwargs)\n        dataloader = DataLoader(\n            dataset, collate_fn=default_data_collator, **dataloader_kwargs\n        )\n        if self.fabric is not None:\n            dataloader = self.fabric.setup_dataloaders(dataloader)\n        return dataloader\n\n    @override\n    def evaluate(self, model: GPT2Model):\n        report = {}\n        for task_name in (pbar := tqdm(self._test_datasets, desc=\"Evaluating tasks\")):\n            pbar.set_description(f\"Evaluating task {task_name}\")\n            dataloader = self.get_test_dataloader(task_name)\n            result = self.evaluate_single_task(task_name, model, dataloader)\n            report[task_name] = result\n        return report\n</code></pre>"}]}